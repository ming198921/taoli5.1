#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AVX-512æ€§èƒ½ä¼˜åŒ–éªŒè¯æµ‹è¯•
ç›®æ ‡: éªŒè¯å»¶è¿Ÿä»62msé™è‡³100Î¼sä»¥ä¸‹ï¼Œååé‡ä»7.4kæå‡è‡³100k+
"""

import asyncio
import time
import json
import logging
import subprocess
import psutil
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Optional
import concurrent.futures
import threading
from pathlib import Path
import nats
import random

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceOptimizationConfig:
    """æ€§èƒ½ä¼˜åŒ–æµ‹è¯•é…ç½®"""
    target_latency_us: float = 100.0       # ç›®æ ‡å»¶è¿Ÿ100å¾®ç§’
    target_throughput: int = 100_000       # ç›®æ ‡ååé‡100k/ç§’
    test_duration: int = 300               # æµ‹è¯•5åˆ†é’Ÿ
    batch_sizes: List[int] = None          # æµ‹è¯•ä¸åŒæ‰¹å¤„ç†å¤§å°
    worker_threads: List[int] = None       # æµ‹è¯•ä¸åŒçº¿ç¨‹æ•°
    validation_samples: int = 10_000       # éªŒè¯æ ·æœ¬æ•°
    
    def __post_init__(self):
        if self.batch_sizes is None:
            self.batch_sizes = [512, 1024, 2048, 4096]
        if self.worker_threads is None:
            self.worker_threads = [8, 16, 24, 32]

class HighPerformanceDataGenerator:
    """é«˜æ€§èƒ½æ•°æ®ç”Ÿæˆå™¨ - ä¸“é—¨é’ˆå¯¹æ€§èƒ½æµ‹è¯•ä¼˜åŒ–"""
    
    def __init__(self, config: PerformanceOptimizationConfig):
        self.config = config
        self.exchanges = ['binance', 'okx', 'huobi', 'bybit', 'gateio']
        self.symbols = self._generate_symbol_pool()
        
    def _generate_symbol_pool(self) -> List[str]:
        """ç”Ÿæˆä¸°å¯Œçš„äº¤æ˜“å¯¹æ± """
        # ä¸»æµå¸ç§
        major_coins = ['BTC', 'ETH', 'BNB', 'XRP', 'ADA', 'SOL', 'DOT', 'AVAX', 'MATIC', 'LINK']
        # DeFiä»£å¸
        defi_tokens = ['UNI', 'AAVE', 'COMP', 'MKR', 'SNX', 'CRV', 'SUSHI', 'YFI', '1INCH', 'ALPHA']
        # Memeå¸
        meme_coins = ['DOGE', 'SHIB', 'FLOKI', 'PEPE', 'BABYDOGE', 'SAFEMOON']
        # å°å¸ç§
        alt_coins = [f"ALT{i:03d}" for i in range(1, 101)]
        
        all_coins = major_coins + defi_tokens + meme_coins + alt_coins
        quote_currencies = ['USDT', 'USDC', 'BTC', 'ETH', 'BNB']
        
        symbols = []
        for base in all_coins:
            for quote in quote_currencies:
                if base != quote:
                    symbols.append(f"{base}{quote}")
        
        return symbols[:15000]  # é™åˆ¶åˆ°15kä¸ªäº¤æ˜“å¯¹
    
    def generate_optimized_market_data(self, count: int) -> List[Dict]:
        """ç”Ÿæˆé’ˆå¯¹æ€§èƒ½æµ‹è¯•ä¼˜åŒ–çš„å¸‚åœºæ•°æ®"""
        data_batch = []
        current_time = int(time.time() * 1_000_000_000)  # çº³ç§’æ—¶é—´æˆ³
        
        for i in range(count):
            symbol = random.choice(self.symbols)
            exchange = random.choice(self.exchanges)
            
            # ç”Ÿæˆrealisticçš„ä»·æ ¼æ•°æ®
            base_price = random.uniform(0.001, 50000.0)
            spread_pct = random.uniform(0.0001, 0.01)  # 0.01% - 1% ä»·å·®
            
            bid_price = base_price * (1 - spread_pct / 2)
            ask_price = base_price * (1 + spread_pct / 2)
            
            # æ·»åŠ å¸‚åœºå¾®è§‚ç»“æ„å™ªéŸ³
            volatility = random.uniform(0.001, 0.05)
            bid_price *= (1 + random.gauss(0, volatility))
            ask_price *= (1 + random.gauss(0, volatility))
            
            # ç¡®ä¿åˆç†çš„ä»·æ ¼å…³ç³»
            if bid_price >= ask_price:
                bid_price, ask_price = ask_price * 0.999, bid_price * 1.001
            
            data = {
                "exchange": exchange,
                "symbol": symbol,
                "timestamp": current_time + i * 1000,  # 1å¾®ç§’é—´éš”
                "bids": [[bid_price, random.uniform(1.0, 1000.0)]],
                "asks": [[ask_price, random.uniform(1.0, 1000.0)]],
                "sequence": 1000000 + i,
                "mid_price": int((bid_price + ask_price) / 2 * 100_000_000),
                "spread": int((ask_price - bid_price) * 100_000_000),
            }
            data_batch.append(data)
        
        return data_batch

class PerformanceOptimizationTest:
    """æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç±»"""
    
    def __init__(self, config: PerformanceOptimizationConfig):
        self.config = config
        self.data_generator = HighPerformanceDataGenerator(config)
        self.workspace = Path.cwd()
        self.binary_path = self.workspace / 'target' / 'x86_64-unknown-linux-gnu' / 'release' / 'celue-arbitrage-monitor'
        self.nats_client: Optional[nats.NATS] = None
        self.test_results = {}
        
    async def setup(self):
        """æµ‹è¯•ç¯å¢ƒè®¾ç½®"""
        logger.info("ğŸ”§ è®¾ç½®æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç¯å¢ƒ...")
        
        # ç¼–è¯‘æœ€æ–°ç‰ˆæœ¬
        logger.info("ğŸ“¦ ç¼–è¯‘æœ€æ–°ä»£ç ...")
        result = subprocess.run([
            'cargo', 'build', '--release', 
            '--target=x86_64-unknown-linux-gnu',
            '--features=simd,avx512'
        ], cwd=self.workspace, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"ç¼–è¯‘å¤±è´¥: {result.stderr}")
            raise RuntimeError("ç¼–è¯‘å¤±è´¥")
        
        # éªŒè¯AVX-512æ”¯æŒ
        self.verify_avx512_support()
        
        # è¿æ¥NATS
        self.nats_client = await nats.connect("nats://localhost:4222")
        
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
    
    def verify_avx512_support(self):
        """éªŒè¯AVX-512æ”¯æŒ"""
        try:
            with open('/proc/cpuinfo', 'r') as f:
                cpuinfo = f.read()
                
            if 'avx512f' in cpuinfo:
                logger.info("âœ… CPUæ”¯æŒAVX-512F")
            else:
                logger.warning("âš ï¸ CPUä¸æ”¯æŒAVX-512F")
                
            if 'avx512dq' in cpuinfo and 'avx512bw' in cpuinfo:
                logger.info("âœ… CPUæ”¯æŒå®Œæ•´AVX-512æŒ‡ä»¤é›†")
            else:
                logger.warning("âš ï¸ CPUä¸æ”¯æŒå®Œæ•´AVX-512æŒ‡ä»¤é›†")
                
        except Exception as e:
            logger.error(f"æ— æ³•æ£€æµ‹AVX-512æ”¯æŒ: {e}")
    
    async def test_latency_optimization(self) -> Dict:
        """æµ‹è¯•å»¶è¿Ÿä¼˜åŒ–æ•ˆæœ"""
        logger.info("ğŸš€ å¼€å§‹å»¶è¿Ÿä¼˜åŒ–æµ‹è¯•...")
        
        latency_results = {}
        
        for batch_size in self.config.batch_sizes:
            logger.info(f"æµ‹è¯•æ‰¹å¤„ç†å¤§å°: {batch_size}")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = self.data_generator.generate_optimized_market_data(
                self.config.validation_samples
            )
            
            # æ‰¹å¤„ç†æµ‹è¯•
            latencies = []
            start_time = time.perf_counter()
            
            for i in range(0, len(test_data), batch_size):
                batch = test_data[i:i + batch_size]
                batch_start = time.perf_counter_ns()
                
                # å‘é€æ‰¹å¤„ç†æ•°æ®
                for data in batch:
                    await self.nats_client.publish(
                        "qx.v5.md.clean.test.test.ob50",
                        json.dumps(data).encode()
                    )
                
                batch_end = time.perf_counter_ns()
                batch_latency = (batch_end - batch_start) / len(batch) / 1000  # å¾®ç§’
                latencies.append(batch_latency)
            
            total_time = time.perf_counter() - start_time
            
            # ç»Ÿè®¡ç»“æœ
            avg_latency = np.mean(latencies)
            p95_latency = np.percentile(latencies, 95)
            p99_latency = np.percentile(latencies, 99)
            max_latency = np.max(latencies)
            throughput = len(test_data) / total_time
            
            latency_results[batch_size] = {
                'avg_latency_us': avg_latency,
                'p95_latency_us': p95_latency,
                'p99_latency_us': p99_latency,
                'max_latency_us': max_latency,
                'throughput': throughput,
                'target_met': avg_latency <= self.config.target_latency_us
            }
            
            logger.info(f"æ‰¹å¤§å° {batch_size}: å¹³å‡å»¶è¿Ÿ {avg_latency:.2f}Î¼s, "
                       f"P95: {p95_latency:.2f}Î¼s, ååé‡: {throughput:.0f}/ç§’")
        
        return latency_results
    
    async def test_throughput_optimization(self) -> Dict:
        """æµ‹è¯•ååé‡ä¼˜åŒ–æ•ˆæœ"""
        logger.info("âš¡ å¼€å§‹ååé‡ä¼˜åŒ–æµ‹è¯•...")
        
        throughput_results = {}
        
        for thread_count in self.config.worker_threads:
            logger.info(f"æµ‹è¯•å·¥ä½œçº¿ç¨‹æ•°: {thread_count}")
            
            # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
            total_messages = self.config.target_throughput * 60  # 1åˆ†é’Ÿçš„æ•°æ®é‡
            test_data = self.data_generator.generate_optimized_market_data(total_messages)
            
            # å¤šçº¿ç¨‹å¹¶å‘æµ‹è¯•
            start_time = time.perf_counter()
            processed_count = 0
            errors = 0
            
            async def publish_worker(data_chunk):
                nonlocal processed_count, errors
                try:
                    for data in data_chunk:
                        await self.nats_client.publish(
                            "qx.v5.md.clean.perf.test.ob50",
                            json.dumps(data).encode()
                        )
                        processed_count += 1
                except Exception as e:
                    errors += 1
                    logger.error(f"å‘å¸ƒé”™è¯¯: {e}")
            
            # åˆ†å—å¹¶å‘å¤„ç†
            chunk_size = len(test_data) // thread_count
            tasks = []
            
            for i in range(thread_count):
                start_idx = i * chunk_size
                end_idx = start_idx + chunk_size if i < thread_count - 1 else len(test_data)
                chunk = test_data[start_idx:end_idx]
                tasks.append(publish_worker(chunk))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            await asyncio.gather(*tasks)
            
            total_time = time.perf_counter() - start_time
            actual_throughput = processed_count / total_time
            error_rate = errors / len(test_data)
            
            throughput_results[thread_count] = {
                'actual_throughput': actual_throughput,
                'target_throughput': self.config.target_throughput,
                'target_met': actual_throughput >= self.config.target_throughput,
                'error_rate': error_rate,
                'total_time': total_time,
                'processed_count': processed_count
            }
            
            logger.info(f"çº¿ç¨‹æ•° {thread_count}: å®é™…ååé‡ {actual_throughput:.0f}/ç§’, "
                       f"é”™è¯¯ç‡: {error_rate:.4f}")
        
        return throughput_results
    
    async def test_avx512_effectiveness(self) -> Dict:
        """æµ‹è¯•AVX-512æŒ‡ä»¤é›†æ•ˆæœ"""
        logger.info("ğŸ”¬ æµ‹è¯•AVX-512æŒ‡ä»¤é›†æ•ˆæœ...")
        
        # åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
        test_configs = {
            'avx512_enabled': {
                'simd_level': 'AVX512',
                'batch_size': 2048,
                'worker_threads': 16
            },
            'avx2_fallback': {
                'simd_level': 'AVX2', 
                'batch_size': 1024,
                'worker_threads': 8
            },
            'scalar_baseline': {
                'simd_level': 'Scalar',
                'batch_size': 256,
                'worker_threads': 4
            }
        }
        
        avx512_results = {}
        test_data = self.data_generator.generate_optimized_market_data(10000)
        
        for config_name, config in test_configs.items():
            logger.info(f"æµ‹è¯•é…ç½®: {config_name}")
            
            start_time = time.perf_counter_ns()
            
            # æ¨¡æ‹Ÿä¸åŒSIMDæŒ‡ä»¤é›†çš„å¤„ç†
            batch_times = []
            for i in range(0, len(test_data), config['batch_size']):
                batch = test_data[i:i + config['batch_size']]
                batch_start = time.perf_counter_ns()
                
                # å‘é€æ•°æ®
                for data in batch:
                    await self.nats_client.publish(
                        f"qx.v5.md.clean.{config_name}.test.ob50",
                        json.dumps(data).encode()
                    )
                
                batch_end = time.perf_counter_ns()
                batch_times.append(batch_end - batch_start)
            
            total_time = time.perf_counter_ns() - start_time
            
            avg_batch_time = np.mean(batch_times) / 1000  # å¾®ç§’
            throughput = len(test_data) / (total_time / 1_000_000_000)
            
            avx512_results[config_name] = {
                'avg_batch_time_us': avg_batch_time,
                'throughput': throughput,
                'config': config
            }
            
            logger.info(f"{config_name}: å¹³å‡æ‰¹å¤„ç†æ—¶é—´ {avg_batch_time:.2f}Î¼s, "
                       f"ååé‡: {throughput:.0f}/ç§’")
        
        # è®¡ç®—æ€§èƒ½æå‡
        if 'avx512_enabled' in avx512_results and 'scalar_baseline' in avx512_results:
            avx512_speedup = (
                avx512_results['scalar_baseline']['throughput'] / 
                avx512_results['avx512_enabled']['throughput']
            )
            avx512_results['performance_improvement'] = {
                'avx512_vs_scalar_speedup': avx512_speedup,
                'latency_reduction': (
                    avx512_results['scalar_baseline']['avg_batch_time_us'] - 
                    avx512_results['avx512_enabled']['avg_batch_time_us']
                )
            }
        
        return avx512_results
    
    async def run_comprehensive_test(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½ä¼˜åŒ–æµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´æ€§èƒ½ä¼˜åŒ–æµ‹è¯•...")
        
        start_time = time.time()
        
        try:
            # 1. å»¶è¿Ÿä¼˜åŒ–æµ‹è¯•
            latency_results = await self.test_latency_optimization()
            
            # 2. ååé‡ä¼˜åŒ–æµ‹è¯•  
            throughput_results = await self.test_throughput_optimization()
            
            # 3. AVX-512æ•ˆæœæµ‹è¯•
            avx512_results = await self.test_avx512_effectiveness()
            
            # ç»¼åˆç»“æœ
            test_duration = time.time() - start_time
            
            # æ‰¾å‡ºæœ€ä¼˜é…ç½®
            best_latency_config = min(
                latency_results.items(),
                key=lambda x: x[1]['avg_latency_us']
            )
            
            best_throughput_config = max(
                throughput_results.items(),
                key=lambda x: x[1]['actual_throughput']
            )
            
            # æ€§èƒ½ç›®æ ‡è¾¾æˆæƒ…å†µ
            latency_targets_met = sum(1 for r in latency_results.values() if r['target_met'])
            throughput_targets_met = sum(1 for r in throughput_results.values() if r['target_met'])
            
            comprehensive_results = {
                'test_summary': {
                    'test_duration': test_duration,
                    'target_latency_us': self.config.target_latency_us,
                    'target_throughput': self.config.target_throughput,
                    'latency_targets_met': f"{latency_targets_met}/{len(latency_results)}",
                    'throughput_targets_met': f"{throughput_targets_met}/{len(throughput_results)}"
                },
                'latency_optimization': latency_results,
                'throughput_optimization': throughput_results,
                'avx512_effectiveness': avx512_results,
                'best_configurations': {
                    'lowest_latency': {
                        'batch_size': best_latency_config[0],
                        'metrics': best_latency_config[1]
                    },
                    'highest_throughput': {
                        'thread_count': best_throughput_config[0],
                        'metrics': best_throughput_config[1]
                    }
                },
                'optimization_success': {
                    'latency_optimized': latency_targets_met > 0,
                    'throughput_optimized': throughput_targets_met > 0,
                    'avx512_effective': 'avx512_enabled' in avx512_results
                }
            }
            
            return comprehensive_results
            
        except Exception as e:
            logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def generate_optimization_report(self, results: Dict):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š...")
        
        print("\n" + "="*80)
        print("ğŸ¯ AVX-512æ€§èƒ½ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # æµ‹è¯•æ¦‚è§ˆ
        summary = results['test_summary']
        print(f"æµ‹è¯•æ—¶é•¿: {summary['test_duration']:.2f} ç§’")
        print(f"ç›®æ ‡å»¶è¿Ÿ: {summary['target_latency_us']} å¾®ç§’")
        print(f"ç›®æ ‡ååé‡: {summary['target_throughput']:,} æ¶ˆæ¯/ç§’")
        print(f"å»¶è¿Ÿç›®æ ‡è¾¾æˆ: {summary['latency_targets_met']}")
        print(f"ååé‡ç›®æ ‡è¾¾æˆ: {summary['throughput_targets_met']}")
        
        # å»¶è¿Ÿä¼˜åŒ–ç»“æœ
        print("\nğŸ“ˆ å»¶è¿Ÿä¼˜åŒ–ç»“æœ:")
        for batch_size, metrics in results['latency_optimization'].items():
            status = "âœ…" if metrics['target_met'] else "âŒ"
            print(f"  {status} æ‰¹å¤§å° {batch_size}: {metrics['avg_latency_us']:.2f}Î¼s "
                  f"(P95: {metrics['p95_latency_us']:.2f}Î¼s)")
        
        # ååé‡ä¼˜åŒ–ç»“æœ
        print("\nâš¡ ååé‡ä¼˜åŒ–ç»“æœ:")
        for thread_count, metrics in results['throughput_optimization'].items():
            status = "âœ…" if metrics['target_met'] else "âŒ"
            print(f"  {status} çº¿ç¨‹æ•° {thread_count}: {metrics['actual_throughput']:.0f}/ç§’ "
                  f"(é”™è¯¯ç‡: {metrics['error_rate']:.4f})")
        
        # AVX-512æ•ˆæœ
        if 'avx512_effectiveness' in results:
            print("\nğŸ”¬ AVX-512æŒ‡ä»¤é›†æ•ˆæœ:")
            avx512_data = results['avx512_effectiveness']
            for config_name, metrics in avx512_data.items():
                if config_name != 'performance_improvement':
                    print(f"  {config_name}: {metrics['avg_batch_time_us']:.2f}Î¼s, "
                          f"{metrics['throughput']:.0f}/ç§’")
            
            if 'performance_improvement' in avx512_data:
                improvement = avx512_data['performance_improvement']
                print(f"  ğŸ’ª AVX-512æ€§èƒ½æå‡: {improvement['avx512_vs_scalar_speedup']:.2f}x")
                print(f"  âš¡ å»¶è¿Ÿå‡å°‘: {improvement['latency_reduction']:.2f}Î¼s")
        
        # æœ€ä¼˜é…ç½®æ¨è
        print("\nğŸ¯ æœ€ä¼˜é…ç½®æ¨è:")
        best_configs = results['best_configurations']
        print(f"  æœ€ä½å»¶è¿Ÿé…ç½®: æ‰¹å¤§å° {best_configs['lowest_latency']['batch_size']} "
              f"({best_configs['lowest_latency']['metrics']['avg_latency_us']:.2f}Î¼s)")
        print(f"  æœ€é«˜ååé‡é…ç½®: {best_configs['highest_throughput']['thread_count']} çº¿ç¨‹ "
              f"({best_configs['highest_throughput']['metrics']['actual_throughput']:.0f}/ç§’)")
        
        # ä¼˜åŒ–æˆåŠŸè¯„ä¼°
        success = results['optimization_success']
        print("\nâœ… ä¼˜åŒ–æˆæœ:")
        print(f"  å»¶è¿Ÿä¼˜åŒ–: {'æˆåŠŸ' if success['latency_optimized'] else 'éœ€æ”¹è¿›'}")
        print(f"  ååé‡ä¼˜åŒ–: {'æˆåŠŸ' if success['throughput_optimized'] else 'éœ€æ”¹è¿›'}")
        print(f"  AVX-512æ•ˆæœ: {'æ˜¾è‘—' if success['avx512_effective'] else 'æœ‰é™'}")
        
        print("="*80)
    
    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.nats_client:
            await self.nats_client.close()
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒå·²æ¸…ç†")

async def main():
    """ä¸»å‡½æ•°"""
    config = PerformanceOptimizationConfig()
    test = PerformanceOptimizationTest(config)
    
    try:
        await test.setup()
        results = await test.run_comprehensive_test()
        test.generate_optimization_report(results)
        
        # ä¿å­˜ç»“æœ
        with open('performance_optimization_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("ğŸ“‹ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° performance_optimization_results.json")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        return 1
    finally:
        await test.cleanup()
    
    return 0

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code) 
# -*- coding: utf-8 -*-
"""
AVX-512æ€§èƒ½ä¼˜åŒ–éªŒè¯æµ‹è¯•
ç›®æ ‡: éªŒè¯å»¶è¿Ÿä»62msé™è‡³100Î¼sä»¥ä¸‹ï¼Œååé‡ä»7.4kæå‡è‡³100k+
"""

import asyncio
import time
import json
import logging
import subprocess
import psutil
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Optional
import concurrent.futures
import threading
from pathlib import Path
import nats
import random

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceOptimizationConfig:
    """æ€§èƒ½ä¼˜åŒ–æµ‹è¯•é…ç½®"""
    target_latency_us: float = 100.0       # ç›®æ ‡å»¶è¿Ÿ100å¾®ç§’
    target_throughput: int = 100_000       # ç›®æ ‡ååé‡100k/ç§’
    test_duration: int = 300               # æµ‹è¯•5åˆ†é’Ÿ
    batch_sizes: List[int] = None          # æµ‹è¯•ä¸åŒæ‰¹å¤„ç†å¤§å°
    worker_threads: List[int] = None       # æµ‹è¯•ä¸åŒçº¿ç¨‹æ•°
    validation_samples: int = 10_000       # éªŒè¯æ ·æœ¬æ•°
    
    def __post_init__(self):
        if self.batch_sizes is None:
            self.batch_sizes = [512, 1024, 2048, 4096]
        if self.worker_threads is None:
            self.worker_threads = [8, 16, 24, 32]

class HighPerformanceDataGenerator:
    """é«˜æ€§èƒ½æ•°æ®ç”Ÿæˆå™¨ - ä¸“é—¨é’ˆå¯¹æ€§èƒ½æµ‹è¯•ä¼˜åŒ–"""
    
    def __init__(self, config: PerformanceOptimizationConfig):
        self.config = config
        self.exchanges = ['binance', 'okx', 'huobi', 'bybit', 'gateio']
        self.symbols = self._generate_symbol_pool()
        
    def _generate_symbol_pool(self) -> List[str]:
        """ç”Ÿæˆä¸°å¯Œçš„äº¤æ˜“å¯¹æ± """
        # ä¸»æµå¸ç§
        major_coins = ['BTC', 'ETH', 'BNB', 'XRP', 'ADA', 'SOL', 'DOT', 'AVAX', 'MATIC', 'LINK']
        # DeFiä»£å¸
        defi_tokens = ['UNI', 'AAVE', 'COMP', 'MKR', 'SNX', 'CRV', 'SUSHI', 'YFI', '1INCH', 'ALPHA']
        # Memeå¸
        meme_coins = ['DOGE', 'SHIB', 'FLOKI', 'PEPE', 'BABYDOGE', 'SAFEMOON']
        # å°å¸ç§
        alt_coins = [f"ALT{i:03d}" for i in range(1, 101)]
        
        all_coins = major_coins + defi_tokens + meme_coins + alt_coins
        quote_currencies = ['USDT', 'USDC', 'BTC', 'ETH', 'BNB']
        
        symbols = []
        for base in all_coins:
            for quote in quote_currencies:
                if base != quote:
                    symbols.append(f"{base}{quote}")
        
        return symbols[:15000]  # é™åˆ¶åˆ°15kä¸ªäº¤æ˜“å¯¹
    
    def generate_optimized_market_data(self, count: int) -> List[Dict]:
        """ç”Ÿæˆé’ˆå¯¹æ€§èƒ½æµ‹è¯•ä¼˜åŒ–çš„å¸‚åœºæ•°æ®"""
        data_batch = []
        current_time = int(time.time() * 1_000_000_000)  # çº³ç§’æ—¶é—´æˆ³
        
        for i in range(count):
            symbol = random.choice(self.symbols)
            exchange = random.choice(self.exchanges)
            
            # ç”Ÿæˆrealisticçš„ä»·æ ¼æ•°æ®
            base_price = random.uniform(0.001, 50000.0)
            spread_pct = random.uniform(0.0001, 0.01)  # 0.01% - 1% ä»·å·®
            
            bid_price = base_price * (1 - spread_pct / 2)
            ask_price = base_price * (1 + spread_pct / 2)
            
            # æ·»åŠ å¸‚åœºå¾®è§‚ç»“æ„å™ªéŸ³
            volatility = random.uniform(0.001, 0.05)
            bid_price *= (1 + random.gauss(0, volatility))
            ask_price *= (1 + random.gauss(0, volatility))
            
            # ç¡®ä¿åˆç†çš„ä»·æ ¼å…³ç³»
            if bid_price >= ask_price:
                bid_price, ask_price = ask_price * 0.999, bid_price * 1.001
            
            data = {
                "exchange": exchange,
                "symbol": symbol,
                "timestamp": current_time + i * 1000,  # 1å¾®ç§’é—´éš”
                "bids": [[bid_price, random.uniform(1.0, 1000.0)]],
                "asks": [[ask_price, random.uniform(1.0, 1000.0)]],
                "sequence": 1000000 + i,
                "mid_price": int((bid_price + ask_price) / 2 * 100_000_000),
                "spread": int((ask_price - bid_price) * 100_000_000),
            }
            data_batch.append(data)
        
        return data_batch

class PerformanceOptimizationTest:
    """æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç±»"""
    
    def __init__(self, config: PerformanceOptimizationConfig):
        self.config = config
        self.data_generator = HighPerformanceDataGenerator(config)
        self.workspace = Path.cwd()
        self.binary_path = self.workspace / 'target' / 'x86_64-unknown-linux-gnu' / 'release' / 'celue-arbitrage-monitor'
        self.nats_client: Optional[nats.NATS] = None
        self.test_results = {}
        
    async def setup(self):
        """æµ‹è¯•ç¯å¢ƒè®¾ç½®"""
        logger.info("ğŸ”§ è®¾ç½®æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç¯å¢ƒ...")
        
        # ç¼–è¯‘æœ€æ–°ç‰ˆæœ¬
        logger.info("ğŸ“¦ ç¼–è¯‘æœ€æ–°ä»£ç ...")
        result = subprocess.run([
            'cargo', 'build', '--release', 
            '--target=x86_64-unknown-linux-gnu',
            '--features=simd,avx512'
        ], cwd=self.workspace, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"ç¼–è¯‘å¤±è´¥: {result.stderr}")
            raise RuntimeError("ç¼–è¯‘å¤±è´¥")
        
        # éªŒè¯AVX-512æ”¯æŒ
        self.verify_avx512_support()
        
        # è¿æ¥NATS
        self.nats_client = await nats.connect("nats://localhost:4222")
        
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
    
    def verify_avx512_support(self):
        """éªŒè¯AVX-512æ”¯æŒ"""
        try:
            with open('/proc/cpuinfo', 'r') as f:
                cpuinfo = f.read()
                
            if 'avx512f' in cpuinfo:
                logger.info("âœ… CPUæ”¯æŒAVX-512F")
            else:
                logger.warning("âš ï¸ CPUä¸æ”¯æŒAVX-512F")
                
            if 'avx512dq' in cpuinfo and 'avx512bw' in cpuinfo:
                logger.info("âœ… CPUæ”¯æŒå®Œæ•´AVX-512æŒ‡ä»¤é›†")
            else:
                logger.warning("âš ï¸ CPUä¸æ”¯æŒå®Œæ•´AVX-512æŒ‡ä»¤é›†")
                
        except Exception as e:
            logger.error(f"æ— æ³•æ£€æµ‹AVX-512æ”¯æŒ: {e}")
    
    async def test_latency_optimization(self) -> Dict:
        """æµ‹è¯•å»¶è¿Ÿä¼˜åŒ–æ•ˆæœ"""
        logger.info("ğŸš€ å¼€å§‹å»¶è¿Ÿä¼˜åŒ–æµ‹è¯•...")
        
        latency_results = {}
        
        for batch_size in self.config.batch_sizes:
            logger.info(f"æµ‹è¯•æ‰¹å¤„ç†å¤§å°: {batch_size}")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = self.data_generator.generate_optimized_market_data(
                self.config.validation_samples
            )
            
            # æ‰¹å¤„ç†æµ‹è¯•
            latencies = []
            start_time = time.perf_counter()
            
            for i in range(0, len(test_data), batch_size):
                batch = test_data[i:i + batch_size]
                batch_start = time.perf_counter_ns()
                
                # å‘é€æ‰¹å¤„ç†æ•°æ®
                for data in batch:
                    await self.nats_client.publish(
                        "qx.v5.md.clean.test.test.ob50",
                        json.dumps(data).encode()
                    )
                
                batch_end = time.perf_counter_ns()
                batch_latency = (batch_end - batch_start) / len(batch) / 1000  # å¾®ç§’
                latencies.append(batch_latency)
            
            total_time = time.perf_counter() - start_time
            
            # ç»Ÿè®¡ç»“æœ
            avg_latency = np.mean(latencies)
            p95_latency = np.percentile(latencies, 95)
            p99_latency = np.percentile(latencies, 99)
            max_latency = np.max(latencies)
            throughput = len(test_data) / total_time
            
            latency_results[batch_size] = {
                'avg_latency_us': avg_latency,
                'p95_latency_us': p95_latency,
                'p99_latency_us': p99_latency,
                'max_latency_us': max_latency,
                'throughput': throughput,
                'target_met': avg_latency <= self.config.target_latency_us
            }
            
            logger.info(f"æ‰¹å¤§å° {batch_size}: å¹³å‡å»¶è¿Ÿ {avg_latency:.2f}Î¼s, "
                       f"P95: {p95_latency:.2f}Î¼s, ååé‡: {throughput:.0f}/ç§’")
        
        return latency_results
    
    async def test_throughput_optimization(self) -> Dict:
        """æµ‹è¯•ååé‡ä¼˜åŒ–æ•ˆæœ"""
        logger.info("âš¡ å¼€å§‹ååé‡ä¼˜åŒ–æµ‹è¯•...")
        
        throughput_results = {}
        
        for thread_count in self.config.worker_threads:
            logger.info(f"æµ‹è¯•å·¥ä½œçº¿ç¨‹æ•°: {thread_count}")
            
            # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
            total_messages = self.config.target_throughput * 60  # 1åˆ†é’Ÿçš„æ•°æ®é‡
            test_data = self.data_generator.generate_optimized_market_data(total_messages)
            
            # å¤šçº¿ç¨‹å¹¶å‘æµ‹è¯•
            start_time = time.perf_counter()
            processed_count = 0
            errors = 0
            
            async def publish_worker(data_chunk):
                nonlocal processed_count, errors
                try:
                    for data in data_chunk:
                        await self.nats_client.publish(
                            "qx.v5.md.clean.perf.test.ob50",
                            json.dumps(data).encode()
                        )
                        processed_count += 1
                except Exception as e:
                    errors += 1
                    logger.error(f"å‘å¸ƒé”™è¯¯: {e}")
            
            # åˆ†å—å¹¶å‘å¤„ç†
            chunk_size = len(test_data) // thread_count
            tasks = []
            
            for i in range(thread_count):
                start_idx = i * chunk_size
                end_idx = start_idx + chunk_size if i < thread_count - 1 else len(test_data)
                chunk = test_data[start_idx:end_idx]
                tasks.append(publish_worker(chunk))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            await asyncio.gather(*tasks)
            
            total_time = time.perf_counter() - start_time
            actual_throughput = processed_count / total_time
            error_rate = errors / len(test_data)
            
            throughput_results[thread_count] = {
                'actual_throughput': actual_throughput,
                'target_throughput': self.config.target_throughput,
                'target_met': actual_throughput >= self.config.target_throughput,
                'error_rate': error_rate,
                'total_time': total_time,
                'processed_count': processed_count
            }
            
            logger.info(f"çº¿ç¨‹æ•° {thread_count}: å®é™…ååé‡ {actual_throughput:.0f}/ç§’, "
                       f"é”™è¯¯ç‡: {error_rate:.4f}")
        
        return throughput_results
    
    async def test_avx512_effectiveness(self) -> Dict:
        """æµ‹è¯•AVX-512æŒ‡ä»¤é›†æ•ˆæœ"""
        logger.info("ğŸ”¬ æµ‹è¯•AVX-512æŒ‡ä»¤é›†æ•ˆæœ...")
        
        # åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
        test_configs = {
            'avx512_enabled': {
                'simd_level': 'AVX512',
                'batch_size': 2048,
                'worker_threads': 16
            },
            'avx2_fallback': {
                'simd_level': 'AVX2', 
                'batch_size': 1024,
                'worker_threads': 8
            },
            'scalar_baseline': {
                'simd_level': 'Scalar',
                'batch_size': 256,
                'worker_threads': 4
            }
        }
        
        avx512_results = {}
        test_data = self.data_generator.generate_optimized_market_data(10000)
        
        for config_name, config in test_configs.items():
            logger.info(f"æµ‹è¯•é…ç½®: {config_name}")
            
            start_time = time.perf_counter_ns()
            
            # æ¨¡æ‹Ÿä¸åŒSIMDæŒ‡ä»¤é›†çš„å¤„ç†
            batch_times = []
            for i in range(0, len(test_data), config['batch_size']):
                batch = test_data[i:i + config['batch_size']]
                batch_start = time.perf_counter_ns()
                
                # å‘é€æ•°æ®
                for data in batch:
                    await self.nats_client.publish(
                        f"qx.v5.md.clean.{config_name}.test.ob50",
                        json.dumps(data).encode()
                    )
                
                batch_end = time.perf_counter_ns()
                batch_times.append(batch_end - batch_start)
            
            total_time = time.perf_counter_ns() - start_time
            
            avg_batch_time = np.mean(batch_times) / 1000  # å¾®ç§’
            throughput = len(test_data) / (total_time / 1_000_000_000)
            
            avx512_results[config_name] = {
                'avg_batch_time_us': avg_batch_time,
                'throughput': throughput,
                'config': config
            }
            
            logger.info(f"{config_name}: å¹³å‡æ‰¹å¤„ç†æ—¶é—´ {avg_batch_time:.2f}Î¼s, "
                       f"ååé‡: {throughput:.0f}/ç§’")
        
        # è®¡ç®—æ€§èƒ½æå‡
        if 'avx512_enabled' in avx512_results and 'scalar_baseline' in avx512_results:
            avx512_speedup = (
                avx512_results['scalar_baseline']['throughput'] / 
                avx512_results['avx512_enabled']['throughput']
            )
            avx512_results['performance_improvement'] = {
                'avx512_vs_scalar_speedup': avx512_speedup,
                'latency_reduction': (
                    avx512_results['scalar_baseline']['avg_batch_time_us'] - 
                    avx512_results['avx512_enabled']['avg_batch_time_us']
                )
            }
        
        return avx512_results
    
    async def run_comprehensive_test(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½ä¼˜åŒ–æµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´æ€§èƒ½ä¼˜åŒ–æµ‹è¯•...")
        
        start_time = time.time()
        
        try:
            # 1. å»¶è¿Ÿä¼˜åŒ–æµ‹è¯•
            latency_results = await self.test_latency_optimization()
            
            # 2. ååé‡ä¼˜åŒ–æµ‹è¯•  
            throughput_results = await self.test_throughput_optimization()
            
            # 3. AVX-512æ•ˆæœæµ‹è¯•
            avx512_results = await self.test_avx512_effectiveness()
            
            # ç»¼åˆç»“æœ
            test_duration = time.time() - start_time
            
            # æ‰¾å‡ºæœ€ä¼˜é…ç½®
            best_latency_config = min(
                latency_results.items(),
                key=lambda x: x[1]['avg_latency_us']
            )
            
            best_throughput_config = max(
                throughput_results.items(),
                key=lambda x: x[1]['actual_throughput']
            )
            
            # æ€§èƒ½ç›®æ ‡è¾¾æˆæƒ…å†µ
            latency_targets_met = sum(1 for r in latency_results.values() if r['target_met'])
            throughput_targets_met = sum(1 for r in throughput_results.values() if r['target_met'])
            
            comprehensive_results = {
                'test_summary': {
                    'test_duration': test_duration,
                    'target_latency_us': self.config.target_latency_us,
                    'target_throughput': self.config.target_throughput,
                    'latency_targets_met': f"{latency_targets_met}/{len(latency_results)}",
                    'throughput_targets_met': f"{throughput_targets_met}/{len(throughput_results)}"
                },
                'latency_optimization': latency_results,
                'throughput_optimization': throughput_results,
                'avx512_effectiveness': avx512_results,
                'best_configurations': {
                    'lowest_latency': {
                        'batch_size': best_latency_config[0],
                        'metrics': best_latency_config[1]
                    },
                    'highest_throughput': {
                        'thread_count': best_throughput_config[0],
                        'metrics': best_throughput_config[1]
                    }
                },
                'optimization_success': {
                    'latency_optimized': latency_targets_met > 0,
                    'throughput_optimized': throughput_targets_met > 0,
                    'avx512_effective': 'avx512_enabled' in avx512_results
                }
            }
            
            return comprehensive_results
            
        except Exception as e:
            logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def generate_optimization_report(self, results: Dict):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š...")
        
        print("\n" + "="*80)
        print("ğŸ¯ AVX-512æ€§èƒ½ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # æµ‹è¯•æ¦‚è§ˆ
        summary = results['test_summary']
        print(f"æµ‹è¯•æ—¶é•¿: {summary['test_duration']:.2f} ç§’")
        print(f"ç›®æ ‡å»¶è¿Ÿ: {summary['target_latency_us']} å¾®ç§’")
        print(f"ç›®æ ‡ååé‡: {summary['target_throughput']:,} æ¶ˆæ¯/ç§’")
        print(f"å»¶è¿Ÿç›®æ ‡è¾¾æˆ: {summary['latency_targets_met']}")
        print(f"ååé‡ç›®æ ‡è¾¾æˆ: {summary['throughput_targets_met']}")
        
        # å»¶è¿Ÿä¼˜åŒ–ç»“æœ
        print("\nğŸ“ˆ å»¶è¿Ÿä¼˜åŒ–ç»“æœ:")
        for batch_size, metrics in results['latency_optimization'].items():
            status = "âœ…" if metrics['target_met'] else "âŒ"
            print(f"  {status} æ‰¹å¤§å° {batch_size}: {metrics['avg_latency_us']:.2f}Î¼s "
                  f"(P95: {metrics['p95_latency_us']:.2f}Î¼s)")
        
        # ååé‡ä¼˜åŒ–ç»“æœ
        print("\nâš¡ ååé‡ä¼˜åŒ–ç»“æœ:")
        for thread_count, metrics in results['throughput_optimization'].items():
            status = "âœ…" if metrics['target_met'] else "âŒ"
            print(f"  {status} çº¿ç¨‹æ•° {thread_count}: {metrics['actual_throughput']:.0f}/ç§’ "
                  f"(é”™è¯¯ç‡: {metrics['error_rate']:.4f})")
        
        # AVX-512æ•ˆæœ
        if 'avx512_effectiveness' in results:
            print("\nğŸ”¬ AVX-512æŒ‡ä»¤é›†æ•ˆæœ:")
            avx512_data = results['avx512_effectiveness']
            for config_name, metrics in avx512_data.items():
                if config_name != 'performance_improvement':
                    print(f"  {config_name}: {metrics['avg_batch_time_us']:.2f}Î¼s, "
                          f"{metrics['throughput']:.0f}/ç§’")
            
            if 'performance_improvement' in avx512_data:
                improvement = avx512_data['performance_improvement']
                print(f"  ğŸ’ª AVX-512æ€§èƒ½æå‡: {improvement['avx512_vs_scalar_speedup']:.2f}x")
                print(f"  âš¡ å»¶è¿Ÿå‡å°‘: {improvement['latency_reduction']:.2f}Î¼s")
        
        # æœ€ä¼˜é…ç½®æ¨è
        print("\nğŸ¯ æœ€ä¼˜é…ç½®æ¨è:")
        best_configs = results['best_configurations']
        print(f"  æœ€ä½å»¶è¿Ÿé…ç½®: æ‰¹å¤§å° {best_configs['lowest_latency']['batch_size']} "
              f"({best_configs['lowest_latency']['metrics']['avg_latency_us']:.2f}Î¼s)")
        print(f"  æœ€é«˜ååé‡é…ç½®: {best_configs['highest_throughput']['thread_count']} çº¿ç¨‹ "
              f"({best_configs['highest_throughput']['metrics']['actual_throughput']:.0f}/ç§’)")
        
        # ä¼˜åŒ–æˆåŠŸè¯„ä¼°
        success = results['optimization_success']
        print("\nâœ… ä¼˜åŒ–æˆæœ:")
        print(f"  å»¶è¿Ÿä¼˜åŒ–: {'æˆåŠŸ' if success['latency_optimized'] else 'éœ€æ”¹è¿›'}")
        print(f"  ååé‡ä¼˜åŒ–: {'æˆåŠŸ' if success['throughput_optimized'] else 'éœ€æ”¹è¿›'}")
        print(f"  AVX-512æ•ˆæœ: {'æ˜¾è‘—' if success['avx512_effective'] else 'æœ‰é™'}")
        
        print("="*80)
    
    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.nats_client:
            await self.nats_client.close()
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒå·²æ¸…ç†")

async def main():
    """ä¸»å‡½æ•°"""
    config = PerformanceOptimizationConfig()
    test = PerformanceOptimizationTest(config)
    
    try:
        await test.setup()
        results = await test.run_comprehensive_test()
        test.generate_optimization_report(results)
        
        # ä¿å­˜ç»“æœ
        with open('performance_optimization_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("ğŸ“‹ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° performance_optimization_results.json")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        return 1
    finally:
        await test.cleanup()
    
    return 0

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code) 