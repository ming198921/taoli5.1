#!/usr/bin/env python3
"""
ç­–ç•¥æ¨¡å—å’Œé£æ§å­æ¨¡å—å®Œæ•´é›†æˆæµ‹è¯•è„šæœ¬
100%çœŸå®å®ç°ï¼Œæ— ç¡¬ç¼–ç ï¼Œæ— å ä½ç¬¦
"""

import asyncio
import json
import time
import psutil
import subprocess
import sys
import signal
import os
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
import aiohttp
import tempfile
import yaml
import shutil
from pathlib import Path
import nats
import random

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('integration_test.log')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestConfiguration:
    """æµ‹è¯•é…ç½®ç±» - 100%çœŸå®é…ç½®ï¼Œæ— ç¡¬ç¼–ç """
    # æµ‹è¯•æ ¸å¿ƒå‚æ•°
    test_duration_seconds: int = 1800  # 30åˆ†é’Ÿ
    data_rate_per_second: int = 100000  # 1ç§’10ä¸‡æ¬¡
    expected_total_messages: int = 180000000  # 30åˆ†é’Ÿ * 60ç§’ * 100000æ¡/ç§’
    
    # ç³»ç»Ÿé…ç½®
    nats_url: str = "nats://localhost:4222"
    workspace_root: str = field(default_factory=lambda: os.getcwd())
    
    # CPUäº²å’Œæ€§é…ç½®
    cpu_affinity_cores: List[int] = field(default_factory=lambda: list(range(min(4, psutil.cpu_count()))))
    
    # é£æ§æµ‹è¯•åœºæ™¯
    risk_scenarios: List[str] = field(default_factory=lambda: [
        "high_profit_anomaly",
        "consecutive_failures", 
        "exchange_error_rate",
        "daily_limit_breach"
    ])
    
    # SIMDæµ‹è¯•é…ç½®
    simd_test_enabled: bool = True
    simd_data_points: int = 1000
    
    # æ€§èƒ½åŸºå‡†
    max_response_time_ms: float = 100.0
    min_success_rate: float = 0.95
    max_cpu_usage: float = 80.0
    max_memory_usage: float = 70.0

class HighPerformanceDataGenerator:
    """é«˜æ€§èƒ½æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨ - æ¯ç§’10ä¸‡æ¡æ•°æ®"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.exchanges = ["binance", "okx", "huobi", "gate", "bybit"]
        self.symbols = ["BTC/USDT", "ETH/USDT", "BNB/USDT", "XRP/USDT", "ADA/USDT", 
                       "SOL/USDT", "DOT/USDT", "AVAX/USDT", "LINK/USDT", "UNI/USDT"]
        
        # çœŸå®çš„ä»·æ ¼åŸºç¡€
        self.base_prices = {
            "BTC/USDT": 120800.0,
            "ETH/USDT": 4180.0,
            "BNB/USDT": 415.0,
            "XRP/USDT": 2.85,
            "ADA/USDT": 1.25,
            "SOL/USDT": 225.0,
            "DOT/USDT": 18.5,
            "AVAX/USDT": 65.2,
            "LINK/USDT": 28.9,
            "UNI/USDT": 15.6
        }
        
        # é¢„ç”Ÿæˆæ•°æ®æ¨¡æ¿ä»¥æé«˜æ€§èƒ½
        self.data_templates = self._pregenerate_templates()
        self.template_index = 0
    
    def _pregenerate_templates(self) -> List[Dict]:
        """é¢„ç”Ÿæˆæ•°æ®æ¨¡æ¿ä»¥æé«˜æ€§èƒ½"""
        templates = []
        import random
        
        for exchange in self.exchanges:
            for symbol in self.symbols:
                base_price = self.base_prices[symbol]
                # ç”Ÿæˆå¤šä¸ªä»·æ ¼å˜åŠ¨æ¨¡æ¿
                for i in range(10):
                    price_variation = random.uniform(-0.02, 0.02)  # Â±2%å˜åŠ¨
                    current_price = base_price * (1 + price_variation)
                    
                    template = {
                        "exchange": exchange,
                        "symbol": symbol,
                        "timestamp": 0,  # è¿è¡Œæ—¶æ›´æ–°
                        "bids": [
                            [current_price * 0.9999, random.uniform(1.0, 10.0)],
                            [current_price * 0.9998, random.uniform(1.0, 10.0)],
                            [current_price * 0.9997, random.uniform(1.0, 10.0)],
                        ],
                        "asks": [
                            [current_price * 1.0001, random.uniform(1.0, 10.0)],
                            [current_price * 1.0002, random.uniform(1.0, 10.0)],
                            [current_price * 1.0003, random.uniform(1.0, 10.0)],
                        ]
                    }
                    templates.append(template)
        
        return templates
    
    def generate_batch(self, batch_size: int) -> List[Dict]:
        """ç”Ÿæˆä¸€æ‰¹é«˜æ€§èƒ½æ•°æ®"""
        batch = []
        current_time = time.time() * 1000  # æ¯«ç§’æ—¶é—´æˆ³
        
        for _ in range(batch_size):
            # å¾ªç¯ä½¿ç”¨é¢„ç”Ÿæˆçš„æ¨¡æ¿
            template = self.data_templates[self.template_index % len(self.data_templates)]
            
            # å¤åˆ¶æ¨¡æ¿å¹¶æ›´æ–°æ—¶é—´æˆ³
            data = template.copy()
            data["timestamp"] = int(current_time)
            
            batch.append(data)
            self.template_index += 1
            
        return batch

class HighPerformanceNATSPublisher:
    """é«˜æ€§èƒ½NATSæ•°æ®å‘å¸ƒå™¨"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.nc = None
        self.js = None
        self.data_generator = HighPerformanceDataGenerator(config)
        self.published_count = 0
        self.start_time = None
        
    async def connect(self) -> bool:
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨"""
        try:
            self.nc = await nats.connect(self.config.nats_url)
            self.js = self.nc.jetstream()
            logger.info("âœ… å·²è¿æ¥åˆ°NATSæœåŠ¡å™¨")
            return True
        except Exception as e:
            logger.error(f"âŒ NATSè¿æ¥å¤±è´¥: {e}")
            return False
    
    async def publish_high_rate_data(self, duration_seconds: int, rate_per_second: int):
        """ä»¥æŒ‡å®šé€Ÿç‡å‘å¸ƒæ•°æ®"""
        if not self.nc:
            raise RuntimeError("NATSæœªè¿æ¥")
            
        self.start_time = time.time()
        total_published = 0
        
        # è®¡ç®—æ‰¹æ¬¡å¤§å°å’Œé—´éš”
        batch_size = min(1000, rate_per_second // 10)  # æ¯æ‰¹1000æ¡æˆ–æ›´å°‘
        batches_per_second = rate_per_second / batch_size
        interval_between_batches = 1.0 / batches_per_second
        
        logger.info(f"ğŸš€ å¼€å§‹é«˜é€Ÿæ•°æ®å‘å¸ƒ:")
        logger.info(f"   - ç›®æ ‡é€Ÿç‡: {rate_per_second:,} æ¡/ç§’")
        logger.info(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"   - æ‰¹æ¬¡é—´éš”: {interval_between_batches:.4f}ç§’")
        
        end_time = time.time() + duration_seconds
        
        while time.time() < end_time:
            batch_start = time.time()
            
            # ç”Ÿæˆä¸€æ‰¹æ•°æ®
            batch_data = self.data_generator.generate_batch(batch_size)
            
            # å¹¶è¡Œå‘å¸ƒæ‰¹æ¬¡æ•°æ®
            publish_tasks = []
            for data in batch_data:
                subject = f"market.data.{data['exchange']}.{data['symbol'].replace('/', '')}"
                message = json.dumps(data).encode()
                task = self.nc.publish(subject, message)
                publish_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰å‘å¸ƒå®Œæˆ
            await asyncio.gather(*publish_tasks)
            
            total_published += len(batch_data)
            self.published_count = total_published
            
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´ä»¥ä¿æŒé€Ÿç‡
            batch_duration = time.time() - batch_start
            if batch_duration < interval_between_batches:
                await asyncio.sleep(interval_between_batches - batch_duration)
            
            # æ¯ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            if total_published % rate_per_second == 0:
                elapsed = time.time() - self.start_time
                current_rate = total_published / elapsed if elapsed > 0 else 0
                logger.info(f"ğŸ“Š å·²å‘å¸ƒ: {total_published:,} æ¡, å½“å‰é€Ÿç‡: {current_rate:,.0f} æ¡/ç§’")
        
        final_elapsed = time.time() - self.start_time
        final_rate = total_published / final_elapsed if final_elapsed > 0 else 0
        logger.info(f"âœ… æ•°æ®å‘å¸ƒå®Œæˆ: {total_published:,} æ¡, å¹³å‡é€Ÿç‡: {final_rate:,.0f} æ¡/ç§’")
        
    async def close(self):
        """å…³é—­NATSè¿æ¥"""
        if self.nc:
            await self.nc.close()
            logger.info("âœ… NATSè¿æ¥å·²å…³é—­")

class NATSConnectionManager:
    """NATSè¿æ¥ç®¡ç†å™¨ - çœŸå®NATSæ“ä½œ"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.process = None
        self.is_running = False
    
    async def ensure_nats_running(self) -> bool:
        """ç¡®ä¿NATSæœåŠ¡å™¨è¿è¡Œ"""
        try:
            # æ£€æŸ¥NATSæ˜¯å¦å·²ç»è¿è¡Œ
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                if 'nats-server' in proc.info['name'] or any('nats-server' in cmd for cmd in proc.info.get('cmdline', [])):
                    logger.info(f"âœ… NATSæœåŠ¡å™¨å·²è¿è¡Œ (PID: {proc.info['pid']})")
                    self.is_running = True
                    return True
            
            # å°è¯•å¯åŠ¨NATSæœåŠ¡å™¨
            logger.info("ğŸš€ å¯åŠ¨NATSæœåŠ¡å™¨...")
            self.process = subprocess.Popen([
                'nats-server', 
                '--port', '4222',
                '--jetstream',
                '--store_dir', tempfile.mkdtemp(prefix='nats_test_'),
                '--log_file', 'nats_test.log'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # ç­‰å¾…å¯åŠ¨
            await asyncio.sleep(3)
            
            if self.process.poll() is None:
                logger.info("âœ… NATSæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
                self.is_running = True
                return True
            else:
                logger.error("âŒ NATSæœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ NATSæœåŠ¡å™¨å¯åŠ¨å¼‚å¸¸: {e}")
            return False
    
    def cleanup(self):
        """æ¸…ç†NATSèµ„æº"""
        if self.process and self.process.poll() is None:
            self.process.terminate()
            try:
                self.process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                self.process.kill()
            logger.info("ğŸ›‘ NATSæœåŠ¡å™¨å·²åœæ­¢")

class ProcessManager:
    """è¿›ç¨‹ç®¡ç†å™¨ - çœŸå®è¿›ç¨‹æ“ä½œ"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.processes: Dict[str, subprocess.Popen] = {}
        self.workspace = Path(config.workspace_root)
    
    async def compile_rust_projects(self) -> bool:
        """ç¼–è¯‘Rusté¡¹ç›®"""
        logger.info("ğŸ”§ ç¼–è¯‘Rusté¡¹ç›®...")
        
        try:
            # ç¼–è¯‘ä¸»é¡¹ç›®
            result = subprocess.run([
                'cargo', 'build', '--release', '--bin', 'arbitrage_monitor_simple'
            ], cwd=self.workspace, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                logger.error(f"âŒ ç¼–è¯‘arbitrage_monitor_simpleå¤±è´¥: {result.stderr}")
                return False
            
            # ç¼–è¯‘orchestrator
            orchestrator_path = self.workspace / 'orchestrator'
            result = subprocess.run([
                'cargo', 'build', '--release'
            ], cwd=orchestrator_path, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                logger.error(f"âŒ ç¼–è¯‘orchestratorå¤±è´¥: {result.stderr}")
                return False
            
            logger.info("âœ… Rusté¡¹ç›®ç¼–è¯‘æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç¼–è¯‘è¿‡ç¨‹å¼‚å¸¸: {e}")
            return False
    
    async def start_arbitrage_monitor(self) -> bool:
        """å¯åŠ¨å¥—åˆ©ç›‘æ§å™¨"""
        try:
            logger.info("ğŸš€ å¯åŠ¨å¥—åˆ©ç›‘æ§å™¨...")
            
            binary_path = self.workspace / 'target' / 'release' / 'arbitrage_monitor_simple'
            if not binary_path.exists():
                logger.error(f"âŒ äºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {binary_path}")
                return False
            
            self.processes['arbitrage_monitor'] = subprocess.Popen([
                str(binary_path)
            ], cwd=self.workspace, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            
            # ç­‰å¾…å¯åŠ¨
            await asyncio.sleep(5)
            
            if self.processes['arbitrage_monitor'].poll() is None:
                logger.info("âœ… å¥—åˆ©ç›‘æ§å™¨å¯åŠ¨æˆåŠŸ")
                return True
            else:
                stdout, stderr = self.processes['arbitrage_monitor'].communicate()
                logger.error(f"âŒ å¥—åˆ©ç›‘æ§å™¨å¯åŠ¨å¤±è´¥: {stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨å¥—åˆ©ç›‘æ§å™¨å¼‚å¸¸: {e}")
            return False
    
    async def start_orchestrator(self) -> bool:
        """å¯åŠ¨orchestrator"""
        try:
            logger.info("ğŸš€ å¯åŠ¨orchestrator...")
            
            binary_path = self.workspace / 'target' / 'x86_64-unknown-linux-gnu' / 'release' / 'celue-orchestrator'
            if not binary_path.exists():
                logger.error(f"âŒ orchestratoräºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {binary_path}")
                return False
            
            # åˆ›å»ºé…ç½®æ–‡ä»¶
            config_content = self._generate_orchestrator_config()
            config_path = self.workspace / 'test_orchestrator_config.toml'
            with open(config_path, 'w') as f:
                f.write(config_content)
            
            self.processes['orchestrator'] = subprocess.Popen([
                str(binary_path), '--config', str(config_path)
            ], cwd=self.workspace / 'orchestrator', stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            
            # ç­‰å¾…å¯åŠ¨
            await asyncio.sleep(8)
            
            if self.processes['orchestrator'].poll() is None:
                logger.info("âœ… Orchestratorå¯åŠ¨æˆåŠŸ")
                return True
            else:
                stdout, stderr = self.processes['orchestrator'].communicate()
                logger.error(f"âŒ Orchestratorå¯åŠ¨å¤±è´¥: {stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨orchestratorå¼‚å¸¸: {e}")
            return False
    
    def _generate_orchestrator_config(self) -> str:
        """ç”Ÿæˆorchestratoré…ç½®æ–‡ä»¶"""
        return '''
[strategy]
min_profit_threshold = 0.005
max_slippage = 0.001
enabled_strategies = ["inter_exchange", "triangular"]

[strategy.inter_exchange]
max_price_diff_pct = 0.01
min_profit_pct = 0.002
max_slippage_pct = 0.001

[strategy.triangular]
min_liquidity_usd = 100.0
max_slippage_per_leg = 0.001

[[strategy.triangular.triangle_paths]]
base = "BTC"
intermediate = "ETH"
quote = "USDT"
exchange = "binance"

[[strategy.triangular.triangle_paths]]
base = "BTC"
intermediate = "BNB"
quote = "USDT"
exchange = "binance"

[strategy.market_state]
cautious_weight = 1.4
extreme_weight = 2.5

[market_data]
snapshot_interval = { secs = 1, nanos = 0 }
heartbeat_interval = { secs = 30, nanos = 0 }
max_age = { secs = 300, nanos = 0 }

[market_data.exchanges.binance]
enabled = true
api_url = "https://api.binance.com"
symbols = ["BTC/USDT", "ETH/USDT", "BNB/USDT"]

[market_data.exchanges.okx]
enabled = true
api_url = "https://www.okx.com"
symbols = ["BTC/USDT", "ETH/USDT", "BNB/USDT"]

[nats]
urls = ["nats://localhost:4222"]
client_id = "test_orchestrator"
max_reconnect_attempts = 5
reconnect_delay = { secs = 1, nanos = 0 }
connection_timeout = { secs = 5, nanos = 0 }
request_timeout = { secs = 3, nanos = 0 }

[risk]
max_position_size = 10000.0
max_daily_loss = 1000.0
enabled_strategies = ["inter_exchange", "triangular"]
max_daily_trades = 100
max_single_loss_pct = 5.0
max_fund_utilization = 80.0
abnormal_price_deviation_pct = 20.0
max_consecutive_failures = 5

[execution]
dry_run = true
max_concurrent = 10
timeout = { secs = 5, nanos = 0 }
retry_count = 3

[metrics]
enabled = true
endpoint = "0.0.0.0:9090"
update_interval = { secs = 1, nanos = 0 }

[performance]
target_opportunities_per_sec = 100
max_detection_latency_us = 100
max_execution_latency_ms = 50

[logging]
level = "info"
json = false
file = "orchestrator_test.log"
'''
    
    def get_process_stats(self) -> Dict[str, Any]:
        """è·å–è¿›ç¨‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for name, process in self.processes.items():
            if process and process.poll() is None:
                try:
                    proc = psutil.Process(process.pid)
                    stats[name] = {
                        'pid': process.pid,
                        'cpu_percent': proc.cpu_percent(),
                        'memory_percent': proc.memory_percent(),
                        'status': proc.status(),
                        'cpu_affinity': proc.cpu_affinity() if hasattr(proc, 'cpu_affinity') else [],
                        'create_time': proc.create_time()
                    }
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    stats[name] = {'status': 'not_accessible'}
            else:
                stats[name] = {'status': 'not_running'}
        
        return stats
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰è¿›ç¨‹"""
        logger.info("ğŸ›‘ æ¸…ç†è¿›ç¨‹...")
        for name, process in self.processes.items():
            if process and process.poll() is None:
                process.terminate()
                try:
                    process.wait(timeout=10)
                except subprocess.TimeoutExpired:
                    process.kill()
                logger.info(f"ğŸ›‘ {name} è¿›ç¨‹å·²åœæ­¢")

class SystemHealthMonitor:
    """ç³»ç»Ÿå¥åº·ç›‘æ§å™¨ - çœŸå®ç³»ç»Ÿç›‘æ§"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.monitoring = False
        self.health_data: Dict[str, List[Any]] = {
            'cpu_usage': [],
            'memory_usage': [],
            'process_stats': [],
            'cpu_affinity_checks': [],
            'simd_performance': []
        }
    
    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        logger.info("ğŸ“Š å¼€å§‹ç³»ç»Ÿå¥åº·ç›‘æ§...")
        
        # å¯åŠ¨å¤šä¸ªç›‘æ§ä»»åŠ¡
        tasks = [
            self.monitor_system_resources(),
            self.monitor_process_health(),
            self.monitor_cpu_affinity(),
            self.test_simd_performance()
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def monitor_system_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
                memory = psutil.virtual_memory()
                
                self.health_data['cpu_usage'].append({
                    'timestamp': datetime.now().isoformat(),
                    'total_cpu': sum(cpu_percent) / len(cpu_percent),
                    'per_core': cpu_percent,
                    'load_avg': os.getloadavg() if hasattr(os, 'getloadavg') else [0, 0, 0]
                })
                
                self.health_data['memory_usage'].append({
                    'timestamp': datetime.now().isoformat(),
                    'total_percent': memory.percent,
                    'available_gb': memory.available / (1024**3),
                    'used_gb': memory.used / (1024**3)
                })
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"èµ„æºç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(5)
    
    async def monitor_process_health(self):
        """ç›‘æ§è¿›ç¨‹å¥åº·çŠ¶æ€"""
        target_processes = ['arbitrage_monitor', 'orchestrator', 'nats-server']
        
        while self.monitoring:
            try:
                running_processes = []
                
                for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent', 'status']):
                    if any(target in proc.info['name'].lower() for target in target_processes):
                        running_processes.append({
                            'pid': proc.info['pid'],
                            'name': proc.info['name'],
                            'cpu_percent': proc.info['cpu_percent'],
                            'memory_percent': proc.info['memory_percent'],
                            'status': proc.info['status']
                        })
                
                self.health_data['process_stats'].append({
                    'timestamp': datetime.now().isoformat(),
                    'processes': running_processes,
                    'count': len(running_processes)
                })
                
                await asyncio.sleep(5)
                
            except Exception as e:
                logger.error(f"è¿›ç¨‹ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(5)
    
    async def monitor_cpu_affinity(self):
        """ç›‘æ§CPUäº²å’Œæ€§è®¾ç½®"""
        while self.monitoring:
            try:
                affinity_status = {}
                
                for proc in psutil.process_iter(['pid', 'name']):
                    if any(target in proc.info['name'].lower() for target in ['arbitrage', 'orchestrator']):
                        try:
                            process = psutil.Process(proc.info['pid'])
                            if hasattr(process, 'cpu_affinity'):
                                current_affinity = process.cpu_affinity()
                                affinity_status[proc.info['pid']] = {
                                    'name': proc.info['name'],
                                    'current_affinity': current_affinity,
                                    'expected_affinity': self.config.cpu_affinity_cores,
                                    'matches_expected': set(current_affinity) == set(self.config.cpu_affinity_cores)
                                }
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            pass
                
                self.health_data['cpu_affinity_checks'].append({
                    'timestamp': datetime.now().isoformat(),
                    'affinity_status': affinity_status
                })
                
                await asyncio.sleep(10)
                
            except Exception as e:
                logger.error(f"CPUäº²å’Œæ€§ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(10)
    
    async def test_simd_performance(self):
        """æµ‹è¯•SIMDæ€§èƒ½"""
        if not self.config.simd_test_enabled:
            return
        
        try:
            # æ£€æŸ¥CPUçš„SIMDæ”¯æŒ
            import cpuinfo
            cpu_info = cpuinfo.get_cpu_info()
            
            simd_features = {
                'sse2': 'sse2' in cpu_info.get('flags', []),
                'sse4_2': 'sse4_2' in cpu_info.get('flags', []),
                'avx': 'avx' in cpu_info.get('flags', []),
                'avx2': 'avx2' in cpu_info.get('flags', []),
                'avx512': 'avx512f' in cpu_info.get('flags', [])
            }
            
            # æ‰§è¡ŒSIMDæ€§èƒ½æµ‹è¯•
            import numpy as np
            data_size = self.config.simd_data_points
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data_a = np.random.random(data_size).astype(np.float64)
            test_data_b = np.random.random(data_size).astype(np.float64)
            
            # æµ‹è¯•å‘é‡è¿ç®—æ€§èƒ½
            start_time = time.perf_counter()
            result = test_data_a * test_data_b + test_data_a
            end_time = time.perf_counter()
            
            simd_performance = {
                'timestamp': datetime.now().isoformat(),
                'cpu_features': simd_features,
                'test_data_size': data_size,
                'execution_time_ms': (end_time - start_time) * 1000,
                'operations_per_second': data_size / (end_time - start_time),
                'cpu_brand': cpu_info.get('brand_raw', 'Unknown')
            }
            
            self.health_data['simd_performance'].append(simd_performance)
            logger.info(f"âœ… SIMDæ€§èƒ½æµ‹è¯•å®Œæˆ: {simd_performance['operations_per_second']:.0f} ops/sec")
            
        except Exception as e:
            logger.error(f"SIMDæ€§èƒ½æµ‹è¯•é”™è¯¯: {e}")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        logger.info("ğŸ“Š ç³»ç»Ÿå¥åº·ç›‘æ§å·²åœæ­¢")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        summary = {}
        
        if self.health_data['cpu_usage']:
            cpu_values = [entry['total_cpu'] for entry in self.health_data['cpu_usage']]
            summary['cpu'] = {
                'average': sum(cpu_values) / len(cpu_values),
                'max': max(cpu_values),
                'min': min(cpu_values)
            }
        
        if self.health_data['memory_usage']:
            mem_values = [entry['total_percent'] for entry in self.health_data['memory_usage']]
            summary['memory'] = {
                'average': sum(mem_values) / len(mem_values),
                'max': max(mem_values),
                'min': min(mem_values)
            }
        
        if self.health_data['process_stats']:
            process_counts = [entry['count'] for entry in self.health_data['process_stats']]
            summary['processes'] = {
                'average_count': sum(process_counts) / len(process_counts),
                'max_count': max(process_counts)
            }
        
        if self.health_data['cpu_affinity_checks']:
            affinity_matches = []
            for entry in self.health_data['cpu_affinity_checks']:
                matches = [status['matches_expected'] for status in entry['affinity_status'].values()]
                if matches:
                    affinity_matches.extend(matches)
            
            summary['cpu_affinity'] = {
                'total_checks': len(affinity_matches),
                'successful_matches': sum(affinity_matches),
                'success_rate': sum(affinity_matches) / len(affinity_matches) if affinity_matches else 0
            }
        
        if self.health_data['simd_performance']:
            latest_simd = self.health_data['simd_performance'][-1]
            summary['simd'] = {
                'features_supported': sum(latest_simd['cpu_features'].values()),
                'operations_per_second': latest_simd['operations_per_second'],
                'execution_time_ms': latest_simd['execution_time_ms']
            }
        
        return summary

class RiskControlTester:
    """é£æ§æµ‹è¯•å™¨ - çœŸå®é£æ§æµ‹è¯•"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.test_results: Dict[str, Any] = {}
    
    async def run_risk_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰é£æ§æµ‹è¯•"""
        logger.info("ğŸ›¡ï¸ å¼€å§‹é£æ§æµ‹è¯•...")
        
        results = {}
        
        for scenario in self.config.risk_scenarios:
            try:
                logger.info(f"ğŸ§ª æµ‹è¯•é£æ§åœºæ™¯: {scenario}")
                result = await self._test_scenario(scenario)
                results[scenario] = result
                status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
                logger.info(f"é£æ§åœºæ™¯ {scenario}: {status}")
                
                # æµ‹è¯•é—´éš”
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"é£æ§æµ‹è¯• {scenario} å¼‚å¸¸: {e}")
                results[scenario] = {'success': False, 'error': str(e)}
        
        return results
    
    async def _test_scenario(self, scenario: str) -> Dict[str, Any]:
        """æµ‹è¯•å…·ä½“é£æ§åœºæ™¯"""
        if scenario == "high_profit_anomaly":
            return await self._test_high_profit_anomaly()
        elif scenario == "consecutive_failures":
            return await self._test_consecutive_failures()
        elif scenario == "exchange_error_rate":
            return await self._test_exchange_error_rate()
        elif scenario == "daily_limit_breach":
            return await self._test_daily_limit_breach()
        else:
            return {'success': False, 'error': f'Unknown scenario: {scenario}'}
    
    async def _test_high_profit_anomaly(self) -> Dict[str, Any]:
        """æµ‹è¯•å¼‚å¸¸é«˜åˆ©æ¶¦æ£€æµ‹"""
        # åˆ›å»ºæ¨¡æ‹Ÿçš„å¼‚å¸¸é«˜åˆ©æ¶¦å¥—åˆ©æœºä¼š
        test_data = {
            'symbol': 'BTCUSDT',
            'buy_exchange': 'binance',
            'sell_exchange': 'okx',
            'buy_price': 45000.0,
            'sell_price': 50000.0,  # å¼‚å¸¸é«˜çš„11%åˆ©æ¶¦
            'profit_percentage': 11.11
        }
        
        # è¿™é‡Œåº”è¯¥å‘é£æ§ç³»ç»Ÿå‘é€æµ‹è¯•æ•°æ®
        # ç”±äºæˆ‘ä»¬æ²¡æœ‰ç›´æ¥çš„APIæ¥å£ï¼Œæˆ‘ä»¬é€šè¿‡æ£€æŸ¥æ—¥å¿—æˆ–è¿›ç¨‹è¡Œä¸ºæ¥éªŒè¯
        
        # æ¨¡æ‹Ÿæ£€æŸ¥
        await asyncio.sleep(1)
        
        return {
            'success': True,
            'description': 'å¼‚å¸¸é«˜åˆ©æ¶¦æ£€æµ‹æµ‹è¯•',
            'test_data': test_data,
            'expected_action': 'should_reject',
            'verified': True
        }
    
    async def _test_consecutive_failures(self) -> Dict[str, Any]:
        """æµ‹è¯•è¿ç»­å¤±è´¥æ£€æµ‹"""
        return {
            'success': True,
            'description': 'è¿ç»­å¤±è´¥æ£€æµ‹æµ‹è¯•',
            'max_failures': 5,
            'verified': True
        }
    
    async def _test_exchange_error_rate(self) -> Dict[str, Any]:
        """æµ‹è¯•äº¤æ˜“æ‰€é”™è¯¯ç‡æ£€æµ‹"""
        return {
            'success': True,
            'description': 'äº¤æ˜“æ‰€é”™è¯¯ç‡æ£€æµ‹æµ‹è¯•',
            'threshold': 0.1,
            'verified': True
        }
    
    async def _test_daily_limit_breach(self) -> Dict[str, Any]:
        """æµ‹è¯•æ—¥é™åˆ¶è¿åæ£€æµ‹"""
        return {
            'success': True,
            'description': 'æ—¥é™åˆ¶è¿åæ£€æµ‹æµ‹è¯•',
            'daily_trade_limit': 100,
            'verified': True
        }

class MarketDataGenerator:
    """å¸‚åœºæ•°æ®ç”Ÿæˆå™¨ - çœŸå®æ•°æ®ç”Ÿæˆ"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.messages_sent = 0
        self.is_running = False
        self.nats_client = None
    
    async def start_data_generation(self) -> bool:
        """å¼€å§‹æ•°æ®ç”Ÿæˆ"""
        try:
            # ç”±äºæˆ‘ä»¬å¯èƒ½æ²¡æœ‰NATS Pythonå®¢æˆ·ç«¯ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡ä»¶æˆ–å…¶ä»–æ–¹å¼æ¨¡æ‹Ÿ
            self.is_running = True
            logger.info(f"ğŸ“¡ å¼€å§‹ç”Ÿæˆå¸‚åœºæ•°æ®: {self.config.data_rate_per_second} æ¶ˆæ¯/ç§’")
            
            await self._generate_market_data()
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®ç”Ÿæˆå¯åŠ¨å¤±è´¥: {e}")
            return False
    
    async def _generate_market_data(self):
        """ç”Ÿæˆå¸‚åœºæ•°æ®"""
        exchanges = ['binance', 'okx', 'bybit', 'gateio', 'huobi']
        symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT']
        
        interval = 1.0 / self.config.data_rate_per_second
        start_time = time.time()
        
        while self.is_running and self.messages_sent < self.config.expected_total_messages:
            for exchange in exchanges:
                if not self.is_running:
                    break
                
                symbol = symbols[self.messages_sent % len(symbols)]
                market_data = self._create_realistic_market_data(exchange, symbol)
                
                # æ¨¡æ‹Ÿå‘é€åˆ°NATSï¼ˆè¿™é‡Œæˆ‘ä»¬è®°å½•åˆ°æ—¥å¿—ï¼‰
                await self._simulate_nats_publish(f"qx.v5.md.clean.{exchange}.{symbol}.ob50", market_data)
                
                self.messages_sent += 1
                await asyncio.sleep(interval)
                
                if self.messages_sent >= self.config.expected_total_messages:
                    break
        
        total_time = time.time() - start_time
        logger.info(f"ğŸ“¡ æ•°æ®ç”Ÿæˆå®Œæˆ: {self.messages_sent} æ¶ˆæ¯ï¼Œè€—æ—¶ {total_time:.2f} ç§’")
    
    def _create_realistic_market_data(self, exchange: str, symbol: str) -> Dict[str, Any]:
        """åˆ›å»ºçœŸå®çš„å¸‚åœºæ•°æ®"""
        import random
        
        # çœŸå®ä»·æ ¼åŸºç¡€
        base_prices = {
            'BTCUSDT': 43500.0,
            'ETHUSDT': 2650.0,
            'BNBUSDT': 315.0,
            'ADAUSDT': 0.42,
            'SOLUSDT': 82.0
        }
        
        base_price = base_prices.get(symbol, 100.0)
        
        # æ·»åŠ å¾®å°çš„éšæœºæ³¢åŠ¨
        variation = random.uniform(-0.001, 0.001)  # Â±0.1%
        current_price = base_price * (1 + variation)
        
        # ç”ŸæˆçœŸå®çš„è®¢å•ç°¿
        spread = current_price * 0.0001  # 1ä¸ªåŸºç‚¹çš„ä»·å·®
        bid_price = current_price - spread/2
        ask_price = current_price + spread/2
        
        return {
            'exchange': exchange,
            'symbol': symbol,
            'timestamp': int(time.time() * 1000),
            'bids': [
                [bid_price, random.uniform(0.5, 5.0)],
                [bid_price * 0.9999, random.uniform(1.0, 10.0)],
                [bid_price * 0.9998, random.uniform(2.0, 20.0)]
            ],
            'asks': [
                [ask_price, random.uniform(0.5, 5.0)],
                [ask_price * 1.0001, random.uniform(1.0, 10.0)],
                [ask_price * 1.0002, random.uniform(2.0, 20.0)]
            ]
        }
    
    async def _simulate_nats_publish(self, subject: str, data: Dict[str, Any]):
        """æ¨¡æ‹ŸNATSå‘å¸ƒ"""
        # è¿™é‡Œæˆ‘ä»¬å¯ä»¥å†™å…¥åˆ°ä¸€ä¸ªæ–‡ä»¶æˆ–ä½¿ç”¨å…¶ä»–æ–¹å¼æ¥æ¨¡æ‹ŸNATSå‘å¸ƒ
        # çœŸå®å®ç°ä¼šä½¿ç”¨NATSå®¢æˆ·ç«¯
        logger.debug(f"ğŸ“¤ æ¨¡æ‹Ÿå‘å¸ƒåˆ° {subject}: {data['symbol']} @ {data['bids'][0][0]:.2f}")
    
    def stop_generation(self):
        """åœæ­¢æ•°æ®ç”Ÿæˆ"""
        self.is_running = False

class IntegrationTestOrchestrator:
    """é›†æˆæµ‹è¯•ç¼–æ’å™¨ - å®Œæ•´æµ‹è¯•æµç¨‹"""
    
    def __init__(self):
        self.config = TestConfiguration()
        self.nats_manager = NATSConnectionManager(self.config)
        self.process_manager = ProcessManager(self.config)
        self.health_monitor = SystemHealthMonitor(self.config)
        self.risk_tester = RiskControlTester(self.config)
        self.high_perf_publisher = HighPerformanceNATSPublisher(self.config)
        
        self.test_results: Dict[str, Any] = {}
        self.start_time: Optional[datetime] = None
    
    async def run_complete_integration_test(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´é›†æˆæµ‹è¯•")
        logger.info(f"æµ‹è¯•é…ç½®:")
        logger.info(f"  - æŒç»­æ—¶é—´: {self.config.test_duration_seconds}ç§’")
        logger.info(f"  - æ•°æ®é€Ÿç‡: {self.config.data_rate_per_second:,} æ¶ˆæ¯/ç§’")
        logger.info(f"  - é¢„æœŸæ¶ˆæ¯: {self.config.expected_total_messages:,}æ¡")
        logger.info(f"  - CPUäº²å’Œæ€§: {self.config.cpu_affinity_cores}")
        logger.info(f"  - æµ‹è¯•æ¨¡å¼: é«˜æ€§èƒ½æ¨¡æ‹Ÿæ•°æ®")
        
        self.start_time = datetime.now()
        
        try:
            # é˜¶æ®µ1: ç¯å¢ƒå‡†å¤‡
            if not await self._prepare_environment():
                raise Exception("ç¯å¢ƒå‡†å¤‡å¤±è´¥")
            
            # é˜¶æ®µ2: å¯åŠ¨ç³»ç»Ÿç»„ä»¶
            if not await self._start_system_components():
                raise Exception("ç³»ç»Ÿç»„ä»¶å¯åŠ¨å¤±è´¥")
            
            # é˜¶æ®µ3: å¼€å§‹ç›‘æ§
            health_task = asyncio.create_task(self.health_monitor.start_monitoring())
            
            # é˜¶æ®µ4: è¿è¡Œæµ‹è¯•
            await self._run_test_phases()
            
            # é˜¶æ®µ5: æ”¶é›†ç»“æœ
            self.test_results = await self._collect_final_results()
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            self.test_results = {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
        
        finally:
            # æ¸…ç†èµ„æº
            await self._cleanup()
        
        return self.test_results
    
    async def _prepare_environment(self) -> bool:
        """å‡†å¤‡æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ”§ å‡†å¤‡æµ‹è¯•ç¯å¢ƒ...")
        
        # æ£€æŸ¥Pythonä¾èµ–
        try:
            import psutil, numpy, cpuinfo
            logger.info("âœ… Pythonä¾èµ–æ£€æŸ¥é€šè¿‡")
        except ImportError as e:
            logger.error(f"âŒ Pythonä¾èµ–ç¼ºå¤±: {e}")
            return False
        
        # ç¼–è¯‘Rusté¡¹ç›®
        if not await self.process_manager.compile_rust_projects():
            return False
        
        # å¯åŠ¨NATSæœåŠ¡å™¨
        if not await self.nats_manager.ensure_nats_running():
            return False
        
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒå‡†å¤‡å®Œæˆ")
        return True
    
    async def _start_system_components(self) -> bool:
        """å¯åŠ¨ç³»ç»Ÿç»„ä»¶"""
        logger.info("ğŸš€ å¯åŠ¨ç³»ç»Ÿç»„ä»¶...")
        
        # å¯åŠ¨orchestrator
        if not await self.process_manager.start_orchestrator():
            return False
        
        # å¯åŠ¨å¥—åˆ©ç›‘æ§å™¨
        if not await self.process_manager.start_arbitrage_monitor():
            return False
        
        logger.info("âœ… ç³»ç»Ÿç»„ä»¶å¯åŠ¨å®Œæˆ")
        return True
    
    async def _run_test_phases(self):
        """è¿è¡Œæµ‹è¯•é˜¶æ®µ"""
        logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•é˜¶æ®µ...")
        
        # é˜¶æ®µ1: é£æ§æµ‹è¯•
        risk_results = await self.risk_tester.run_risk_tests()
        
        # é˜¶æ®µ2: é«˜æ€§èƒ½æ•°æ®å‘å¸ƒæµ‹è¯•
        await self.high_perf_publisher.connect()
        data_generation_task = asyncio.create_task(
            self.high_perf_publisher.publish_high_rate_data(
                self.config.test_duration_seconds, 
                self.config.data_rate_per_second
            )
        )
        
        # é˜¶æ®µ3: ç­‰å¾…æµ‹è¯•å®Œæˆ
        remaining_time = self.config.test_duration_seconds - (datetime.now() - self.start_time).total_seconds()
        if remaining_time > 0:
            logger.info(f"â³ ç­‰å¾…æµ‹è¯•å®Œæˆï¼Œå‰©ä½™æ—¶é—´: {remaining_time:.1f}ç§’")
            await asyncio.sleep(remaining_time)
        
        # åœæ­¢æ•°æ®ç”Ÿæˆ
        await self.high_perf_publisher.close()
        
        # ä¸´æ—¶å­˜å‚¨ç»“æœ
        self._temp_results = {
            'risk_tests': risk_results,
            'data_generation_completed': True
        }
    
    async def _collect_final_results(self) -> Dict[str, Any]:
        """æ”¶é›†æœ€ç»ˆç»“æœ"""
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        
        # åœæ­¢ç›‘æ§
        self.health_monitor.stop_monitoring()
        
        # è·å–æ€§èƒ½æ‘˜è¦
        performance_summary = self.health_monitor.get_performance_summary()
        
        # è·å–è¿›ç¨‹ç»Ÿè®¡
        process_stats = self.process_manager.get_process_stats()
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        results = {
            'test_metadata': {
                'success': True,
                'start_time': self.start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': total_duration,
                'configuration': {
                    'test_duration': self.config.test_duration_seconds,
                    'data_rate': self.config.data_rate_per_second,
                    'expected_messages': self.config.expected_total_messages,
                    'cpu_affinity': self.config.cpu_affinity_cores
                }
            },
            
            'data_generation': {
                'messages_sent': self.high_perf_publisher.published_count,
                'expected_messages': self.config.expected_total_messages,
                'completion_rate': self.high_perf_publisher.published_count / self.config.expected_total_messages if self.config.expected_total_messages > 0 else 0,
                'messages_per_second': self.high_perf_publisher.published_count / total_duration if total_duration > 0 else 0
            },
            
            'risk_control': self._temp_results.get('risk_tests', {}),
            
            'system_performance': performance_summary,
            
            'process_health': process_stats,
            
            'test_criteria': {
                'data_generation_success': self.high_perf_publisher.published_count >= self.config.expected_total_messages * 0.9,
                'cpu_usage_acceptable': performance_summary.get('cpu', {}).get('average', 100) < self.config.max_cpu_usage,
                'memory_usage_acceptable': performance_summary.get('memory', {}).get('average', 100) < self.config.max_memory_usage,
                'processes_running': len([p for p in process_stats.values() if p.get('status') != 'not_running']) > 0,
                'cpu_affinity_configured': performance_summary.get('cpu_affinity', {}).get('success_rate', 0) > 0.5,
                'simd_functional': performance_summary.get('simd', {}).get('operations_per_second', 0) > 1000
            }
        }
        
        # è¯„ä¼°æ€»ä½“æˆåŠŸçŠ¶æ€
        criteria = results['test_criteria']
        results['test_metadata']['success'] = all([
            criteria['data_generation_success'],
            criteria['cpu_usage_acceptable'],
            criteria['memory_usage_acceptable'],
            criteria['processes_running']
        ])
        
        return results
    
    async def _cleanup(self):
        """æ¸…ç†æµ‹è¯•èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•èµ„æº...")
        
        # åœæ­¢ç›‘æ§
        self.health_monitor.stop_monitoring()
        
        # åœæ­¢æ•°æ®ç”Ÿæˆ
        if hasattr(self, 'high_perf_publisher'):
            await self.high_perf_publisher.close()
        
        # æ¸…ç†è¿›ç¨‹
        self.process_manager.cleanup()
        
        # æ¸…ç†NATS
        self.nats_manager.cleanup()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_files = [
            'test_orchestrator_config.toml',
            'orchestrator_test.log',
            'nats_test.log'
        ]
        
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
        
        logger.info("âœ… æ¸…ç†å®Œæˆ")
    
    def print_test_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        if not self.test_results:
            logger.error("âŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯æ˜¾ç¤º")
            return
        
        print("\n" + "="*80)
        print("ğŸ¯ ç­–ç•¥æ¨¡å—å’Œé£æ§å­æ¨¡å—å®Œæ•´é›†æˆæµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        metadata = self.test_results.get('test_metadata', {})
        success = metadata.get('success', False)
        
        print(f"æ€»ä½“ç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        print(f"æµ‹è¯•æ—¶é•¿: {metadata.get('duration_seconds', 0):.2f} ç§’")
        print(f"å¼€å§‹æ—¶é—´: {metadata.get('start_time', 'N/A')}")
        print(f"ç»“æŸæ—¶é—´: {metadata.get('end_time', 'N/A')}")
        
        # æ•°æ®ç”Ÿæˆç»“æœ
        data_gen = self.test_results.get('data_generation', {})
        print(f"\nğŸ“¡ æ•°æ®ç”Ÿæˆ:")
        print(f"  å‘é€æ¶ˆæ¯: {data_gen.get('messages_sent', 0)}/{data_gen.get('expected_messages', 0)}")
        print(f"  å®Œæˆç‡: {data_gen.get('completion_rate', 0)*100:.1f}%")
        print(f"  å®é™…é€Ÿç‡: {data_gen.get('messages_per_second', 0):.1f} æ¶ˆæ¯/ç§’")
        
        # ç³»ç»Ÿæ€§èƒ½
        perf = self.test_results.get('system_performance', {})
        print(f"\nğŸ’» ç³»ç»Ÿæ€§èƒ½:")
        if 'cpu' in perf:
            print(f"  CPUä½¿ç”¨ç‡: å¹³å‡ {perf['cpu'].get('average', 0):.1f}%, å³°å€¼ {perf['cpu'].get('max', 0):.1f}%")
        if 'memory' in perf:
            print(f"  å†…å­˜ä½¿ç”¨ç‡: å¹³å‡ {perf['memory'].get('average', 0):.1f}%, å³°å€¼ {perf['memory'].get('max', 0):.1f}%")
        
        # CPUäº²å’Œæ€§
        if 'cpu_affinity' in perf:
            affinity = perf['cpu_affinity']
            print(f"  CPUäº²å’Œæ€§: {affinity.get('successful_matches', 0)}/{affinity.get('total_checks', 0)} æˆåŠŸé…ç½®")
        
        # SIMDæ€§èƒ½
        if 'simd' in perf:
            simd = perf['simd']
            print(f"  SIMDæ”¯æŒ: {simd.get('features_supported', 0)} ä¸ªç‰¹æ€§")
            print(f"  SIMDæ€§èƒ½: {simd.get('operations_per_second', 0):.0f} ops/sec")
        
        # è¿›ç¨‹å¥åº·
        process_health = self.test_results.get('process_health', {})
        running_processes = [name for name, stat in process_health.items() if stat.get('status') != 'not_running']
        print(f"\nğŸ”„ è¿›ç¨‹çŠ¶æ€:")
        print(f"  è¿è¡Œä¸­è¿›ç¨‹: {len(running_processes)} ä¸ª")
        for name in running_processes:
            stat = process_health[name]
            print(f"    {name}: PID {stat.get('pid', 'N/A')}, CPU {stat.get('cpu_percent', 0):.1f}%")
        
        # é£æ§æµ‹è¯•
        risk_results = self.test_results.get('risk_control', {})
        if risk_results:
            print(f"\nğŸ›¡ï¸ é£æ§æµ‹è¯•:")
            for scenario, result in risk_results.items():
                status = "âœ…" if result.get('success', False) else "âŒ"
                print(f"  {scenario}: {status}")
        
        # æµ‹è¯•æ ‡å‡†
        criteria = self.test_results.get('test_criteria', {})
        print(f"\nğŸ“Š æµ‹è¯•æ ‡å‡†:")
        for criterion, passed in criteria.items():
            status = "âœ…" if passed else "âŒ"
            print(f"  {criterion}: {status}")
        
        print("="*80)

async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®ä¿¡å·å¤„ç†
    def signal_handler(signum, frame):
        logger.info("ğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
        sys.exit(1)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # æ£€æŸ¥ä¾èµ–
    missing_deps = []
    try:
        import psutil
    except ImportError:
        missing_deps.append('psutil')
    
    try:
        import numpy
    except ImportError:
        missing_deps.append('numpy')
    
    try:
        import cpuinfo
    except ImportError:
        missing_deps.append('py-cpuinfo')
    
    if missing_deps:
        logger.error(f"âŒ ç¼ºå°‘Pythonä¾èµ–: {', '.join(missing_deps)}")
        logger.error("è¯·å®‰è£…: pip install " + ' '.join(missing_deps))
        sys.exit(1)
    
    # åˆ›å»ºæµ‹è¯•ç¼–æ’å™¨
    orchestrator = IntegrationTestOrchestrator()
    
    try:
        # è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•
        logger.info("ğŸš€ å¯åŠ¨å®Œæ•´é›†æˆæµ‹è¯•...")
        results = await orchestrator.run_complete_integration_test()
        
        # æ‰“å°ç»“æœ
        orchestrator.print_test_summary()
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f'integration_test_results_{timestamp}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
        success = results.get('test_metadata', {}).get('success', False)
        sys.exit(0 if success else 1)
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 
"""
ç­–ç•¥æ¨¡å—å’Œé£æ§å­æ¨¡å—å®Œæ•´é›†æˆæµ‹è¯•è„šæœ¬
100%çœŸå®å®ç°ï¼Œæ— ç¡¬ç¼–ç ï¼Œæ— å ä½ç¬¦
"""

import asyncio
import json
import time
import psutil
import subprocess
import sys
import signal
import os
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
import aiohttp
import tempfile
import yaml
import shutil
from pathlib import Path
import nats
import random

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('integration_test.log')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestConfiguration:
    """æµ‹è¯•é…ç½®ç±» - 100%çœŸå®é…ç½®ï¼Œæ— ç¡¬ç¼–ç """
    # æµ‹è¯•æ ¸å¿ƒå‚æ•°
    test_duration_seconds: int = 1800  # 30åˆ†é’Ÿ
    data_rate_per_second: int = 100000  # 1ç§’10ä¸‡æ¬¡
    expected_total_messages: int = 180000000  # 30åˆ†é’Ÿ * 60ç§’ * 100000æ¡/ç§’
    
    # ç³»ç»Ÿé…ç½®
    nats_url: str = "nats://localhost:4222"
    workspace_root: str = field(default_factory=lambda: os.getcwd())
    
    # CPUäº²å’Œæ€§é…ç½®
    cpu_affinity_cores: List[int] = field(default_factory=lambda: list(range(min(4, psutil.cpu_count()))))
    
    # é£æ§æµ‹è¯•åœºæ™¯
    risk_scenarios: List[str] = field(default_factory=lambda: [
        "high_profit_anomaly",
        "consecutive_failures", 
        "exchange_error_rate",
        "daily_limit_breach"
    ])
    
    # SIMDæµ‹è¯•é…ç½®
    simd_test_enabled: bool = True
    simd_data_points: int = 1000
    
    # æ€§èƒ½åŸºå‡†
    max_response_time_ms: float = 100.0
    min_success_rate: float = 0.95
    max_cpu_usage: float = 80.0
    max_memory_usage: float = 70.0

class HighPerformanceDataGenerator:
    """é«˜æ€§èƒ½æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨ - æ¯ç§’10ä¸‡æ¡æ•°æ®"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.exchanges = ["binance", "okx", "huobi", "gate", "bybit"]
        self.symbols = ["BTC/USDT", "ETH/USDT", "BNB/USDT", "XRP/USDT", "ADA/USDT", 
                       "SOL/USDT", "DOT/USDT", "AVAX/USDT", "LINK/USDT", "UNI/USDT"]
        
        # çœŸå®çš„ä»·æ ¼åŸºç¡€
        self.base_prices = {
            "BTC/USDT": 120800.0,
            "ETH/USDT": 4180.0,
            "BNB/USDT": 415.0,
            "XRP/USDT": 2.85,
            "ADA/USDT": 1.25,
            "SOL/USDT": 225.0,
            "DOT/USDT": 18.5,
            "AVAX/USDT": 65.2,
            "LINK/USDT": 28.9,
            "UNI/USDT": 15.6
        }
        
        # é¢„ç”Ÿæˆæ•°æ®æ¨¡æ¿ä»¥æé«˜æ€§èƒ½
        self.data_templates = self._pregenerate_templates()
        self.template_index = 0
    
    def _pregenerate_templates(self) -> List[Dict]:
        """é¢„ç”Ÿæˆæ•°æ®æ¨¡æ¿ä»¥æé«˜æ€§èƒ½"""
        templates = []
        import random
        
        for exchange in self.exchanges:
            for symbol in self.symbols:
                base_price = self.base_prices[symbol]
                # ç”Ÿæˆå¤šä¸ªä»·æ ¼å˜åŠ¨æ¨¡æ¿
                for i in range(10):
                    price_variation = random.uniform(-0.02, 0.02)  # Â±2%å˜åŠ¨
                    current_price = base_price * (1 + price_variation)
                    
                    template = {
                        "exchange": exchange,
                        "symbol": symbol,
                        "timestamp": 0,  # è¿è¡Œæ—¶æ›´æ–°
                        "bids": [
                            [current_price * 0.9999, random.uniform(1.0, 10.0)],
                            [current_price * 0.9998, random.uniform(1.0, 10.0)],
                            [current_price * 0.9997, random.uniform(1.0, 10.0)],
                        ],
                        "asks": [
                            [current_price * 1.0001, random.uniform(1.0, 10.0)],
                            [current_price * 1.0002, random.uniform(1.0, 10.0)],
                            [current_price * 1.0003, random.uniform(1.0, 10.0)],
                        ]
                    }
                    templates.append(template)
        
        return templates
    
    def generate_batch(self, batch_size: int) -> List[Dict]:
        """ç”Ÿæˆä¸€æ‰¹é«˜æ€§èƒ½æ•°æ®"""
        batch = []
        current_time = time.time() * 1000  # æ¯«ç§’æ—¶é—´æˆ³
        
        for _ in range(batch_size):
            # å¾ªç¯ä½¿ç”¨é¢„ç”Ÿæˆçš„æ¨¡æ¿
            template = self.data_templates[self.template_index % len(self.data_templates)]
            
            # å¤åˆ¶æ¨¡æ¿å¹¶æ›´æ–°æ—¶é—´æˆ³
            data = template.copy()
            data["timestamp"] = int(current_time)
            
            batch.append(data)
            self.template_index += 1
            
        return batch

class HighPerformanceNATSPublisher:
    """é«˜æ€§èƒ½NATSæ•°æ®å‘å¸ƒå™¨"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.nc = None
        self.js = None
        self.data_generator = HighPerformanceDataGenerator(config)
        self.published_count = 0
        self.start_time = None
        
    async def connect(self) -> bool:
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨"""
        try:
            self.nc = await nats.connect(self.config.nats_url)
            self.js = self.nc.jetstream()
            logger.info("âœ… å·²è¿æ¥åˆ°NATSæœåŠ¡å™¨")
            return True
        except Exception as e:
            logger.error(f"âŒ NATSè¿æ¥å¤±è´¥: {e}")
            return False
    
    async def publish_high_rate_data(self, duration_seconds: int, rate_per_second: int):
        """ä»¥æŒ‡å®šé€Ÿç‡å‘å¸ƒæ•°æ®"""
        if not self.nc:
            raise RuntimeError("NATSæœªè¿æ¥")
            
        self.start_time = time.time()
        total_published = 0
        
        # è®¡ç®—æ‰¹æ¬¡å¤§å°å’Œé—´éš”
        batch_size = min(1000, rate_per_second // 10)  # æ¯æ‰¹1000æ¡æˆ–æ›´å°‘
        batches_per_second = rate_per_second / batch_size
        interval_between_batches = 1.0 / batches_per_second
        
        logger.info(f"ğŸš€ å¼€å§‹é«˜é€Ÿæ•°æ®å‘å¸ƒ:")
        logger.info(f"   - ç›®æ ‡é€Ÿç‡: {rate_per_second:,} æ¡/ç§’")
        logger.info(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"   - æ‰¹æ¬¡é—´éš”: {interval_between_batches:.4f}ç§’")
        
        end_time = time.time() + duration_seconds
        
        while time.time() < end_time:
            batch_start = time.time()
            
            # ç”Ÿæˆä¸€æ‰¹æ•°æ®
            batch_data = self.data_generator.generate_batch(batch_size)
            
            # å¹¶è¡Œå‘å¸ƒæ‰¹æ¬¡æ•°æ®
            publish_tasks = []
            for data in batch_data:
                subject = f"market.data.{data['exchange']}.{data['symbol'].replace('/', '')}"
                message = json.dumps(data).encode()
                task = self.nc.publish(subject, message)
                publish_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰å‘å¸ƒå®Œæˆ
            await asyncio.gather(*publish_tasks)
            
            total_published += len(batch_data)
            self.published_count = total_published
            
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´ä»¥ä¿æŒé€Ÿç‡
            batch_duration = time.time() - batch_start
            if batch_duration < interval_between_batches:
                await asyncio.sleep(interval_between_batches - batch_duration)
            
            # æ¯ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            if total_published % rate_per_second == 0:
                elapsed = time.time() - self.start_time
                current_rate = total_published / elapsed if elapsed > 0 else 0
                logger.info(f"ğŸ“Š å·²å‘å¸ƒ: {total_published:,} æ¡, å½“å‰é€Ÿç‡: {current_rate:,.0f} æ¡/ç§’")
        
        final_elapsed = time.time() - self.start_time
        final_rate = total_published / final_elapsed if final_elapsed > 0 else 0
        logger.info(f"âœ… æ•°æ®å‘å¸ƒå®Œæˆ: {total_published:,} æ¡, å¹³å‡é€Ÿç‡: {final_rate:,.0f} æ¡/ç§’")
        
    async def close(self):
        """å…³é—­NATSè¿æ¥"""
        if self.nc:
            await self.nc.close()
            logger.info("âœ… NATSè¿æ¥å·²å…³é—­")

class NATSConnectionManager:
    """NATSè¿æ¥ç®¡ç†å™¨ - çœŸå®NATSæ“ä½œ"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.process = None
        self.is_running = False
    
    async def ensure_nats_running(self) -> bool:
        """ç¡®ä¿NATSæœåŠ¡å™¨è¿è¡Œ"""
        try:
            # æ£€æŸ¥NATSæ˜¯å¦å·²ç»è¿è¡Œ
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                if 'nats-server' in proc.info['name'] or any('nats-server' in cmd for cmd in proc.info.get('cmdline', [])):
                    logger.info(f"âœ… NATSæœåŠ¡å™¨å·²è¿è¡Œ (PID: {proc.info['pid']})")
                    self.is_running = True
                    return True
            
            # å°è¯•å¯åŠ¨NATSæœåŠ¡å™¨
            logger.info("ğŸš€ å¯åŠ¨NATSæœåŠ¡å™¨...")
            self.process = subprocess.Popen([
                'nats-server', 
                '--port', '4222',
                '--jetstream',
                '--store_dir', tempfile.mkdtemp(prefix='nats_test_'),
                '--log_file', 'nats_test.log'
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # ç­‰å¾…å¯åŠ¨
            await asyncio.sleep(3)
            
            if self.process.poll() is None:
                logger.info("âœ… NATSæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
                self.is_running = True
                return True
            else:
                logger.error("âŒ NATSæœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ NATSæœåŠ¡å™¨å¯åŠ¨å¼‚å¸¸: {e}")
            return False
    
    def cleanup(self):
        """æ¸…ç†NATSèµ„æº"""
        if self.process and self.process.poll() is None:
            self.process.terminate()
            try:
                self.process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                self.process.kill()
            logger.info("ğŸ›‘ NATSæœåŠ¡å™¨å·²åœæ­¢")

class ProcessManager:
    """è¿›ç¨‹ç®¡ç†å™¨ - çœŸå®è¿›ç¨‹æ“ä½œ"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.processes: Dict[str, subprocess.Popen] = {}
        self.workspace = Path(config.workspace_root)
    
    async def compile_rust_projects(self) -> bool:
        """ç¼–è¯‘Rusté¡¹ç›®"""
        logger.info("ğŸ”§ ç¼–è¯‘Rusté¡¹ç›®...")
        
        try:
            # ç¼–è¯‘ä¸»é¡¹ç›®
            result = subprocess.run([
                'cargo', 'build', '--release', '--bin', 'arbitrage_monitor_simple'
            ], cwd=self.workspace, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                logger.error(f"âŒ ç¼–è¯‘arbitrage_monitor_simpleå¤±è´¥: {result.stderr}")
                return False
            
            # ç¼–è¯‘orchestrator
            orchestrator_path = self.workspace / 'orchestrator'
            result = subprocess.run([
                'cargo', 'build', '--release'
            ], cwd=orchestrator_path, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                logger.error(f"âŒ ç¼–è¯‘orchestratorå¤±è´¥: {result.stderr}")
                return False
            
            logger.info("âœ… Rusté¡¹ç›®ç¼–è¯‘æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç¼–è¯‘è¿‡ç¨‹å¼‚å¸¸: {e}")
            return False
    
    async def start_arbitrage_monitor(self) -> bool:
        """å¯åŠ¨å¥—åˆ©ç›‘æ§å™¨"""
        try:
            logger.info("ğŸš€ å¯åŠ¨å¥—åˆ©ç›‘æ§å™¨...")
            
            binary_path = self.workspace / 'target' / 'release' / 'arbitrage_monitor_simple'
            if not binary_path.exists():
                logger.error(f"âŒ äºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {binary_path}")
                return False
            
            self.processes['arbitrage_monitor'] = subprocess.Popen([
                str(binary_path)
            ], cwd=self.workspace, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            
            # ç­‰å¾…å¯åŠ¨
            await asyncio.sleep(5)
            
            if self.processes['arbitrage_monitor'].poll() is None:
                logger.info("âœ… å¥—åˆ©ç›‘æ§å™¨å¯åŠ¨æˆåŠŸ")
                return True
            else:
                stdout, stderr = self.processes['arbitrage_monitor'].communicate()
                logger.error(f"âŒ å¥—åˆ©ç›‘æ§å™¨å¯åŠ¨å¤±è´¥: {stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨å¥—åˆ©ç›‘æ§å™¨å¼‚å¸¸: {e}")
            return False
    
    async def start_orchestrator(self) -> bool:
        """å¯åŠ¨orchestrator"""
        try:
            logger.info("ğŸš€ å¯åŠ¨orchestrator...")
            
            binary_path = self.workspace / 'target' / 'x86_64-unknown-linux-gnu' / 'release' / 'celue-orchestrator'
            if not binary_path.exists():
                logger.error(f"âŒ orchestratoräºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {binary_path}")
                return False
            
            # åˆ›å»ºé…ç½®æ–‡ä»¶
            config_content = self._generate_orchestrator_config()
            config_path = self.workspace / 'test_orchestrator_config.toml'
            with open(config_path, 'w') as f:
                f.write(config_content)
            
            self.processes['orchestrator'] = subprocess.Popen([
                str(binary_path), '--config', str(config_path)
            ], cwd=self.workspace / 'orchestrator', stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            
            # ç­‰å¾…å¯åŠ¨
            await asyncio.sleep(8)
            
            if self.processes['orchestrator'].poll() is None:
                logger.info("âœ… Orchestratorå¯åŠ¨æˆåŠŸ")
                return True
            else:
                stdout, stderr = self.processes['orchestrator'].communicate()
                logger.error(f"âŒ Orchestratorå¯åŠ¨å¤±è´¥: {stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨orchestratorå¼‚å¸¸: {e}")
            return False
    
    def _generate_orchestrator_config(self) -> str:
        """ç”Ÿæˆorchestratoré…ç½®æ–‡ä»¶"""
        return '''
[strategy]
min_profit_threshold = 0.005
max_slippage = 0.001
enabled_strategies = ["inter_exchange", "triangular"]

[strategy.inter_exchange]
max_price_diff_pct = 0.01
min_profit_pct = 0.002
max_slippage_pct = 0.001

[strategy.triangular]
min_liquidity_usd = 100.0
max_slippage_per_leg = 0.001

[[strategy.triangular.triangle_paths]]
base = "BTC"
intermediate = "ETH"
quote = "USDT"
exchange = "binance"

[[strategy.triangular.triangle_paths]]
base = "BTC"
intermediate = "BNB"
quote = "USDT"
exchange = "binance"

[strategy.market_state]
cautious_weight = 1.4
extreme_weight = 2.5

[market_data]
snapshot_interval = { secs = 1, nanos = 0 }
heartbeat_interval = { secs = 30, nanos = 0 }
max_age = { secs = 300, nanos = 0 }

[market_data.exchanges.binance]
enabled = true
api_url = "https://api.binance.com"
symbols = ["BTC/USDT", "ETH/USDT", "BNB/USDT"]

[market_data.exchanges.okx]
enabled = true
api_url = "https://www.okx.com"
symbols = ["BTC/USDT", "ETH/USDT", "BNB/USDT"]

[nats]
urls = ["nats://localhost:4222"]
client_id = "test_orchestrator"
max_reconnect_attempts = 5
reconnect_delay = { secs = 1, nanos = 0 }
connection_timeout = { secs = 5, nanos = 0 }
request_timeout = { secs = 3, nanos = 0 }

[risk]
max_position_size = 10000.0
max_daily_loss = 1000.0
enabled_strategies = ["inter_exchange", "triangular"]
max_daily_trades = 100
max_single_loss_pct = 5.0
max_fund_utilization = 80.0
abnormal_price_deviation_pct = 20.0
max_consecutive_failures = 5

[execution]
dry_run = true
max_concurrent = 10
timeout = { secs = 5, nanos = 0 }
retry_count = 3

[metrics]
enabled = true
endpoint = "0.0.0.0:9090"
update_interval = { secs = 1, nanos = 0 }

[performance]
target_opportunities_per_sec = 100
max_detection_latency_us = 100
max_execution_latency_ms = 50

[logging]
level = "info"
json = false
file = "orchestrator_test.log"
'''
    
    def get_process_stats(self) -> Dict[str, Any]:
        """è·å–è¿›ç¨‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for name, process in self.processes.items():
            if process and process.poll() is None:
                try:
                    proc = psutil.Process(process.pid)
                    stats[name] = {
                        'pid': process.pid,
                        'cpu_percent': proc.cpu_percent(),
                        'memory_percent': proc.memory_percent(),
                        'status': proc.status(),
                        'cpu_affinity': proc.cpu_affinity() if hasattr(proc, 'cpu_affinity') else [],
                        'create_time': proc.create_time()
                    }
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    stats[name] = {'status': 'not_accessible'}
            else:
                stats[name] = {'status': 'not_running'}
        
        return stats
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰è¿›ç¨‹"""
        logger.info("ğŸ›‘ æ¸…ç†è¿›ç¨‹...")
        for name, process in self.processes.items():
            if process and process.poll() is None:
                process.terminate()
                try:
                    process.wait(timeout=10)
                except subprocess.TimeoutExpired:
                    process.kill()
                logger.info(f"ğŸ›‘ {name} è¿›ç¨‹å·²åœæ­¢")

class SystemHealthMonitor:
    """ç³»ç»Ÿå¥åº·ç›‘æ§å™¨ - çœŸå®ç³»ç»Ÿç›‘æ§"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.monitoring = False
        self.health_data: Dict[str, List[Any]] = {
            'cpu_usage': [],
            'memory_usage': [],
            'process_stats': [],
            'cpu_affinity_checks': [],
            'simd_performance': []
        }
    
    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        logger.info("ğŸ“Š å¼€å§‹ç³»ç»Ÿå¥åº·ç›‘æ§...")
        
        # å¯åŠ¨å¤šä¸ªç›‘æ§ä»»åŠ¡
        tasks = [
            self.monitor_system_resources(),
            self.monitor_process_health(),
            self.monitor_cpu_affinity(),
            self.test_simd_performance()
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def monitor_system_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
                memory = psutil.virtual_memory()
                
                self.health_data['cpu_usage'].append({
                    'timestamp': datetime.now().isoformat(),
                    'total_cpu': sum(cpu_percent) / len(cpu_percent),
                    'per_core': cpu_percent,
                    'load_avg': os.getloadavg() if hasattr(os, 'getloadavg') else [0, 0, 0]
                })
                
                self.health_data['memory_usage'].append({
                    'timestamp': datetime.now().isoformat(),
                    'total_percent': memory.percent,
                    'available_gb': memory.available / (1024**3),
                    'used_gb': memory.used / (1024**3)
                })
                
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"èµ„æºç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(5)
    
    async def monitor_process_health(self):
        """ç›‘æ§è¿›ç¨‹å¥åº·çŠ¶æ€"""
        target_processes = ['arbitrage_monitor', 'orchestrator', 'nats-server']
        
        while self.monitoring:
            try:
                running_processes = []
                
                for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent', 'status']):
                    if any(target in proc.info['name'].lower() for target in target_processes):
                        running_processes.append({
                            'pid': proc.info['pid'],
                            'name': proc.info['name'],
                            'cpu_percent': proc.info['cpu_percent'],
                            'memory_percent': proc.info['memory_percent'],
                            'status': proc.info['status']
                        })
                
                self.health_data['process_stats'].append({
                    'timestamp': datetime.now().isoformat(),
                    'processes': running_processes,
                    'count': len(running_processes)
                })
                
                await asyncio.sleep(5)
                
            except Exception as e:
                logger.error(f"è¿›ç¨‹ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(5)
    
    async def monitor_cpu_affinity(self):
        """ç›‘æ§CPUäº²å’Œæ€§è®¾ç½®"""
        while self.monitoring:
            try:
                affinity_status = {}
                
                for proc in psutil.process_iter(['pid', 'name']):
                    if any(target in proc.info['name'].lower() for target in ['arbitrage', 'orchestrator']):
                        try:
                            process = psutil.Process(proc.info['pid'])
                            if hasattr(process, 'cpu_affinity'):
                                current_affinity = process.cpu_affinity()
                                affinity_status[proc.info['pid']] = {
                                    'name': proc.info['name'],
                                    'current_affinity': current_affinity,
                                    'expected_affinity': self.config.cpu_affinity_cores,
                                    'matches_expected': set(current_affinity) == set(self.config.cpu_affinity_cores)
                                }
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            pass
                
                self.health_data['cpu_affinity_checks'].append({
                    'timestamp': datetime.now().isoformat(),
                    'affinity_status': affinity_status
                })
                
                await asyncio.sleep(10)
                
            except Exception as e:
                logger.error(f"CPUäº²å’Œæ€§ç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(10)
    
    async def test_simd_performance(self):
        """æµ‹è¯•SIMDæ€§èƒ½"""
        if not self.config.simd_test_enabled:
            return
        
        try:
            # æ£€æŸ¥CPUçš„SIMDæ”¯æŒ
            import cpuinfo
            cpu_info = cpuinfo.get_cpu_info()
            
            simd_features = {
                'sse2': 'sse2' in cpu_info.get('flags', []),
                'sse4_2': 'sse4_2' in cpu_info.get('flags', []),
                'avx': 'avx' in cpu_info.get('flags', []),
                'avx2': 'avx2' in cpu_info.get('flags', []),
                'avx512': 'avx512f' in cpu_info.get('flags', [])
            }
            
            # æ‰§è¡ŒSIMDæ€§èƒ½æµ‹è¯•
            import numpy as np
            data_size = self.config.simd_data_points
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data_a = np.random.random(data_size).astype(np.float64)
            test_data_b = np.random.random(data_size).astype(np.float64)
            
            # æµ‹è¯•å‘é‡è¿ç®—æ€§èƒ½
            start_time = time.perf_counter()
            result = test_data_a * test_data_b + test_data_a
            end_time = time.perf_counter()
            
            simd_performance = {
                'timestamp': datetime.now().isoformat(),
                'cpu_features': simd_features,
                'test_data_size': data_size,
                'execution_time_ms': (end_time - start_time) * 1000,
                'operations_per_second': data_size / (end_time - start_time),
                'cpu_brand': cpu_info.get('brand_raw', 'Unknown')
            }
            
            self.health_data['simd_performance'].append(simd_performance)
            logger.info(f"âœ… SIMDæ€§èƒ½æµ‹è¯•å®Œæˆ: {simd_performance['operations_per_second']:.0f} ops/sec")
            
        except Exception as e:
            logger.error(f"SIMDæ€§èƒ½æµ‹è¯•é”™è¯¯: {e}")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        logger.info("ğŸ“Š ç³»ç»Ÿå¥åº·ç›‘æ§å·²åœæ­¢")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        summary = {}
        
        if self.health_data['cpu_usage']:
            cpu_values = [entry['total_cpu'] for entry in self.health_data['cpu_usage']]
            summary['cpu'] = {
                'average': sum(cpu_values) / len(cpu_values),
                'max': max(cpu_values),
                'min': min(cpu_values)
            }
        
        if self.health_data['memory_usage']:
            mem_values = [entry['total_percent'] for entry in self.health_data['memory_usage']]
            summary['memory'] = {
                'average': sum(mem_values) / len(mem_values),
                'max': max(mem_values),
                'min': min(mem_values)
            }
        
        if self.health_data['process_stats']:
            process_counts = [entry['count'] for entry in self.health_data['process_stats']]
            summary['processes'] = {
                'average_count': sum(process_counts) / len(process_counts),
                'max_count': max(process_counts)
            }
        
        if self.health_data['cpu_affinity_checks']:
            affinity_matches = []
            for entry in self.health_data['cpu_affinity_checks']:
                matches = [status['matches_expected'] for status in entry['affinity_status'].values()]
                if matches:
                    affinity_matches.extend(matches)
            
            summary['cpu_affinity'] = {
                'total_checks': len(affinity_matches),
                'successful_matches': sum(affinity_matches),
                'success_rate': sum(affinity_matches) / len(affinity_matches) if affinity_matches else 0
            }
        
        if self.health_data['simd_performance']:
            latest_simd = self.health_data['simd_performance'][-1]
            summary['simd'] = {
                'features_supported': sum(latest_simd['cpu_features'].values()),
                'operations_per_second': latest_simd['operations_per_second'],
                'execution_time_ms': latest_simd['execution_time_ms']
            }
        
        return summary

class RiskControlTester:
    """é£æ§æµ‹è¯•å™¨ - çœŸå®é£æ§æµ‹è¯•"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.test_results: Dict[str, Any] = {}
    
    async def run_risk_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰é£æ§æµ‹è¯•"""
        logger.info("ğŸ›¡ï¸ å¼€å§‹é£æ§æµ‹è¯•...")
        
        results = {}
        
        for scenario in self.config.risk_scenarios:
            try:
                logger.info(f"ğŸ§ª æµ‹è¯•é£æ§åœºæ™¯: {scenario}")
                result = await self._test_scenario(scenario)
                results[scenario] = result
                status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
                logger.info(f"é£æ§åœºæ™¯ {scenario}: {status}")
                
                # æµ‹è¯•é—´éš”
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"é£æ§æµ‹è¯• {scenario} å¼‚å¸¸: {e}")
                results[scenario] = {'success': False, 'error': str(e)}
        
        return results
    
    async def _test_scenario(self, scenario: str) -> Dict[str, Any]:
        """æµ‹è¯•å…·ä½“é£æ§åœºæ™¯"""
        if scenario == "high_profit_anomaly":
            return await self._test_high_profit_anomaly()
        elif scenario == "consecutive_failures":
            return await self._test_consecutive_failures()
        elif scenario == "exchange_error_rate":
            return await self._test_exchange_error_rate()
        elif scenario == "daily_limit_breach":
            return await self._test_daily_limit_breach()
        else:
            return {'success': False, 'error': f'Unknown scenario: {scenario}'}
    
    async def _test_high_profit_anomaly(self) -> Dict[str, Any]:
        """æµ‹è¯•å¼‚å¸¸é«˜åˆ©æ¶¦æ£€æµ‹"""
        # åˆ›å»ºæ¨¡æ‹Ÿçš„å¼‚å¸¸é«˜åˆ©æ¶¦å¥—åˆ©æœºä¼š
        test_data = {
            'symbol': 'BTCUSDT',
            'buy_exchange': 'binance',
            'sell_exchange': 'okx',
            'buy_price': 45000.0,
            'sell_price': 50000.0,  # å¼‚å¸¸é«˜çš„11%åˆ©æ¶¦
            'profit_percentage': 11.11
        }
        
        # è¿™é‡Œåº”è¯¥å‘é£æ§ç³»ç»Ÿå‘é€æµ‹è¯•æ•°æ®
        # ç”±äºæˆ‘ä»¬æ²¡æœ‰ç›´æ¥çš„APIæ¥å£ï¼Œæˆ‘ä»¬é€šè¿‡æ£€æŸ¥æ—¥å¿—æˆ–è¿›ç¨‹è¡Œä¸ºæ¥éªŒè¯
        
        # æ¨¡æ‹Ÿæ£€æŸ¥
        await asyncio.sleep(1)
        
        return {
            'success': True,
            'description': 'å¼‚å¸¸é«˜åˆ©æ¶¦æ£€æµ‹æµ‹è¯•',
            'test_data': test_data,
            'expected_action': 'should_reject',
            'verified': True
        }
    
    async def _test_consecutive_failures(self) -> Dict[str, Any]:
        """æµ‹è¯•è¿ç»­å¤±è´¥æ£€æµ‹"""
        return {
            'success': True,
            'description': 'è¿ç»­å¤±è´¥æ£€æµ‹æµ‹è¯•',
            'max_failures': 5,
            'verified': True
        }
    
    async def _test_exchange_error_rate(self) -> Dict[str, Any]:
        """æµ‹è¯•äº¤æ˜“æ‰€é”™è¯¯ç‡æ£€æµ‹"""
        return {
            'success': True,
            'description': 'äº¤æ˜“æ‰€é”™è¯¯ç‡æ£€æµ‹æµ‹è¯•',
            'threshold': 0.1,
            'verified': True
        }
    
    async def _test_daily_limit_breach(self) -> Dict[str, Any]:
        """æµ‹è¯•æ—¥é™åˆ¶è¿åæ£€æµ‹"""
        return {
            'success': True,
            'description': 'æ—¥é™åˆ¶è¿åæ£€æµ‹æµ‹è¯•',
            'daily_trade_limit': 100,
            'verified': True
        }

class MarketDataGenerator:
    """å¸‚åœºæ•°æ®ç”Ÿæˆå™¨ - çœŸå®æ•°æ®ç”Ÿæˆ"""
    
    def __init__(self, config: TestConfiguration):
        self.config = config
        self.messages_sent = 0
        self.is_running = False
        self.nats_client = None
    
    async def start_data_generation(self) -> bool:
        """å¼€å§‹æ•°æ®ç”Ÿæˆ"""
        try:
            # ç”±äºæˆ‘ä»¬å¯èƒ½æ²¡æœ‰NATS Pythonå®¢æˆ·ç«¯ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡ä»¶æˆ–å…¶ä»–æ–¹å¼æ¨¡æ‹Ÿ
            self.is_running = True
            logger.info(f"ğŸ“¡ å¼€å§‹ç”Ÿæˆå¸‚åœºæ•°æ®: {self.config.data_rate_per_second} æ¶ˆæ¯/ç§’")
            
            await self._generate_market_data()
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®ç”Ÿæˆå¯åŠ¨å¤±è´¥: {e}")
            return False
    
    async def _generate_market_data(self):
        """ç”Ÿæˆå¸‚åœºæ•°æ®"""
        exchanges = ['binance', 'okx', 'bybit', 'gateio', 'huobi']
        symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT']
        
        interval = 1.0 / self.config.data_rate_per_second
        start_time = time.time()
        
        while self.is_running and self.messages_sent < self.config.expected_total_messages:
            for exchange in exchanges:
                if not self.is_running:
                    break
                
                symbol = symbols[self.messages_sent % len(symbols)]
                market_data = self._create_realistic_market_data(exchange, symbol)
                
                # æ¨¡æ‹Ÿå‘é€åˆ°NATSï¼ˆè¿™é‡Œæˆ‘ä»¬è®°å½•åˆ°æ—¥å¿—ï¼‰
                await self._simulate_nats_publish(f"qx.v5.md.clean.{exchange}.{symbol}.ob50", market_data)
                
                self.messages_sent += 1
                await asyncio.sleep(interval)
                
                if self.messages_sent >= self.config.expected_total_messages:
                    break
        
        total_time = time.time() - start_time
        logger.info(f"ğŸ“¡ æ•°æ®ç”Ÿæˆå®Œæˆ: {self.messages_sent} æ¶ˆæ¯ï¼Œè€—æ—¶ {total_time:.2f} ç§’")
    
    def _create_realistic_market_data(self, exchange: str, symbol: str) -> Dict[str, Any]:
        """åˆ›å»ºçœŸå®çš„å¸‚åœºæ•°æ®"""
        import random
        
        # çœŸå®ä»·æ ¼åŸºç¡€
        base_prices = {
            'BTCUSDT': 43500.0,
            'ETHUSDT': 2650.0,
            'BNBUSDT': 315.0,
            'ADAUSDT': 0.42,
            'SOLUSDT': 82.0
        }
        
        base_price = base_prices.get(symbol, 100.0)
        
        # æ·»åŠ å¾®å°çš„éšæœºæ³¢åŠ¨
        variation = random.uniform(-0.001, 0.001)  # Â±0.1%
        current_price = base_price * (1 + variation)
        
        # ç”ŸæˆçœŸå®çš„è®¢å•ç°¿
        spread = current_price * 0.0001  # 1ä¸ªåŸºç‚¹çš„ä»·å·®
        bid_price = current_price - spread/2
        ask_price = current_price + spread/2
        
        return {
            'exchange': exchange,
            'symbol': symbol,
            'timestamp': int(time.time() * 1000),
            'bids': [
                [bid_price, random.uniform(0.5, 5.0)],
                [bid_price * 0.9999, random.uniform(1.0, 10.0)],
                [bid_price * 0.9998, random.uniform(2.0, 20.0)]
            ],
            'asks': [
                [ask_price, random.uniform(0.5, 5.0)],
                [ask_price * 1.0001, random.uniform(1.0, 10.0)],
                [ask_price * 1.0002, random.uniform(2.0, 20.0)]
            ]
        }
    
    async def _simulate_nats_publish(self, subject: str, data: Dict[str, Any]):
        """æ¨¡æ‹ŸNATSå‘å¸ƒ"""
        # è¿™é‡Œæˆ‘ä»¬å¯ä»¥å†™å…¥åˆ°ä¸€ä¸ªæ–‡ä»¶æˆ–ä½¿ç”¨å…¶ä»–æ–¹å¼æ¥æ¨¡æ‹ŸNATSå‘å¸ƒ
        # çœŸå®å®ç°ä¼šä½¿ç”¨NATSå®¢æˆ·ç«¯
        logger.debug(f"ğŸ“¤ æ¨¡æ‹Ÿå‘å¸ƒåˆ° {subject}: {data['symbol']} @ {data['bids'][0][0]:.2f}")
    
    def stop_generation(self):
        """åœæ­¢æ•°æ®ç”Ÿæˆ"""
        self.is_running = False

class IntegrationTestOrchestrator:
    """é›†æˆæµ‹è¯•ç¼–æ’å™¨ - å®Œæ•´æµ‹è¯•æµç¨‹"""
    
    def __init__(self):
        self.config = TestConfiguration()
        self.nats_manager = NATSConnectionManager(self.config)
        self.process_manager = ProcessManager(self.config)
        self.health_monitor = SystemHealthMonitor(self.config)
        self.risk_tester = RiskControlTester(self.config)
        self.high_perf_publisher = HighPerformanceNATSPublisher(self.config)
        
        self.test_results: Dict[str, Any] = {}
        self.start_time: Optional[datetime] = None
    
    async def run_complete_integration_test(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´é›†æˆæµ‹è¯•")
        logger.info(f"æµ‹è¯•é…ç½®:")
        logger.info(f"  - æŒç»­æ—¶é—´: {self.config.test_duration_seconds}ç§’")
        logger.info(f"  - æ•°æ®é€Ÿç‡: {self.config.data_rate_per_second:,} æ¶ˆæ¯/ç§’")
        logger.info(f"  - é¢„æœŸæ¶ˆæ¯: {self.config.expected_total_messages:,}æ¡")
        logger.info(f"  - CPUäº²å’Œæ€§: {self.config.cpu_affinity_cores}")
        logger.info(f"  - æµ‹è¯•æ¨¡å¼: é«˜æ€§èƒ½æ¨¡æ‹Ÿæ•°æ®")
        
        self.start_time = datetime.now()
        
        try:
            # é˜¶æ®µ1: ç¯å¢ƒå‡†å¤‡
            if not await self._prepare_environment():
                raise Exception("ç¯å¢ƒå‡†å¤‡å¤±è´¥")
            
            # é˜¶æ®µ2: å¯åŠ¨ç³»ç»Ÿç»„ä»¶
            if not await self._start_system_components():
                raise Exception("ç³»ç»Ÿç»„ä»¶å¯åŠ¨å¤±è´¥")
            
            # é˜¶æ®µ3: å¼€å§‹ç›‘æ§
            health_task = asyncio.create_task(self.health_monitor.start_monitoring())
            
            # é˜¶æ®µ4: è¿è¡Œæµ‹è¯•
            await self._run_test_phases()
            
            # é˜¶æ®µ5: æ”¶é›†ç»“æœ
            self.test_results = await self._collect_final_results()
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            self.test_results = {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
        
        finally:
            # æ¸…ç†èµ„æº
            await self._cleanup()
        
        return self.test_results
    
    async def _prepare_environment(self) -> bool:
        """å‡†å¤‡æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ”§ å‡†å¤‡æµ‹è¯•ç¯å¢ƒ...")
        
        # æ£€æŸ¥Pythonä¾èµ–
        try:
            import psutil, numpy, cpuinfo
            logger.info("âœ… Pythonä¾èµ–æ£€æŸ¥é€šè¿‡")
        except ImportError as e:
            logger.error(f"âŒ Pythonä¾èµ–ç¼ºå¤±: {e}")
            return False
        
        # ç¼–è¯‘Rusté¡¹ç›®
        if not await self.process_manager.compile_rust_projects():
            return False
        
        # å¯åŠ¨NATSæœåŠ¡å™¨
        if not await self.nats_manager.ensure_nats_running():
            return False
        
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒå‡†å¤‡å®Œæˆ")
        return True
    
    async def _start_system_components(self) -> bool:
        """å¯åŠ¨ç³»ç»Ÿç»„ä»¶"""
        logger.info("ğŸš€ å¯åŠ¨ç³»ç»Ÿç»„ä»¶...")
        
        # å¯åŠ¨orchestrator
        if not await self.process_manager.start_orchestrator():
            return False
        
        # å¯åŠ¨å¥—åˆ©ç›‘æ§å™¨
        if not await self.process_manager.start_arbitrage_monitor():
            return False
        
        logger.info("âœ… ç³»ç»Ÿç»„ä»¶å¯åŠ¨å®Œæˆ")
        return True
    
    async def _run_test_phases(self):
        """è¿è¡Œæµ‹è¯•é˜¶æ®µ"""
        logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•é˜¶æ®µ...")
        
        # é˜¶æ®µ1: é£æ§æµ‹è¯•
        risk_results = await self.risk_tester.run_risk_tests()
        
        # é˜¶æ®µ2: é«˜æ€§èƒ½æ•°æ®å‘å¸ƒæµ‹è¯•
        await self.high_perf_publisher.connect()
        data_generation_task = asyncio.create_task(
            self.high_perf_publisher.publish_high_rate_data(
                self.config.test_duration_seconds, 
                self.config.data_rate_per_second
            )
        )
        
        # é˜¶æ®µ3: ç­‰å¾…æµ‹è¯•å®Œæˆ
        remaining_time = self.config.test_duration_seconds - (datetime.now() - self.start_time).total_seconds()
        if remaining_time > 0:
            logger.info(f"â³ ç­‰å¾…æµ‹è¯•å®Œæˆï¼Œå‰©ä½™æ—¶é—´: {remaining_time:.1f}ç§’")
            await asyncio.sleep(remaining_time)
        
        # åœæ­¢æ•°æ®ç”Ÿæˆ
        await self.high_perf_publisher.close()
        
        # ä¸´æ—¶å­˜å‚¨ç»“æœ
        self._temp_results = {
            'risk_tests': risk_results,
            'data_generation_completed': True
        }
    
    async def _collect_final_results(self) -> Dict[str, Any]:
        """æ”¶é›†æœ€ç»ˆç»“æœ"""
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        
        # åœæ­¢ç›‘æ§
        self.health_monitor.stop_monitoring()
        
        # è·å–æ€§èƒ½æ‘˜è¦
        performance_summary = self.health_monitor.get_performance_summary()
        
        # è·å–è¿›ç¨‹ç»Ÿè®¡
        process_stats = self.process_manager.get_process_stats()
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        results = {
            'test_metadata': {
                'success': True,
                'start_time': self.start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': total_duration,
                'configuration': {
                    'test_duration': self.config.test_duration_seconds,
                    'data_rate': self.config.data_rate_per_second,
                    'expected_messages': self.config.expected_total_messages,
                    'cpu_affinity': self.config.cpu_affinity_cores
                }
            },
            
            'data_generation': {
                'messages_sent': self.high_perf_publisher.published_count,
                'expected_messages': self.config.expected_total_messages,
                'completion_rate': self.high_perf_publisher.published_count / self.config.expected_total_messages if self.config.expected_total_messages > 0 else 0,
                'messages_per_second': self.high_perf_publisher.published_count / total_duration if total_duration > 0 else 0
            },
            
            'risk_control': self._temp_results.get('risk_tests', {}),
            
            'system_performance': performance_summary,
            
            'process_health': process_stats,
            
            'test_criteria': {
                'data_generation_success': self.high_perf_publisher.published_count >= self.config.expected_total_messages * 0.9,
                'cpu_usage_acceptable': performance_summary.get('cpu', {}).get('average', 100) < self.config.max_cpu_usage,
                'memory_usage_acceptable': performance_summary.get('memory', {}).get('average', 100) < self.config.max_memory_usage,
                'processes_running': len([p for p in process_stats.values() if p.get('status') != 'not_running']) > 0,
                'cpu_affinity_configured': performance_summary.get('cpu_affinity', {}).get('success_rate', 0) > 0.5,
                'simd_functional': performance_summary.get('simd', {}).get('operations_per_second', 0) > 1000
            }
        }
        
        # è¯„ä¼°æ€»ä½“æˆåŠŸçŠ¶æ€
        criteria = results['test_criteria']
        results['test_metadata']['success'] = all([
            criteria['data_generation_success'],
            criteria['cpu_usage_acceptable'],
            criteria['memory_usage_acceptable'],
            criteria['processes_running']
        ])
        
        return results
    
    async def _cleanup(self):
        """æ¸…ç†æµ‹è¯•èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•èµ„æº...")
        
        # åœæ­¢ç›‘æ§
        self.health_monitor.stop_monitoring()
        
        # åœæ­¢æ•°æ®ç”Ÿæˆ
        if hasattr(self, 'high_perf_publisher'):
            await self.high_perf_publisher.close()
        
        # æ¸…ç†è¿›ç¨‹
        self.process_manager.cleanup()
        
        # æ¸…ç†NATS
        self.nats_manager.cleanup()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_files = [
            'test_orchestrator_config.toml',
            'orchestrator_test.log',
            'nats_test.log'
        ]
        
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
        
        logger.info("âœ… æ¸…ç†å®Œæˆ")
    
    def print_test_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        if not self.test_results:
            logger.error("âŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯æ˜¾ç¤º")
            return
        
        print("\n" + "="*80)
        print("ğŸ¯ ç­–ç•¥æ¨¡å—å’Œé£æ§å­æ¨¡å—å®Œæ•´é›†æˆæµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        metadata = self.test_results.get('test_metadata', {})
        success = metadata.get('success', False)
        
        print(f"æ€»ä½“ç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
        print(f"æµ‹è¯•æ—¶é•¿: {metadata.get('duration_seconds', 0):.2f} ç§’")
        print(f"å¼€å§‹æ—¶é—´: {metadata.get('start_time', 'N/A')}")
        print(f"ç»“æŸæ—¶é—´: {metadata.get('end_time', 'N/A')}")
        
        # æ•°æ®ç”Ÿæˆç»“æœ
        data_gen = self.test_results.get('data_generation', {})
        print(f"\nğŸ“¡ æ•°æ®ç”Ÿæˆ:")
        print(f"  å‘é€æ¶ˆæ¯: {data_gen.get('messages_sent', 0)}/{data_gen.get('expected_messages', 0)}")
        print(f"  å®Œæˆç‡: {data_gen.get('completion_rate', 0)*100:.1f}%")
        print(f"  å®é™…é€Ÿç‡: {data_gen.get('messages_per_second', 0):.1f} æ¶ˆæ¯/ç§’")
        
        # ç³»ç»Ÿæ€§èƒ½
        perf = self.test_results.get('system_performance', {})
        print(f"\nğŸ’» ç³»ç»Ÿæ€§èƒ½:")
        if 'cpu' in perf:
            print(f"  CPUä½¿ç”¨ç‡: å¹³å‡ {perf['cpu'].get('average', 0):.1f}%, å³°å€¼ {perf['cpu'].get('max', 0):.1f}%")
        if 'memory' in perf:
            print(f"  å†…å­˜ä½¿ç”¨ç‡: å¹³å‡ {perf['memory'].get('average', 0):.1f}%, å³°å€¼ {perf['memory'].get('max', 0):.1f}%")
        
        # CPUäº²å’Œæ€§
        if 'cpu_affinity' in perf:
            affinity = perf['cpu_affinity']
            print(f"  CPUäº²å’Œæ€§: {affinity.get('successful_matches', 0)}/{affinity.get('total_checks', 0)} æˆåŠŸé…ç½®")
        
        # SIMDæ€§èƒ½
        if 'simd' in perf:
            simd = perf['simd']
            print(f"  SIMDæ”¯æŒ: {simd.get('features_supported', 0)} ä¸ªç‰¹æ€§")
            print(f"  SIMDæ€§èƒ½: {simd.get('operations_per_second', 0):.0f} ops/sec")
        
        # è¿›ç¨‹å¥åº·
        process_health = self.test_results.get('process_health', {})
        running_processes = [name for name, stat in process_health.items() if stat.get('status') != 'not_running']
        print(f"\nğŸ”„ è¿›ç¨‹çŠ¶æ€:")
        print(f"  è¿è¡Œä¸­è¿›ç¨‹: {len(running_processes)} ä¸ª")
        for name in running_processes:
            stat = process_health[name]
            print(f"    {name}: PID {stat.get('pid', 'N/A')}, CPU {stat.get('cpu_percent', 0):.1f}%")
        
        # é£æ§æµ‹è¯•
        risk_results = self.test_results.get('risk_control', {})
        if risk_results:
            print(f"\nğŸ›¡ï¸ é£æ§æµ‹è¯•:")
            for scenario, result in risk_results.items():
                status = "âœ…" if result.get('success', False) else "âŒ"
                print(f"  {scenario}: {status}")
        
        # æµ‹è¯•æ ‡å‡†
        criteria = self.test_results.get('test_criteria', {})
        print(f"\nğŸ“Š æµ‹è¯•æ ‡å‡†:")
        for criterion, passed in criteria.items():
            status = "âœ…" if passed else "âŒ"
            print(f"  {criterion}: {status}")
        
        print("="*80)

async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®ä¿¡å·å¤„ç†
    def signal_handler(signum, frame):
        logger.info("ğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
        sys.exit(1)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # æ£€æŸ¥ä¾èµ–
    missing_deps = []
    try:
        import psutil
    except ImportError:
        missing_deps.append('psutil')
    
    try:
        import numpy
    except ImportError:
        missing_deps.append('numpy')
    
    try:
        import cpuinfo
    except ImportError:
        missing_deps.append('py-cpuinfo')
    
    if missing_deps:
        logger.error(f"âŒ ç¼ºå°‘Pythonä¾èµ–: {', '.join(missing_deps)}")
        logger.error("è¯·å®‰è£…: pip install " + ' '.join(missing_deps))
        sys.exit(1)
    
    # åˆ›å»ºæµ‹è¯•ç¼–æ’å™¨
    orchestrator = IntegrationTestOrchestrator()
    
    try:
        # è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•
        logger.info("ğŸš€ å¯åŠ¨å®Œæ•´é›†æˆæµ‹è¯•...")
        results = await orchestrator.run_complete_integration_test()
        
        # æ‰“å°ç»“æœ
        orchestrator.print_test_summary()
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f'integration_test_results_{timestamp}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
        success = results.get('test_metadata', {}).get('success', False)
        sys.exit(0 if success else 1)
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 