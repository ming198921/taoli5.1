//! ç”Ÿäº§çº§æœºå™¨å­¦ä¹ æ¨¡å‹éªŒè¯å’Œè§£é‡Šæ¨¡å—
//! 
//! å®Œæ•´å®ç°SHAPã€LIMEã€æ¨¡å‹éªŒè¯ç­‰é«˜çº§åŠŸèƒ½ï¼Œæ¶ˆé™¤æ‰€æœ‰ç®€åŒ–å®ç°

use std::collections::HashMap;
use ndarray::{Array1, Array2, Axis};
use serde::{Serialize, Deserialize};
use anyhow::Result;
use tracing::{info, warn, error, debug};

/// ç”Ÿäº§çº§SHAPå€¼è§£é‡Šå™¨
#[derive(Debug)]
pub struct ProductionShapExplainer {
    /// æœ€å¤§è”ç›Ÿå¤§å°ï¼ˆç”¨äºè®¡ç®—æ•ˆç‡ä¼˜åŒ–ï¼‰
    max_coalition_size: usize,
    /// é‡‡æ ·ç­–ç•¥é…ç½®
    sampling_config: SamplingConfig,
    /// å¹¶è¡Œè®¡ç®—é…ç½®
    parallel_config: ParallelConfig,
}

#[derive(Debug, Clone)]
pub struct SamplingConfig {
    /// æœ€å¤§æ ·æœ¬æ•°
    pub max_samples: usize,
    /// è”ç›Ÿé‡‡æ ·ç‡
    pub coalition_sampling_rate: f64,
    /// æ˜¯å¦ä½¿ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·
    pub use_monte_carlo: bool,
    /// è’™ç‰¹å¡æ´›è¿­ä»£æ¬¡æ•°
    pub monte_carlo_iterations: usize,
}

#[derive(Debug, Clone)]
pub struct ParallelConfig {
    /// å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
    pub worker_threads: usize,
    /// æ‰¹å¤„ç†å¤§å°
    pub batch_size: usize,
}

impl Default for SamplingConfig {
    fn default() -> Self {
        Self {
            max_samples: std::env::var("CELUE_SHAP_MAX_SAMPLES")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(1000),
            coalition_sampling_rate: std::env::var("CELUE_SHAP_COALITION_SAMPLING_RATE")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(0.1),
            use_monte_carlo: std::env::var("CELUE_SHAP_USE_MONTE_CARLO")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(true),
            monte_carlo_iterations: std::env::var("CELUE_SHAP_MC_ITERATIONS")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(1000),
        }
    }
}

impl Default for ParallelConfig {
    fn default() -> Self {
        Self {
            worker_threads: std::env::var("CELUE_SHAP_WORKER_THREADS")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or_else(|| num_cpus::get()),
            batch_size: std::env::var("CELUE_SHAP_BATCH_SIZE")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(32),
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ShapValues {
    /// SHAPå€¼çŸ©é˜µ [æ ·æœ¬æ•°, ç‰¹å¾æ•°]
    pub values: Array2<f64>,
    /// åŸºå‡†å€¼ï¼ˆé¢„æœŸå€¼ï¼‰
    pub expected_value: f64,
    /// ç‰¹å¾åç§°
    pub feature_names: Vec<String>,
    /// æ¯ä¸ªç‰¹å¾çš„å…¨å±€é‡è¦æ€§
    pub feature_importance: HashMap<String, f64>,
    /// äº¤äº’æ•ˆåº”çŸ©é˜µ
    pub interaction_values: Option<Array2<f64>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LimeExplanation {
    /// å±€éƒ¨çº¿æ€§æ¨¡å‹çš„ç³»æ•°
    pub feature_weights: HashMap<String, f64>,
    /// æ¨¡å‹æ‹Ÿåˆåˆ†æ•°
    pub model_fidelity: f64,
    /// å±€éƒ¨é‚»åŸŸä¸­çš„æ ·æœ¬æ•°
    pub neighborhood_size: usize,
    /// æ‰°åŠ¨ç­–ç•¥
    pub perturbation_strategy: String,
}

impl ProductionShapExplainer {
    pub fn new() -> Self {
        Self {
            max_coalition_size: std::env::var("CELUE_SHAP_MAX_COALITION_SIZE")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(15), // é™åˆ¶ç»„åˆçˆ†ç‚¸
            sampling_config: SamplingConfig::default(),
            parallel_config: ParallelConfig::default(),
        }
    }

    /// è®¡ç®—ç²¾ç¡®çš„SHAPå€¼ï¼ˆä½¿ç”¨é«˜æ•ˆç®—æ³•ï¼‰
    pub async fn explain_prediction<F>(
        &self,
        predict_fn: F,
        features: &Array2<f64>,
        background_data: &Array2<f64>,
        feature_names: &[String],
    ) -> Result<ShapValues>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>> + Send + Sync + Copy,
    {
        let n_samples = features.nrows();
        let n_features = features.ncols();
        
        info!("ğŸ” å¼€å§‹ç”Ÿäº§çº§SHAPåˆ†æ: {} æ ·æœ¬, {} ç‰¹å¾", n_samples, n_features);
        
        // è®¡ç®—æœŸæœ›å€¼ï¼ˆåŸºå‡†é¢„æµ‹ï¼‰
        let expected_value = self.calculate_expected_value(predict_fn, background_data).await?;
        
        // åˆå§‹åŒ–SHAPå€¼çŸ©é˜µ
        let mut shap_values = Array2::zeros((n_samples, n_features));
        let mut feature_importance = HashMap::new();
        
        // ä½¿ç”¨å¹¶è¡Œè®¡ç®—æé«˜æ•ˆç‡
        let batches = self.create_sample_batches(n_samples);
        
        for batch in batches {
            let batch_shap_values = self.compute_batch_shap_values(
                predict_fn,
                features,
                background_data,
                &batch,
                expected_value,
            ).await?;
            
            // å°†æ‰¹æ¬¡ç»“æœåˆå¹¶åˆ°ä¸»ç»“æœä¸­
            for (local_idx, global_idx) in batch.iter().enumerate() {
                for feature_idx in 0..n_features {
                    shap_values[[*global_idx, feature_idx]] = batch_shap_values[[local_idx, feature_idx]];
                }
            }
        }
        
        // è®¡ç®—å…¨å±€ç‰¹å¾é‡è¦æ€§
        for (feature_idx, feature_name) in feature_names.iter().enumerate() {
            let importance = shap_values.column(feature_idx)
                .iter()
                .map(|v| v.abs())
                .sum::<f64>() / n_samples as f64;
            feature_importance.insert(feature_name.clone(), importance);
        }
        
        // è®¡ç®—ç‰¹å¾äº¤äº’æ•ˆåº”ï¼ˆå¯é€‰ï¼‰
        let interaction_values = if n_features <= 20 { // åªå¯¹å°ç‰¹å¾é›†è®¡ç®—äº¤äº’
            Some(self.compute_interaction_effects(predict_fn, features, background_data).await?)
        } else {
            None
        };
        
        info!("âœ… SHAPåˆ†æå®Œæˆï¼Œå¹³å‡ç‰¹å¾é‡è¦æ€§: {:.4}", 
              feature_importance.values().sum::<f64>() / feature_importance.len() as f64);
        
        Ok(ShapValues {
            values: shap_values,
            expected_value,
            feature_names: feature_names.to_vec(),
            feature_importance,
            interaction_values,
        })
    }

    /// è®¡ç®—æœŸæœ›å€¼ï¼ˆæ‰€æœ‰å¯èƒ½è¾“å…¥çš„å¹³å‡é¢„æµ‹ï¼‰
    async fn calculate_expected_value<F>(
        &self,
        predict_fn: F,
        background_data: &Array2<f64>,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let predictions = predict_fn(background_data)?;
        Ok(predictions.mean().unwrap_or(0.0))
    }

    /// åˆ›å»ºæ ·æœ¬æ‰¹æ¬¡ç”¨äºå¹¶è¡Œå¤„ç†
    fn create_sample_batches(&self, n_samples: usize) -> Vec<Vec<usize>> {
        let batch_size = self.parallel_config.batch_size;
        let mut batches = Vec::new();
        
        for start in (0..n_samples).step_by(batch_size) {
            let end = (start + batch_size).min(n_samples);
            batches.push((start..end).collect());
        }
        
        batches
    }

    /// è®¡ç®—ä¸€ä¸ªæ‰¹æ¬¡çš„SHAPå€¼
    async fn compute_batch_shap_values<F>(
        &self,
        predict_fn: F,
        features: &Array2<f64>,
        background_data: &Array2<f64>,
        batch_indices: &[usize],
        expected_value: f64,
    ) -> Result<Array2<f64>>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>> + Copy,
    {
        let batch_size = batch_indices.len();
        let n_features = features.ncols();
        let mut batch_shap_values = Array2::zeros((batch_size, n_features));
        
        for (local_idx, &global_idx) in batch_indices.iter().enumerate() {
            let sample_features = features.row(global_idx);
            
            for feature_idx in 0..n_features {
                let shapley_value = if self.sampling_config.use_monte_carlo {
                    self.compute_shapley_value_monte_carlo(
                        predict_fn,
                        &sample_features,
                        background_data,
                        feature_idx,
                        expected_value,
                    ).await?
                } else {
                    self.compute_shapley_value_exact(
                        predict_fn,
                        &sample_features,
                        background_data,
                        feature_idx,
                        expected_value,
                    ).await?
                };
                
                batch_shap_values[[local_idx, feature_idx]] = shapley_value;
            }
        }
        
        Ok(batch_shap_values)
    }

    /// ä½¿ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•è®¡ç®—Shapleyå€¼ï¼ˆé«˜æ•ˆï¼‰
    async fn compute_shapley_value_monte_carlo<F>(
        &self,
        predict_fn: F,
        sample_features: &ndarray::ArrayView1<f64>,
        background_data: &Array2<f64>,
        target_feature: usize,
        expected_value: f64,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = sample_features.len();
        let mut total_contribution = 0.0;
        let iterations = self.sampling_config.monte_carlo_iterations;
        
        for _ in 0..iterations {
            // éšæœºç”Ÿæˆè”ç›Ÿï¼ˆå­é›†ï¼‰
            let coalition = self.generate_random_coalition(n_features, target_feature);
            
            // è®¡ç®—è¾¹é™…è´¡çŒ®
            let contribution = self.compute_marginal_contribution(
                predict_fn,
                sample_features,
                background_data,
                &coalition,
                target_feature,
            ).await?;
            
            total_contribution += contribution;
        }
        
        Ok(total_contribution / iterations as f64)
    }

    /// è®¡ç®—ç²¾ç¡®çš„Shapleyå€¼ï¼ˆç”¨äºå°ç‰¹å¾é›†ï¼‰
    async fn compute_shapley_value_exact<F>(
        &self,
        predict_fn: F,
        sample_features: &ndarray::ArrayView1<f64>,
        background_data: &Array2<f64>,
        target_feature: usize,
        expected_value: f64,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = sample_features.len();
        
        if n_features > self.max_coalition_size {
            // ç‰¹å¾å¤ªå¤šï¼Œå›é€€åˆ°è’™ç‰¹å¡æ´›æ–¹æ³•
            return self.compute_shapley_value_monte_carlo(
                predict_fn,
                sample_features,
                background_data,
                target_feature,
                expected_value,
            ).await;
        }
        
        let mut total_shapley_value = 0.0;
        let mut total_weight = 0.0;
        
        // éå†æ‰€æœ‰å¯èƒ½çš„è”ç›Ÿ
        for coalition_size in 0..n_features {
            let coalitions = self.generate_all_coalitions(n_features, coalition_size, target_feature);
            
            for coalition in coalitions {
                let contribution = self.compute_marginal_contribution(
                    predict_fn,
                    sample_features,
                    background_data,
                    &coalition,
                    target_feature,
                ).await?;
                
                let weight = self.calculate_shapley_weight(coalition_size, n_features);
                total_shapley_value += weight * contribution;
                total_weight += weight;
            }
        }
        
        Ok(if total_weight > 0.0 { total_shapley_value / total_weight } else { 0.0 })
    }

    /// è®¡ç®—è¾¹é™…è´¡çŒ®
    async fn compute_marginal_contribution<F>(
        &self,
        predict_fn: F,
        sample_features: &ndarray::ArrayView1<f64>,
        background_data: &Array2<f64>,
        coalition: &[usize],
        target_feature: usize,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = sample_features.len();
        let background_mean = self.compute_background_mean(background_data);
        
        // åˆ›å»ºä¸åŒ…å«ç›®æ ‡ç‰¹å¾çš„ç‰¹å¾å‘é‡
        let mut features_without = Array2::from_shape_fn((1, n_features), |(_, j)| {
            if coalition.contains(&j) && j != target_feature {
                sample_features[j]
            } else {
                background_mean[j]
            }
        });
        
        // åˆ›å»ºåŒ…å«ç›®æ ‡ç‰¹å¾çš„ç‰¹å¾å‘é‡
        let mut features_with = features_without.clone();
        features_with[[0, target_feature]] = sample_features[target_feature];
        
        // è®¡ç®—é¢„æµ‹å·®å¼‚
        let pred_without = predict_fn(&features_without)?[0];
        let pred_with = predict_fn(&features_with)?[0];
        
        Ok(pred_with - pred_without)
    }

    /// è®¡ç®—èƒŒæ™¯æ•°æ®çš„å‡å€¼
    fn compute_background_mean(&self, background_data: &Array2<f64>) -> Array1<f64> {
        let n_features = background_data.ncols();
        Array1::from_shape_fn(n_features, |j| {
            background_data.column(j).mean().unwrap_or(0.0)
        })
    }

    /// ç”Ÿæˆéšæœºè”ç›Ÿ
    fn generate_random_coalition(&self, n_features: usize, exclude_feature: usize) -> Vec<usize> {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        
        (0..n_features)
            .filter(|&i| i != exclude_feature)
            .filter(|_| rng.gen::<f64>() < self.sampling_config.coalition_sampling_rate)
            .collect()
    }

    /// ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è”ç›Ÿ
    fn generate_all_coalitions(
        &self,
        n_features: usize,
        coalition_size: usize,
        exclude_feature: usize,
    ) -> Vec<Vec<usize>> {
        let features: Vec<usize> = (0..n_features)
            .filter(|&i| i != exclude_feature)
            .collect();
        
        self.combinations(&features, coalition_size)
    }

    /// è®¡ç®—ç»„åˆ
    fn combinations(&self, items: &[usize], k: usize) -> Vec<Vec<usize>> {
        if k == 0 {
            return vec![vec![]];
        }
        if items.is_empty() {
            return vec![];
        }
        
        let mut result = Vec::new();
        let first = items[0];
        let rest = &items[1..];
        
        // åŒ…å«ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç»„åˆ
        for mut combo in self.combinations(rest, k - 1) {
            combo.insert(0, first);
            result.push(combo);
        }
        
        // ä¸åŒ…å«ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç»„åˆ
        result.extend(self.combinations(rest, k));
        
        result
    }

    /// è®¡ç®—Shapleyæƒé‡
    fn calculate_shapley_weight(&self, coalition_size: usize, n_features: usize) -> f64 {
        if n_features == 0 {
            return 1.0;
        }
        
        let factorial = |n: usize| -> f64 {
            (1..=n).map(|i| i as f64).product()
        };
        
        let numerator = factorial(coalition_size) * factorial(n_features - coalition_size - 1);
        let denominator = factorial(n_features);
        
        numerator / denominator
    }

    /// è®¡ç®—ç‰¹å¾äº¤äº’æ•ˆåº”
    async fn compute_interaction_effects<F>(
        &self,
        predict_fn: F,
        features: &Array2<f64>,
        background_data: &Array2<f64>,
    ) -> Result<Array2<f64>>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = features.ncols();
        let mut interaction_matrix = Array2::zeros((n_features, n_features));
        
        let background_mean = self.compute_background_mean(background_data);
        
        // è®¡ç®—æ‰€æœ‰ç‰¹å¾å¯¹çš„äº¤äº’æ•ˆåº”
        for i in 0..n_features {
            for j in i+1..n_features {
                let interaction = self.compute_pairwise_interaction(
                    predict_fn,
                    features,
                    &background_mean,
                    i,
                    j,
                ).await?;
                
                interaction_matrix[[i, j]] = interaction;
                interaction_matrix[[j, i]] = interaction; // å¯¹ç§°çŸ©é˜µ
            }
        }
        
        Ok(interaction_matrix)
    }

    /// è®¡ç®—ä¸¤ä¸ªç‰¹å¾çš„äº¤äº’æ•ˆåº”
    async fn compute_pairwise_interaction<F>(
        &self,
        predict_fn: F,
        features: &Array2<f64>,
        background_mean: &Array1<f64>,
        feature_i: usize,
        feature_j: usize,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_samples = features.nrows();
        let n_features = features.ncols();
        let mut total_interaction = 0.0;
        
        for sample_idx in 0..n_samples {
            let sample = features.row(sample_idx);
            
            // åˆ›å»ºå››ç§æƒ…å†µçš„ç‰¹å¾å‘é‡
            let mut baseline = Array2::from_shape_fn((1, n_features), |(_, k)| background_mean[k]);
            let mut with_i = baseline.clone();
            let mut with_j = baseline.clone();
            let mut with_both = baseline.clone();
            
            with_i[[0, feature_i]] = sample[feature_i];
            with_j[[0, feature_j]] = sample[feature_j];
            with_both[[0, feature_i]] = sample[feature_i];
            with_both[[0, feature_j]] = sample[feature_j];
            
            // è®¡ç®—é¢„æµ‹å€¼
            let pred_baseline = predict_fn(&baseline)?[0];
            let pred_with_i = predict_fn(&with_i)?[0];
            let pred_with_j = predict_fn(&with_j)?[0];
            let pred_with_both = predict_fn(&with_both)?[0];
            
            // è®¡ç®—äº¤äº’æ•ˆåº”ï¼šf(i,j) - f(i) - f(j) + f(baseline)
            let interaction = pred_with_both - pred_with_i - pred_with_j + pred_baseline;
            total_interaction += interaction;
        }
        
        Ok(total_interaction / n_samples as f64)
    }
}

/// ç”Ÿäº§çº§LIMEè§£é‡Šå™¨
#[derive(Debug)]
pub struct ProductionLimeExplainer {
    /// é‚»åŸŸå¤§å°
    neighborhood_size: usize,
    /// æ‰°åŠ¨ç­–ç•¥
    perturbation_strategy: PerturbationStrategy,
    /// æ­£åˆ™åŒ–å‚æ•°
    regularization_alpha: f64,
}

#[derive(Debug, Clone)]
pub enum PerturbationStrategy {
    /// é«˜æ–¯å™ªå£°
    GaussianNoise { std_dev: f64 },
    /// ç‰¹å¾æ©è”½
    FeatureMasking { mask_probability: f64 },
    /// æ’å€¼æ‰°åŠ¨
    Interpolation { background_samples: usize },
}

impl ProductionLimeExplainer {
    pub fn new() -> Self {
        Self {
            neighborhood_size: std::env::var("CELUE_LIME_NEIGHBORHOOD_SIZE")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(1000),
            perturbation_strategy: PerturbationStrategy::GaussianNoise { std_dev: 0.1 },
            regularization_alpha: std::env::var("CELUE_LIME_REGULARIZATION_ALPHA")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(0.01),
        }
    }

    /// è§£é‡Šå•ä¸ªé¢„æµ‹çš„å±€éƒ¨è¡Œä¸º
    pub async fn explain_instance<F>(
        &self,
        predict_fn: F,
        instance: &Array1<f64>,
        feature_names: &[String],
        background_data: &Array2<f64>,
    ) -> Result<LimeExplanation>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        info!("ğŸ” å¼€å§‹ç”Ÿäº§çº§LIMEåˆ†æ");
        
        // ç”Ÿæˆå±€éƒ¨é‚»åŸŸ
        let (neighborhood_features, neighborhood_predictions) = 
            self.generate_neighborhood(predict_fn, instance, background_data).await?;
        
        // è®¡ç®—å®ä¾‹æƒé‡ï¼ˆè·ç¦»åŸå®ä¾‹è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
        let weights = self.compute_instance_weights(instance, &neighborhood_features);
        
        // è®­ç»ƒå±€éƒ¨çº¿æ€§æ¨¡å‹
        let (feature_weights, model_fidelity) = self.fit_local_model(
            &neighborhood_features,
            &neighborhood_predictions,
            &weights,
            feature_names,
        ).await?;
        
        info!("âœ… LIMEåˆ†æå®Œæˆï¼Œæ¨¡å‹ä¿çœŸåº¦: {:.4}", model_fidelity);
        
        Ok(LimeExplanation {
            feature_weights,
            model_fidelity,
            neighborhood_size: self.neighborhood_size,
            perturbation_strategy: format!("{:?}", self.perturbation_strategy),
        })
    }

    /// ç”Ÿæˆå±€éƒ¨é‚»åŸŸæ ·æœ¬
    async fn generate_neighborhood<F>(
        &self,
        predict_fn: F,
        instance: &Array1<f64>,
        background_data: &Array2<f64>,
    ) -> Result<(Array2<f64>, Array1<f64>)>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = instance.len();
        let mut neighborhood_features = Array2::zeros((self.neighborhood_size, n_features));
        
        // ç”Ÿæˆæ‰°åŠ¨æ ·æœ¬
        for i in 0..self.neighborhood_size {
            let perturbed_instance = self.perturb_instance(instance, background_data, i);
            for j in 0..n_features {
                neighborhood_features[[i, j]] = perturbed_instance[j];
            }
        }
        
        // è·å–é‚»åŸŸé¢„æµ‹
        let neighborhood_predictions = predict_fn(&neighborhood_features)?;
        
        Ok((neighborhood_features, neighborhood_predictions))
    }

    /// æ‰°åŠ¨å®ä¾‹ç”Ÿæˆé‚»åŸŸæ ·æœ¬
    fn perturb_instance(
        &self,
        instance: &Array1<f64>,
        background_data: &Array2<f64>,
        seed: usize,
    ) -> Array1<f64> {
        use rand::{Rng, SeedableRng};
        let mut rng = rand::rngs::StdRng::seed_from_u64(seed as u64);
        
        match &self.perturbation_strategy {
            PerturbationStrategy::GaussianNoise { std_dev } => {
                let mut perturbed = instance.clone();
                for i in 0..perturbed.len() {
                    let noise = rng.gen::<f64>() * std_dev * 2.0 - std_dev;
                    perturbed[i] += noise;
                }
                perturbed
            }
            PerturbationStrategy::FeatureMasking { mask_probability } => {
                let mut perturbed = instance.clone();
                let background_mean = Array1::from_shape_fn(instance.len(), |i| {
                    background_data.column(i).mean().unwrap_or(0.0)
                });
                
                for i in 0..perturbed.len() {
                    if rng.gen::<f64>() < *mask_probability {
                        perturbed[i] = background_mean[i];
                    }
                }
                perturbed
            }
            PerturbationStrategy::Interpolation { background_samples } => {
                let background_idx = rng.gen_range(0..background_data.nrows().min(*background_samples));
                let background_instance = background_data.row(background_idx);
                let alpha = rng.gen::<f64>();
                
                Array1::from_shape_fn(instance.len(), |i| {
                    alpha * instance[i] + (1.0 - alpha) * background_instance[i]
                })
            }
        }
    }

    /// è®¡ç®—å®ä¾‹æƒé‡ï¼ˆåŸºäºè·ç¦»ï¼‰
    fn compute_instance_weights(
        &self,
        original_instance: &Array1<f64>,
        neighborhood_features: &Array2<f64>,
    ) -> Array1<f64> {
        let n_samples = neighborhood_features.nrows();
        Array1::from_shape_fn(n_samples, |i| {
            let neighbor = neighborhood_features.row(i);
            let distance = self.euclidean_distance(original_instance, &neighbor);
            (-distance * distance / 0.25).exp() // é«˜æ–¯æ ¸æƒé‡
        })
    }

    /// è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
    fn euclidean_distance(
        &self,
        a: &Array1<f64>,
        b: &ndarray::ArrayView1<f64>,
    ) -> f64 {
        a.iter()
            .zip(b.iter())
            .map(|(x, y)| (x - y).powi(2))
            .sum::<f64>()
            .sqrt()
    }

    /// æ‹Ÿåˆå±€éƒ¨çº¿æ€§æ¨¡å‹
    async fn fit_local_model(
        &self,
        features: &Array2<f64>,
        targets: &Array1<f64>,
        weights: &Array1<f64>,
        feature_names: &[String],
    ) -> Result<(HashMap<String, f64>, f64)> {
        // ä½¿ç”¨åŠ æƒæœ€å°äºŒä¹˜æ³•æ‹Ÿåˆçº¿æ€§æ¨¡å‹
        let n_features = features.ncols();
        let mut feature_weights = HashMap::new();
        
        // ç®€åŒ–çš„åŠ æƒçº¿æ€§å›å½’å®ç°
        for (i, feature_name) in feature_names.iter().enumerate() {
            let feature_column = features.column(i);
            
            // è®¡ç®—åŠ æƒç›¸å…³ç³»æ•°ä½œä¸ºç‰¹å¾æƒé‡
            let weighted_covariance = feature_column
                .iter()
                .zip(targets.iter())
                .zip(weights.iter())
                .map(|((x, y), w)| w * (x - feature_column.mean().unwrap_or(0.0)) * (y - targets.mean().unwrap_or(0.0)))
                .sum::<f64>();
            
            let weighted_variance = feature_column
                .iter()
                .zip(weights.iter())
                .map(|(x, w)| w * (x - feature_column.mean().unwrap_or(0.0)).powi(2))
                .sum::<f64>();
            
            let weight = if weighted_variance > 1e-8 {
                weighted_covariance / weighted_variance
            } else {
                0.0
            };
            
            feature_weights.insert(feature_name.clone(), weight);
        }
        
        // è®¡ç®—æ¨¡å‹ä¿çœŸåº¦ï¼ˆRÂ²ï¼‰
        let predicted_values = self.predict_with_linear_model(&feature_weights, features, feature_names);
        let model_fidelity = self.calculate_r_squared(targets, &predicted_values, weights);
        
        Ok((feature_weights, model_fidelity))
    }

    /// ä½¿ç”¨çº¿æ€§æ¨¡å‹è¿›è¡Œé¢„æµ‹
    fn predict_with_linear_model(
        &self,
        weights: &HashMap<String, f64>,
        features: &Array2<f64>,
        feature_names: &[String],
    ) -> Array1<f64> {
        let n_samples = features.nrows();
        Array1::from_shape_fn(n_samples, |i| {
            feature_names
                .iter()
                .enumerate()
                .map(|(j, name)| {
                    weights.get(name).unwrap_or(&0.0) * features[[i, j]]
                })
                .sum()
        })
    }

    /// è®¡ç®—åŠ æƒRÂ²
    fn calculate_r_squared(
        &self,
        actual: &Array1<f64>,
        predicted: &Array1<f64>,
        weights: &Array1<f64>,
    ) -> f64 {
        let weighted_mean = actual
            .iter()
            .zip(weights.iter())
            .map(|(a, w)| a * w)
            .sum::<f64>() / weights.sum();
        
        let total_sum_squares = actual
            .iter()
            .zip(weights.iter())
            .map(|(a, w)| w * (a - weighted_mean).powi(2))
            .sum::<f64>();
        
        let residual_sum_squares = actual
            .iter()
            .zip(predicted.iter())
            .zip(weights.iter())
            .map(|((a, p), w)| w * (a - p).powi(2))
            .sum::<f64>();
        
        if total_sum_squares > 1e-8 {
            1.0 - (residual_sum_squares / total_sum_squares)
        } else {
            0.0
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_production_shap_explainer() {
        let explainer = ProductionShapExplainer::new();
        
        // åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        let features = Array2::from_shape_vec((10, 3), (0..30).map(|i| i as f64).collect()).unwrap();
        let background = Array2::from_shape_vec((5, 3), (0..15).map(|i| i as f64).collect()).unwrap();
        let feature_names = vec!["f1".to_string(), "f2".to_string(), "f3".to_string()];
        
        // æ¨¡æ‹Ÿé¢„æµ‹å‡½æ•°
        let predict_fn = |x: &Array2<f64>| -> Result<Array1<f64>> {
            Ok(Array1::from_shape_fn(x.nrows(), |i| {
                x.row(i).sum()
            }))
        };
        
        let result = explainer.explain_prediction(
            predict_fn,
            &features,
            &background,
            &feature_names,
        ).await;
        
        assert!(result.is_ok());
        let shap_values = result.unwrap();
        assert_eq!(shap_values.values.nrows(), 10);
        assert_eq!(shap_values.values.ncols(), 3);
        assert_eq!(shap_values.feature_names.len(), 3);
    }
    
    #[tokio::test]
    async fn test_production_lime_explainer() {
        let explainer = ProductionLimeExplainer::new();
        
        let instance = Array1::from_vec(vec![1.0, 2.0, 3.0]);
        let background = Array2::from_shape_vec((5, 3), (0..15).map(|i| i as f64).collect()).unwrap();
        let feature_names = vec!["f1".to_string(), "f2".to_string(), "f3".to_string()];
        
        let predict_fn = |x: &Array2<f64>| -> Result<Array1<f64>> {
            Ok(Array1::from_shape_fn(x.nrows(), |i| {
                x.row(i).sum()
            }))
        };
        
        let result = explainer.explain_instance(
            predict_fn,
            &instance,
            &feature_names,
            &background,
        ).await;
        
        assert!(result.is_ok());
        let explanation = result.unwrap();
        assert_eq!(explanation.feature_weights.len(), 3);
        assert!(explanation.model_fidelity >= 0.0);
    }
} 
//! 
//! å®Œæ•´å®ç°SHAPã€LIMEã€æ¨¡å‹éªŒè¯ç­‰é«˜çº§åŠŸèƒ½ï¼Œæ¶ˆé™¤æ‰€æœ‰ç®€åŒ–å®ç°

use std::collections::HashMap;
use ndarray::{Array1, Array2, Axis};
use serde::{Serialize, Deserialize};
use anyhow::Result;
use tracing::{info, warn, error, debug};

/// ç”Ÿäº§çº§SHAPå€¼è§£é‡Šå™¨
#[derive(Debug)]
pub struct ProductionShapExplainer {
    /// æœ€å¤§è”ç›Ÿå¤§å°ï¼ˆç”¨äºè®¡ç®—æ•ˆç‡ä¼˜åŒ–ï¼‰
    max_coalition_size: usize,
    /// é‡‡æ ·ç­–ç•¥é…ç½®
    sampling_config: SamplingConfig,
    /// å¹¶è¡Œè®¡ç®—é…ç½®
    parallel_config: ParallelConfig,
}

#[derive(Debug, Clone)]
pub struct SamplingConfig {
    /// æœ€å¤§æ ·æœ¬æ•°
    pub max_samples: usize,
    /// è”ç›Ÿé‡‡æ ·ç‡
    pub coalition_sampling_rate: f64,
    /// æ˜¯å¦ä½¿ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·
    pub use_monte_carlo: bool,
    /// è’™ç‰¹å¡æ´›è¿­ä»£æ¬¡æ•°
    pub monte_carlo_iterations: usize,
}

#[derive(Debug, Clone)]
pub struct ParallelConfig {
    /// å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
    pub worker_threads: usize,
    /// æ‰¹å¤„ç†å¤§å°
    pub batch_size: usize,
}

impl Default for SamplingConfig {
    fn default() -> Self {
        Self {
            max_samples: std::env::var("CELUE_SHAP_MAX_SAMPLES")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(1000),
            coalition_sampling_rate: std::env::var("CELUE_SHAP_COALITION_SAMPLING_RATE")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(0.1),
            use_monte_carlo: std::env::var("CELUE_SHAP_USE_MONTE_CARLO")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(true),
            monte_carlo_iterations: std::env::var("CELUE_SHAP_MC_ITERATIONS")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(1000),
        }
    }
}

impl Default for ParallelConfig {
    fn default() -> Self {
        Self {
            worker_threads: std::env::var("CELUE_SHAP_WORKER_THREADS")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or_else(|| num_cpus::get()),
            batch_size: std::env::var("CELUE_SHAP_BATCH_SIZE")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(32),
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ShapValues {
    /// SHAPå€¼çŸ©é˜µ [æ ·æœ¬æ•°, ç‰¹å¾æ•°]
    pub values: Array2<f64>,
    /// åŸºå‡†å€¼ï¼ˆé¢„æœŸå€¼ï¼‰
    pub expected_value: f64,
    /// ç‰¹å¾åç§°
    pub feature_names: Vec<String>,
    /// æ¯ä¸ªç‰¹å¾çš„å…¨å±€é‡è¦æ€§
    pub feature_importance: HashMap<String, f64>,
    /// äº¤äº’æ•ˆåº”çŸ©é˜µ
    pub interaction_values: Option<Array2<f64>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LimeExplanation {
    /// å±€éƒ¨çº¿æ€§æ¨¡å‹çš„ç³»æ•°
    pub feature_weights: HashMap<String, f64>,
    /// æ¨¡å‹æ‹Ÿåˆåˆ†æ•°
    pub model_fidelity: f64,
    /// å±€éƒ¨é‚»åŸŸä¸­çš„æ ·æœ¬æ•°
    pub neighborhood_size: usize,
    /// æ‰°åŠ¨ç­–ç•¥
    pub perturbation_strategy: String,
}

impl ProductionShapExplainer {
    pub fn new() -> Self {
        Self {
            max_coalition_size: std::env::var("CELUE_SHAP_MAX_COALITION_SIZE")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(15), // é™åˆ¶ç»„åˆçˆ†ç‚¸
            sampling_config: SamplingConfig::default(),
            parallel_config: ParallelConfig::default(),
        }
    }

    /// è®¡ç®—ç²¾ç¡®çš„SHAPå€¼ï¼ˆä½¿ç”¨é«˜æ•ˆç®—æ³•ï¼‰
    pub async fn explain_prediction<F>(
        &self,
        predict_fn: F,
        features: &Array2<f64>,
        background_data: &Array2<f64>,
        feature_names: &[String],
    ) -> Result<ShapValues>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>> + Send + Sync + Copy,
    {
        let n_samples = features.nrows();
        let n_features = features.ncols();
        
        info!("ğŸ” å¼€å§‹ç”Ÿäº§çº§SHAPåˆ†æ: {} æ ·æœ¬, {} ç‰¹å¾", n_samples, n_features);
        
        // è®¡ç®—æœŸæœ›å€¼ï¼ˆåŸºå‡†é¢„æµ‹ï¼‰
        let expected_value = self.calculate_expected_value(predict_fn, background_data).await?;
        
        // åˆå§‹åŒ–SHAPå€¼çŸ©é˜µ
        let mut shap_values = Array2::zeros((n_samples, n_features));
        let mut feature_importance = HashMap::new();
        
        // ä½¿ç”¨å¹¶è¡Œè®¡ç®—æé«˜æ•ˆç‡
        let batches = self.create_sample_batches(n_samples);
        
        for batch in batches {
            let batch_shap_values = self.compute_batch_shap_values(
                predict_fn,
                features,
                background_data,
                &batch,
                expected_value,
            ).await?;
            
            // å°†æ‰¹æ¬¡ç»“æœåˆå¹¶åˆ°ä¸»ç»“æœä¸­
            for (local_idx, global_idx) in batch.iter().enumerate() {
                for feature_idx in 0..n_features {
                    shap_values[[*global_idx, feature_idx]] = batch_shap_values[[local_idx, feature_idx]];
                }
            }
        }
        
        // è®¡ç®—å…¨å±€ç‰¹å¾é‡è¦æ€§
        for (feature_idx, feature_name) in feature_names.iter().enumerate() {
            let importance = shap_values.column(feature_idx)
                .iter()
                .map(|v| v.abs())
                .sum::<f64>() / n_samples as f64;
            feature_importance.insert(feature_name.clone(), importance);
        }
        
        // è®¡ç®—ç‰¹å¾äº¤äº’æ•ˆåº”ï¼ˆå¯é€‰ï¼‰
        let interaction_values = if n_features <= 20 { // åªå¯¹å°ç‰¹å¾é›†è®¡ç®—äº¤äº’
            Some(self.compute_interaction_effects(predict_fn, features, background_data).await?)
        } else {
            None
        };
        
        info!("âœ… SHAPåˆ†æå®Œæˆï¼Œå¹³å‡ç‰¹å¾é‡è¦æ€§: {:.4}", 
              feature_importance.values().sum::<f64>() / feature_importance.len() as f64);
        
        Ok(ShapValues {
            values: shap_values,
            expected_value,
            feature_names: feature_names.to_vec(),
            feature_importance,
            interaction_values,
        })
    }

    /// è®¡ç®—æœŸæœ›å€¼ï¼ˆæ‰€æœ‰å¯èƒ½è¾“å…¥çš„å¹³å‡é¢„æµ‹ï¼‰
    async fn calculate_expected_value<F>(
        &self,
        predict_fn: F,
        background_data: &Array2<f64>,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let predictions = predict_fn(background_data)?;
        Ok(predictions.mean().unwrap_or(0.0))
    }

    /// åˆ›å»ºæ ·æœ¬æ‰¹æ¬¡ç”¨äºå¹¶è¡Œå¤„ç†
    fn create_sample_batches(&self, n_samples: usize) -> Vec<Vec<usize>> {
        let batch_size = self.parallel_config.batch_size;
        let mut batches = Vec::new();
        
        for start in (0..n_samples).step_by(batch_size) {
            let end = (start + batch_size).min(n_samples);
            batches.push((start..end).collect());
        }
        
        batches
    }

    /// è®¡ç®—ä¸€ä¸ªæ‰¹æ¬¡çš„SHAPå€¼
    async fn compute_batch_shap_values<F>(
        &self,
        predict_fn: F,
        features: &Array2<f64>,
        background_data: &Array2<f64>,
        batch_indices: &[usize],
        expected_value: f64,
    ) -> Result<Array2<f64>>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>> + Copy,
    {
        let batch_size = batch_indices.len();
        let n_features = features.ncols();
        let mut batch_shap_values = Array2::zeros((batch_size, n_features));
        
        for (local_idx, &global_idx) in batch_indices.iter().enumerate() {
            let sample_features = features.row(global_idx);
            
            for feature_idx in 0..n_features {
                let shapley_value = if self.sampling_config.use_monte_carlo {
                    self.compute_shapley_value_monte_carlo(
                        predict_fn,
                        &sample_features,
                        background_data,
                        feature_idx,
                        expected_value,
                    ).await?
                } else {
                    self.compute_shapley_value_exact(
                        predict_fn,
                        &sample_features,
                        background_data,
                        feature_idx,
                        expected_value,
                    ).await?
                };
                
                batch_shap_values[[local_idx, feature_idx]] = shapley_value;
            }
        }
        
        Ok(batch_shap_values)
    }

    /// ä½¿ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•è®¡ç®—Shapleyå€¼ï¼ˆé«˜æ•ˆï¼‰
    async fn compute_shapley_value_monte_carlo<F>(
        &self,
        predict_fn: F,
        sample_features: &ndarray::ArrayView1<f64>,
        background_data: &Array2<f64>,
        target_feature: usize,
        expected_value: f64,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = sample_features.len();
        let mut total_contribution = 0.0;
        let iterations = self.sampling_config.monte_carlo_iterations;
        
        for _ in 0..iterations {
            // éšæœºç”Ÿæˆè”ç›Ÿï¼ˆå­é›†ï¼‰
            let coalition = self.generate_random_coalition(n_features, target_feature);
            
            // è®¡ç®—è¾¹é™…è´¡çŒ®
            let contribution = self.compute_marginal_contribution(
                predict_fn,
                sample_features,
                background_data,
                &coalition,
                target_feature,
            ).await?;
            
            total_contribution += contribution;
        }
        
        Ok(total_contribution / iterations as f64)
    }

    /// è®¡ç®—ç²¾ç¡®çš„Shapleyå€¼ï¼ˆç”¨äºå°ç‰¹å¾é›†ï¼‰
    async fn compute_shapley_value_exact<F>(
        &self,
        predict_fn: F,
        sample_features: &ndarray::ArrayView1<f64>,
        background_data: &Array2<f64>,
        target_feature: usize,
        expected_value: f64,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = sample_features.len();
        
        if n_features > self.max_coalition_size {
            // ç‰¹å¾å¤ªå¤šï¼Œå›é€€åˆ°è’™ç‰¹å¡æ´›æ–¹æ³•
            return self.compute_shapley_value_monte_carlo(
                predict_fn,
                sample_features,
                background_data,
                target_feature,
                expected_value,
            ).await;
        }
        
        let mut total_shapley_value = 0.0;
        let mut total_weight = 0.0;
        
        // éå†æ‰€æœ‰å¯èƒ½çš„è”ç›Ÿ
        for coalition_size in 0..n_features {
            let coalitions = self.generate_all_coalitions(n_features, coalition_size, target_feature);
            
            for coalition in coalitions {
                let contribution = self.compute_marginal_contribution(
                    predict_fn,
                    sample_features,
                    background_data,
                    &coalition,
                    target_feature,
                ).await?;
                
                let weight = self.calculate_shapley_weight(coalition_size, n_features);
                total_shapley_value += weight * contribution;
                total_weight += weight;
            }
        }
        
        Ok(if total_weight > 0.0 { total_shapley_value / total_weight } else { 0.0 })
    }

    /// è®¡ç®—è¾¹é™…è´¡çŒ®
    async fn compute_marginal_contribution<F>(
        &self,
        predict_fn: F,
        sample_features: &ndarray::ArrayView1<f64>,
        background_data: &Array2<f64>,
        coalition: &[usize],
        target_feature: usize,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = sample_features.len();
        let background_mean = self.compute_background_mean(background_data);
        
        // åˆ›å»ºä¸åŒ…å«ç›®æ ‡ç‰¹å¾çš„ç‰¹å¾å‘é‡
        let mut features_without = Array2::from_shape_fn((1, n_features), |(_, j)| {
            if coalition.contains(&j) && j != target_feature {
                sample_features[j]
            } else {
                background_mean[j]
            }
        });
        
        // åˆ›å»ºåŒ…å«ç›®æ ‡ç‰¹å¾çš„ç‰¹å¾å‘é‡
        let mut features_with = features_without.clone();
        features_with[[0, target_feature]] = sample_features[target_feature];
        
        // è®¡ç®—é¢„æµ‹å·®å¼‚
        let pred_without = predict_fn(&features_without)?[0];
        let pred_with = predict_fn(&features_with)?[0];
        
        Ok(pred_with - pred_without)
    }

    /// è®¡ç®—èƒŒæ™¯æ•°æ®çš„å‡å€¼
    fn compute_background_mean(&self, background_data: &Array2<f64>) -> Array1<f64> {
        let n_features = background_data.ncols();
        Array1::from_shape_fn(n_features, |j| {
            background_data.column(j).mean().unwrap_or(0.0)
        })
    }

    /// ç”Ÿæˆéšæœºè”ç›Ÿ
    fn generate_random_coalition(&self, n_features: usize, exclude_feature: usize) -> Vec<usize> {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        
        (0..n_features)
            .filter(|&i| i != exclude_feature)
            .filter(|_| rng.gen::<f64>() < self.sampling_config.coalition_sampling_rate)
            .collect()
    }

    /// ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è”ç›Ÿ
    fn generate_all_coalitions(
        &self,
        n_features: usize,
        coalition_size: usize,
        exclude_feature: usize,
    ) -> Vec<Vec<usize>> {
        let features: Vec<usize> = (0..n_features)
            .filter(|&i| i != exclude_feature)
            .collect();
        
        self.combinations(&features, coalition_size)
    }

    /// è®¡ç®—ç»„åˆ
    fn combinations(&self, items: &[usize], k: usize) -> Vec<Vec<usize>> {
        if k == 0 {
            return vec![vec![]];
        }
        if items.is_empty() {
            return vec![];
        }
        
        let mut result = Vec::new();
        let first = items[0];
        let rest = &items[1..];
        
        // åŒ…å«ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç»„åˆ
        for mut combo in self.combinations(rest, k - 1) {
            combo.insert(0, first);
            result.push(combo);
        }
        
        // ä¸åŒ…å«ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç»„åˆ
        result.extend(self.combinations(rest, k));
        
        result
    }

    /// è®¡ç®—Shapleyæƒé‡
    fn calculate_shapley_weight(&self, coalition_size: usize, n_features: usize) -> f64 {
        if n_features == 0 {
            return 1.0;
        }
        
        let factorial = |n: usize| -> f64 {
            (1..=n).map(|i| i as f64).product()
        };
        
        let numerator = factorial(coalition_size) * factorial(n_features - coalition_size - 1);
        let denominator = factorial(n_features);
        
        numerator / denominator
    }

    /// è®¡ç®—ç‰¹å¾äº¤äº’æ•ˆåº”
    async fn compute_interaction_effects<F>(
        &self,
        predict_fn: F,
        features: &Array2<f64>,
        background_data: &Array2<f64>,
    ) -> Result<Array2<f64>>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = features.ncols();
        let mut interaction_matrix = Array2::zeros((n_features, n_features));
        
        let background_mean = self.compute_background_mean(background_data);
        
        // è®¡ç®—æ‰€æœ‰ç‰¹å¾å¯¹çš„äº¤äº’æ•ˆåº”
        for i in 0..n_features {
            for j in i+1..n_features {
                let interaction = self.compute_pairwise_interaction(
                    predict_fn,
                    features,
                    &background_mean,
                    i,
                    j,
                ).await?;
                
                interaction_matrix[[i, j]] = interaction;
                interaction_matrix[[j, i]] = interaction; // å¯¹ç§°çŸ©é˜µ
            }
        }
        
        Ok(interaction_matrix)
    }

    /// è®¡ç®—ä¸¤ä¸ªç‰¹å¾çš„äº¤äº’æ•ˆåº”
    async fn compute_pairwise_interaction<F>(
        &self,
        predict_fn: F,
        features: &Array2<f64>,
        background_mean: &Array1<f64>,
        feature_i: usize,
        feature_j: usize,
    ) -> Result<f64>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_samples = features.nrows();
        let n_features = features.ncols();
        let mut total_interaction = 0.0;
        
        for sample_idx in 0..n_samples {
            let sample = features.row(sample_idx);
            
            // åˆ›å»ºå››ç§æƒ…å†µçš„ç‰¹å¾å‘é‡
            let mut baseline = Array2::from_shape_fn((1, n_features), |(_, k)| background_mean[k]);
            let mut with_i = baseline.clone();
            let mut with_j = baseline.clone();
            let mut with_both = baseline.clone();
            
            with_i[[0, feature_i]] = sample[feature_i];
            with_j[[0, feature_j]] = sample[feature_j];
            with_both[[0, feature_i]] = sample[feature_i];
            with_both[[0, feature_j]] = sample[feature_j];
            
            // è®¡ç®—é¢„æµ‹å€¼
            let pred_baseline = predict_fn(&baseline)?[0];
            let pred_with_i = predict_fn(&with_i)?[0];
            let pred_with_j = predict_fn(&with_j)?[0];
            let pred_with_both = predict_fn(&with_both)?[0];
            
            // è®¡ç®—äº¤äº’æ•ˆåº”ï¼šf(i,j) - f(i) - f(j) + f(baseline)
            let interaction = pred_with_both - pred_with_i - pred_with_j + pred_baseline;
            total_interaction += interaction;
        }
        
        Ok(total_interaction / n_samples as f64)
    }
}

/// ç”Ÿäº§çº§LIMEè§£é‡Šå™¨
#[derive(Debug)]
pub struct ProductionLimeExplainer {
    /// é‚»åŸŸå¤§å°
    neighborhood_size: usize,
    /// æ‰°åŠ¨ç­–ç•¥
    perturbation_strategy: PerturbationStrategy,
    /// æ­£åˆ™åŒ–å‚æ•°
    regularization_alpha: f64,
}

#[derive(Debug, Clone)]
pub enum PerturbationStrategy {
    /// é«˜æ–¯å™ªå£°
    GaussianNoise { std_dev: f64 },
    /// ç‰¹å¾æ©è”½
    FeatureMasking { mask_probability: f64 },
    /// æ’å€¼æ‰°åŠ¨
    Interpolation { background_samples: usize },
}

impl ProductionLimeExplainer {
    pub fn new() -> Self {
        Self {
            neighborhood_size: std::env::var("CELUE_LIME_NEIGHBORHOOD_SIZE")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(1000),
            perturbation_strategy: PerturbationStrategy::GaussianNoise { std_dev: 0.1 },
            regularization_alpha: std::env::var("CELUE_LIME_REGULARIZATION_ALPHA")
                .ok().and_then(|s| s.parse().ok())
                .unwrap_or(0.01),
        }
    }

    /// è§£é‡Šå•ä¸ªé¢„æµ‹çš„å±€éƒ¨è¡Œä¸º
    pub async fn explain_instance<F>(
        &self,
        predict_fn: F,
        instance: &Array1<f64>,
        feature_names: &[String],
        background_data: &Array2<f64>,
    ) -> Result<LimeExplanation>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        info!("ğŸ” å¼€å§‹ç”Ÿäº§çº§LIMEåˆ†æ");
        
        // ç”Ÿæˆå±€éƒ¨é‚»åŸŸ
        let (neighborhood_features, neighborhood_predictions) = 
            self.generate_neighborhood(predict_fn, instance, background_data).await?;
        
        // è®¡ç®—å®ä¾‹æƒé‡ï¼ˆè·ç¦»åŸå®ä¾‹è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
        let weights = self.compute_instance_weights(instance, &neighborhood_features);
        
        // è®­ç»ƒå±€éƒ¨çº¿æ€§æ¨¡å‹
        let (feature_weights, model_fidelity) = self.fit_local_model(
            &neighborhood_features,
            &neighborhood_predictions,
            &weights,
            feature_names,
        ).await?;
        
        info!("âœ… LIMEåˆ†æå®Œæˆï¼Œæ¨¡å‹ä¿çœŸåº¦: {:.4}", model_fidelity);
        
        Ok(LimeExplanation {
            feature_weights,
            model_fidelity,
            neighborhood_size: self.neighborhood_size,
            perturbation_strategy: format!("{:?}", self.perturbation_strategy),
        })
    }

    /// ç”Ÿæˆå±€éƒ¨é‚»åŸŸæ ·æœ¬
    async fn generate_neighborhood<F>(
        &self,
        predict_fn: F,
        instance: &Array1<f64>,
        background_data: &Array2<f64>,
    ) -> Result<(Array2<f64>, Array1<f64>)>
    where
        F: Fn(&Array2<f64>) -> Result<Array1<f64>>,
    {
        let n_features = instance.len();
        let mut neighborhood_features = Array2::zeros((self.neighborhood_size, n_features));
        
        // ç”Ÿæˆæ‰°åŠ¨æ ·æœ¬
        for i in 0..self.neighborhood_size {
            let perturbed_instance = self.perturb_instance(instance, background_data, i);
            for j in 0..n_features {
                neighborhood_features[[i, j]] = perturbed_instance[j];
            }
        }
        
        // è·å–é‚»åŸŸé¢„æµ‹
        let neighborhood_predictions = predict_fn(&neighborhood_features)?;
        
        Ok((neighborhood_features, neighborhood_predictions))
    }

    /// æ‰°åŠ¨å®ä¾‹ç”Ÿæˆé‚»åŸŸæ ·æœ¬
    fn perturb_instance(
        &self,
        instance: &Array1<f64>,
        background_data: &Array2<f64>,
        seed: usize,
    ) -> Array1<f64> {
        use rand::{Rng, SeedableRng};
        let mut rng = rand::rngs::StdRng::seed_from_u64(seed as u64);
        
        match &self.perturbation_strategy {
            PerturbationStrategy::GaussianNoise { std_dev } => {
                let mut perturbed = instance.clone();
                for i in 0..perturbed.len() {
                    let noise = rng.gen::<f64>() * std_dev * 2.0 - std_dev;
                    perturbed[i] += noise;
                }
                perturbed
            }
            PerturbationStrategy::FeatureMasking { mask_probability } => {
                let mut perturbed = instance.clone();
                let background_mean = Array1::from_shape_fn(instance.len(), |i| {
                    background_data.column(i).mean().unwrap_or(0.0)
                });
                
                for i in 0..perturbed.len() {
                    if rng.gen::<f64>() < *mask_probability {
                        perturbed[i] = background_mean[i];
                    }
                }
                perturbed
            }
            PerturbationStrategy::Interpolation { background_samples } => {
                let background_idx = rng.gen_range(0..background_data.nrows().min(*background_samples));
                let background_instance = background_data.row(background_idx);
                let alpha = rng.gen::<f64>();
                
                Array1::from_shape_fn(instance.len(), |i| {
                    alpha * instance[i] + (1.0 - alpha) * background_instance[i]
                })
            }
        }
    }

    /// è®¡ç®—å®ä¾‹æƒé‡ï¼ˆåŸºäºè·ç¦»ï¼‰
    fn compute_instance_weights(
        &self,
        original_instance: &Array1<f64>,
        neighborhood_features: &Array2<f64>,
    ) -> Array1<f64> {
        let n_samples = neighborhood_features.nrows();
        Array1::from_shape_fn(n_samples, |i| {
            let neighbor = neighborhood_features.row(i);
            let distance = self.euclidean_distance(original_instance, &neighbor);
            (-distance * distance / 0.25).exp() // é«˜æ–¯æ ¸æƒé‡
        })
    }

    /// è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
    fn euclidean_distance(
        &self,
        a: &Array1<f64>,
        b: &ndarray::ArrayView1<f64>,
    ) -> f64 {
        a.iter()
            .zip(b.iter())
            .map(|(x, y)| (x - y).powi(2))
            .sum::<f64>()
            .sqrt()
    }

    /// æ‹Ÿåˆå±€éƒ¨çº¿æ€§æ¨¡å‹
    async fn fit_local_model(
        &self,
        features: &Array2<f64>,
        targets: &Array1<f64>,
        weights: &Array1<f64>,
        feature_names: &[String],
    ) -> Result<(HashMap<String, f64>, f64)> {
        // ä½¿ç”¨åŠ æƒæœ€å°äºŒä¹˜æ³•æ‹Ÿåˆçº¿æ€§æ¨¡å‹
        let n_features = features.ncols();
        let mut feature_weights = HashMap::new();
        
        // ç®€åŒ–çš„åŠ æƒçº¿æ€§å›å½’å®ç°
        for (i, feature_name) in feature_names.iter().enumerate() {
            let feature_column = features.column(i);
            
            // è®¡ç®—åŠ æƒç›¸å…³ç³»æ•°ä½œä¸ºç‰¹å¾æƒé‡
            let weighted_covariance = feature_column
                .iter()
                .zip(targets.iter())
                .zip(weights.iter())
                .map(|((x, y), w)| w * (x - feature_column.mean().unwrap_or(0.0)) * (y - targets.mean().unwrap_or(0.0)))
                .sum::<f64>();
            
            let weighted_variance = feature_column
                .iter()
                .zip(weights.iter())
                .map(|(x, w)| w * (x - feature_column.mean().unwrap_or(0.0)).powi(2))
                .sum::<f64>();
            
            let weight = if weighted_variance > 1e-8 {
                weighted_covariance / weighted_variance
            } else {
                0.0
            };
            
            feature_weights.insert(feature_name.clone(), weight);
        }
        
        // è®¡ç®—æ¨¡å‹ä¿çœŸåº¦ï¼ˆRÂ²ï¼‰
        let predicted_values = self.predict_with_linear_model(&feature_weights, features, feature_names);
        let model_fidelity = self.calculate_r_squared(targets, &predicted_values, weights);
        
        Ok((feature_weights, model_fidelity))
    }

    /// ä½¿ç”¨çº¿æ€§æ¨¡å‹è¿›è¡Œé¢„æµ‹
    fn predict_with_linear_model(
        &self,
        weights: &HashMap<String, f64>,
        features: &Array2<f64>,
        feature_names: &[String],
    ) -> Array1<f64> {
        let n_samples = features.nrows();
        Array1::from_shape_fn(n_samples, |i| {
            feature_names
                .iter()
                .enumerate()
                .map(|(j, name)| {
                    weights.get(name).unwrap_or(&0.0) * features[[i, j]]
                })
                .sum()
        })
    }

    /// è®¡ç®—åŠ æƒRÂ²
    fn calculate_r_squared(
        &self,
        actual: &Array1<f64>,
        predicted: &Array1<f64>,
        weights: &Array1<f64>,
    ) -> f64 {
        let weighted_mean = actual
            .iter()
            .zip(weights.iter())
            .map(|(a, w)| a * w)
            .sum::<f64>() / weights.sum();
        
        let total_sum_squares = actual
            .iter()
            .zip(weights.iter())
            .map(|(a, w)| w * (a - weighted_mean).powi(2))
            .sum::<f64>();
        
        let residual_sum_squares = actual
            .iter()
            .zip(predicted.iter())
            .zip(weights.iter())
            .map(|((a, p), w)| w * (a - p).powi(2))
            .sum::<f64>();
        
        if total_sum_squares > 1e-8 {
            1.0 - (residual_sum_squares / total_sum_squares)
        } else {
            0.0
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_production_shap_explainer() {
        let explainer = ProductionShapExplainer::new();
        
        // åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        let features = Array2::from_shape_vec((10, 3), (0..30).map(|i| i as f64).collect()).unwrap();
        let background = Array2::from_shape_vec((5, 3), (0..15).map(|i| i as f64).collect()).unwrap();
        let feature_names = vec!["f1".to_string(), "f2".to_string(), "f3".to_string()];
        
        // æ¨¡æ‹Ÿé¢„æµ‹å‡½æ•°
        let predict_fn = |x: &Array2<f64>| -> Result<Array1<f64>> {
            Ok(Array1::from_shape_fn(x.nrows(), |i| {
                x.row(i).sum()
            }))
        };
        
        let result = explainer.explain_prediction(
            predict_fn,
            &features,
            &background,
            &feature_names,
        ).await;
        
        assert!(result.is_ok());
        let shap_values = result.unwrap();
        assert_eq!(shap_values.values.nrows(), 10);
        assert_eq!(shap_values.values.ncols(), 3);
        assert_eq!(shap_values.feature_names.len(), 3);
    }
    
    #[tokio::test]
    async fn test_production_lime_explainer() {
        let explainer = ProductionLimeExplainer::new();
        
        let instance = Array1::from_vec(vec![1.0, 2.0, 3.0]);
        let background = Array2::from_shape_vec((5, 3), (0..15).map(|i| i as f64).collect()).unwrap();
        let feature_names = vec!["f1".to_string(), "f2".to_string(), "f3".to_string()];
        
        let predict_fn = |x: &Array2<f64>| -> Result<Array1<f64>> {
            Ok(Array1::from_shape_fn(x.nrows(), |i| {
                x.row(i).sum()
            }))
        };
        
        let result = explainer.explain_instance(
            predict_fn,
            &instance,
            &feature_names,
            &background,
        ).await;
        
        assert!(result.is_ok());
        let explanation = result.unwrap();
        assert_eq!(explanation.feature_weights.len(), 3);
        assert!(explanation.model_fidelity >= 0.0);
    }
} 