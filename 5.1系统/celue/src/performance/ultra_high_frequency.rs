use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, AtomicBool, Ordering};
use crossbeam::queue::ArrayQueue;
use parking_lot::{RwLock, Mutex};
use tokio::sync::Semaphore;
use aligned_vec::{AVec, ConstAlign};
use bytemuck::{Pod, Zeroable};
use std::time::Instant;
use crate::performance::simd_fixed_point::{SIMDFixedPointProcessor, FixedPrice};

/// è¶…é«˜é¢‘æ•°æ®å¤„ç†å™¨ - é’ˆå¯¹100,000æ¡/ç§’ä¼˜åŒ–
pub struct UltraHighFrequencyProcessor {
    /// æ— é”è¾“å…¥é˜Ÿåˆ— - ä½¿ç”¨å¤šä¸ªé˜Ÿåˆ—å‡å°‘ç«äº‰
    input_queues: Vec<Arc<ArrayQueue<ProcessingTask>>>,
    /// æ‰¹å¤„ç†ç¼“å†²åŒºæ± 
    buffer_pool: Arc<BufferPool>,
    /// SIMDå¤„ç†å™¨æ± 
    simd_processors: Vec<Arc<SIMDFixedPointProcessor>>,
    /// å·¥ä½œçº¿ç¨‹æ§åˆ¶
    worker_control: Arc<WorkerControl>,
    /// æ€§èƒ½ç›‘æ§
    perf_monitor: Arc<PerformanceMonitor>,
    /// é˜Ÿåˆ—è½®è¯¢ç´¢å¼•
    queue_index: AtomicUsize,
    /// æ‰¹å¤„ç†å¤§å° - åŠ¨æ€è°ƒæ•´
    batch_size: AtomicUsize,
}

/// å¤„ç†ä»»åŠ¡
#[repr(C)]
#[derive(Clone, Copy, Pod, Zeroable)]
pub struct ProcessingTask {
    pub timestamp_nanos: u64,
    pub buy_price: u64,    // FixedPriceæ ¼å¼
    pub sell_price: u64,   // FixedPriceæ ¼å¼
    pub volume: u64,       // FixedPriceæ ¼å¼
    pub exchange_id: u32,
    pub symbol_id: u32,
    pub task_type: u32,    // 0=inter_exchange, 1=triangular
    pub padding: u32,      // ä¿æŒ64å­—èŠ‚å¯¹é½
}

/// ç¼“å†²åŒºæ±  - é¿å…å†…å­˜åˆ†é…
pub struct BufferPool {
    /// å¯¹é½çš„ç¼“å†²åŒºæ± 
    buffers: ArrayQueue<AVec<ProcessingTask, ConstAlign<64>>>,
    /// ç¼“å†²åŒºå¤§å°
    buffer_size: usize,
    /// æ± å¤§å°
    pool_size: usize,
}

/// å·¥ä½œçº¿ç¨‹æ§åˆ¶
pub struct WorkerControl {
    /// æ´»è·ƒçº¿ç¨‹æ•°
    active_workers: AtomicUsize,
    /// åœæ­¢æ ‡å¿—
    should_stop: AtomicBool,
    /// çº¿ç¨‹å¹¶å‘æ§åˆ¶
    worker_semaphore: Semaphore,
    /// CPUäº²å’Œæ€§é…ç½®
    cpu_affinity: Vec<usize>,
}

/// æ€§èƒ½ç›‘æ§
pub struct PerformanceMonitor {
    /// å¤„ç†çš„ä»»åŠ¡æ•°
    processed_tasks: AtomicUsize,
    /// æ€»å»¶è¿Ÿï¼ˆçº³ç§’ï¼‰
    total_latency_ns: AtomicUsize,
    /// æœ€å¤§å»¶è¿Ÿï¼ˆçº³ç§’ï¼‰
    max_latency_ns: AtomicUsize,
    /// å½“å‰æ‰¹å¤„ç†å¤§å°
    current_batch_size: AtomicUsize,
    /// å¼€å§‹æ—¶é—´
    start_time: Instant,
}

impl UltraHighFrequencyProcessor {
    /// åˆ›å»ºè¶…é«˜é¢‘å¤„ç†å™¨
    pub fn new(config: UltraHFConfig) -> Arc<Self> {
        let num_queues = config.num_input_queues;
        let queue_capacity = config.queue_capacity;
        
        // åˆ›å»ºå¤šä¸ªè¾“å…¥é˜Ÿåˆ—å‡å°‘é”ç«äº‰
        let input_queues: Vec<_> = (0..num_queues)
            .map(|_| Arc::new(ArrayQueue::new(queue_capacity)))
            .collect();
        
        // åˆ›å»ºç¼“å†²åŒºæ± 
        let buffer_pool = Arc::new(BufferPool::new(
            config.buffer_size, 
            config.buffer_pool_size
        ));
        
        // åˆ›å»ºSIMDå¤„ç†å™¨æ± 
        let simd_processors: Vec<_> = (0..config.num_simd_processors)
            .map(|_| Arc::new(SIMDFixedPointProcessor::new(config.simd_batch_size)))
            .collect();
        
        // å·¥ä½œçº¿ç¨‹æ§åˆ¶
        let worker_control = Arc::new(WorkerControl {
            active_workers: AtomicUsize::new(0),
            should_stop: AtomicBool::new(false),
            worker_semaphore: Semaphore::new(config.max_workers),
            cpu_affinity: config.cpu_affinity,
        });
        
        // æ€§èƒ½ç›‘æ§
        let perf_monitor = Arc::new(PerformanceMonitor {
            processed_tasks: AtomicUsize::new(0),
            total_latency_ns: AtomicUsize::new(0),
            max_latency_ns: AtomicUsize::new(0),
            current_batch_size: AtomicUsize::new(config.initial_batch_size),
            start_time: Instant::now(),
        });
        
        Arc::new(Self {
            input_queues,
            buffer_pool,
            simd_processors,
            worker_control,
            perf_monitor,
            queue_index: AtomicUsize::new(0),
            batch_size: AtomicUsize::new(config.initial_batch_size),
        })
    }
    
    /// å¯åŠ¨è¶…é«˜é¢‘å¤„ç†å™¨
    pub async fn start(&self, num_workers: usize) {
        println!("ğŸš€ å¯åŠ¨è¶…é«˜é¢‘å¤„ç†å™¨ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {}", num_workers);
        
        // å¯åŠ¨å¤„ç†å·¥ä½œçº¿ç¨‹
        for worker_id in 0..num_workers {
            let processor = Arc::clone(self);
            let cpu_core = if worker_id < self.worker_control.cpu_affinity.len() {
                Some(self.worker_control.cpu_affinity[worker_id])
            } else {
                None
            };
            
            tokio::spawn(async move {
                processor.worker_loop(worker_id, cpu_core).await;
            });
        }
        
        // å¯åŠ¨æ€§èƒ½ç›‘æ§çº¿ç¨‹
        let monitor = Arc::clone(&self.perf_monitor);
        tokio::spawn(async move {
            Self::performance_monitor_loop(monitor).await;
        });
        
        // å¯åŠ¨åŠ¨æ€æ‰¹å¤„ç†è°ƒæ•´
        let processor = Arc::clone(self);
        tokio::spawn(async move {
            processor.dynamic_batch_adjustment().await;
        });
    }
    
    /// æ— é”æäº¤ä»»åŠ¡
    pub fn submit_task(&self, task: ProcessingTask) -> bool {
        // è½®è¯¢é€‰æ‹©é˜Ÿåˆ—å‡å°‘ç«äº‰
        let queue_idx = self.queue_index.fetch_add(1, Ordering::Relaxed) % self.input_queues.len();
        
        // å°è¯•æäº¤åˆ°é€‰å®šé˜Ÿåˆ—
        if self.input_queues[queue_idx].push(task).is_ok() {
            return true;
        }
        
        // å¦‚æœå¤±è´¥ï¼Œå°è¯•å…¶ä»–é˜Ÿåˆ—
        for i in 0..self.input_queues.len() {
            let idx = (queue_idx + i + 1) % self.input_queues.len();
            if self.input_queues[idx].push(task).is_ok() {
                return true;
            }
        }
        
        false // æ‰€æœ‰é˜Ÿåˆ—éƒ½æ»¡äº†
    }
    
    /// å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯
    async fn worker_loop(&self, worker_id: usize, cpu_core: Option<usize>) {
        // è®¾ç½®CPUäº²å’Œæ€§
        if let Some(core) = cpu_core {
            self.set_cpu_affinity(core);
        }
        
        self.worker_control.active_workers.fetch_add(1, Ordering::SeqCst);
        
        println!("âš¡ å·¥ä½œçº¿ç¨‹ {} å¯åŠ¨ï¼ŒCPUæ ¸å¿ƒ: {:?}", worker_id, cpu_core);
        
        // è·å–ä¸“ç”¨ç¼“å†²åŒº
        let mut buffer = self.buffer_pool.get_buffer().await;
        let current_batch_size = self.batch_size.load(Ordering::Relaxed);
        let simd_processor_idx = worker_id % self.simd_processors.len();
        let simd_processor = &self.simd_processors[simd_processor_idx];
        
        while !self.worker_control.should_stop.load(Ordering::Relaxed) {
            // æ‰¹é‡æ”¶é›†ä»»åŠ¡
            let collected = self.collect_batch_tasks(&mut buffer, current_batch_size).await;
            
            if collected > 0 {
                // æ‰¹é‡å¤„ç†
                let start_time = Instant::now();
                self.process_batch_simd(&buffer[..collected], simd_processor).await;
                let processing_time = start_time.elapsed();
                
                // æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                self.update_performance_metrics(collected, processing_time);
                
                buffer.clear();
            } else {
                // æ²¡æœ‰ä»»åŠ¡æ—¶çŸ­æš‚ä¼‘çœ 
                tokio::task::yield_now().await;
            }
        }
        
        // å½’è¿˜ç¼“å†²åŒº
        self.buffer_pool.return_buffer(buffer).await;
        self.worker_control.active_workers.fetch_sub(1, Ordering::SeqCst);
        
        println!("ğŸ›‘ å·¥ä½œçº¿ç¨‹ {} åœæ­¢", worker_id);
    }
    
    /// æ‰¹é‡æ”¶é›†ä»»åŠ¡
    async fn collect_batch_tasks(
        &self, 
        buffer: &mut AVec<ProcessingTask, ConstAlign<64>>, 
        target_size: usize
    ) -> usize {
        let mut collected = 0;
        let start_queue = self.queue_index.load(Ordering::Relaxed) % self.input_queues.len();
        
        // ä»å¤šä¸ªé˜Ÿåˆ—æ”¶é›†ä»»åŠ¡ï¼Œé¿å…é¥¥é¥¿
        for round in 0..target_size {
            let queue_idx = (start_queue + round) % self.input_queues.len();
            
            if let Some(task) = self.input_queues[queue_idx].pop() {
                buffer.push(task);
                collected += 1;
                
                if collected >= target_size {
                    break;
                }
            }
            
            // å¦‚æœå•è½®æ”¶é›†ä¸è¶³ï¼Œç»§ç»­ä¸‹ä¸€è½®
            if round == self.input_queues.len() - 1 && collected < target_size / 4 {
                break; // é¿å…ç©ºè½¬
            }
        }
        
        collected
    }
    
    /// SIMDæ‰¹é‡å¤„ç†
    async fn process_batch_simd(
        &self,
        tasks: &[ProcessingTask],
        simd_processor: &SIMDFixedPointProcessor,
    ) {
        if tasks.is_empty() {
            return;
        }
        
        // å‡†å¤‡SIMDè¾“å…¥æ•°æ®
        let buy_prices: Vec<FixedPrice> = tasks.iter()
            .map(|t| FixedPrice::from_raw(t.buy_price))
            .collect();
        
        let sell_prices: Vec<FixedPrice> = tasks.iter()
            .map(|t| FixedPrice::from_raw(t.sell_price))
            .collect();
        
        let volumes: Vec<FixedPrice> = tasks.iter()
            .map(|t| FixedPrice::from_raw(t.volume))
            .collect();
        
        // AVX-512å¹¶è¡Œè®¡ç®—åˆ©æ¶¦
        match simd_processor.calculate_profit_batch_optimal(&buy_prices, &sell_prices, &volumes) {
            Ok(profits) => {
                // å¤„ç†æœ‰åˆ©å¯å›¾çš„æœºä¼š
                for (i, profit) in profits.iter().enumerate() {
                    if profit.to_f64() > 0.001 { // æœ€å°åˆ©æ¶¦é˜ˆå€¼
                        self.handle_profitable_opportunity(&tasks[i], *profit).await;
                    }
                }
            }
            Err(e) => {
                eprintln!("âš ï¸ SIMDå¤„ç†é”™è¯¯: {}", e);
            }
        }
    }
    
    /// å¤„ç†ç›ˆåˆ©æœºä¼š
    async fn handle_profitable_opportunity(&self, task: &ProcessingTask, profit: FixedPrice) {
        // è¿™é‡Œé›†æˆå®é™…çš„äº¤æ˜“ç­–ç•¥é€»è¾‘
        // åŒ…æ‹¬é£é™©æ§åˆ¶ã€è®¢å•æ‰§è¡Œç­‰
        
        // å¿«é€Ÿé£é™©æ£€æŸ¥
        if self.quick_risk_check(task, profit) {
            // è®°å½•æœºä¼šï¼ˆå¼‚æ­¥ï¼‰
            tokio::spawn(async move {
                // å®é™…çš„äº¤æ˜“æ‰§è¡Œé€»è¾‘
                println!("ğŸ’° å‘ç°ç›ˆåˆ©æœºä¼š: äº¤æ˜“å¯¹={}-{}, åˆ©æ¶¦={:.6}", 
                    task.symbol_id, task.exchange_id, profit.to_f64());
            });
        }
    }
    
    /// å¿«é€Ÿé£é™©æ£€æŸ¥
    fn quick_risk_check(&self, task: &ProcessingTask, profit: FixedPrice) -> bool {
        // å®ç°å¿«é€Ÿé£é™©æ£€æŸ¥é€»è¾‘
        let profit_pct = profit.to_f64() / FixedPrice::from_raw(task.buy_price).to_f64();
        
        // åŸºæœ¬æ£€æŸ¥ï¼šåˆ©æ¶¦ç‡åˆç†æ€§
        profit_pct > 0.001 && profit_pct < 0.1 // 0.1% - 10%
    }
    
    /// æ›´æ–°æ€§èƒ½æŒ‡æ ‡
    fn update_performance_metrics(&self, processed_count: usize, processing_time: std::time::Duration) {
        let latency_ns = processing_time.as_nanos() as usize;
        
        self.perf_monitor.processed_tasks.fetch_add(processed_count, Ordering::Relaxed);
        self.perf_monitor.total_latency_ns.fetch_add(latency_ns, Ordering::Relaxed);
        
        // æ›´æ–°æœ€å¤§å»¶è¿Ÿ
        let current_max = self.perf_monitor.max_latency_ns.load(Ordering::Relaxed);
        if latency_ns > current_max {
            self.perf_monitor.max_latency_ns.compare_exchange_weak(
                current_max, latency_ns, Ordering::Relaxed, Ordering::Relaxed
            ).ok();
        }
    }
    
    /// åŠ¨æ€æ‰¹å¤„ç†å¤§å°è°ƒæ•´
    async fn dynamic_batch_adjustment(&self) {
        let mut adjustment_interval = tokio::time::interval(std::time::Duration::from_millis(100));
        
        loop {
            adjustment_interval.tick().await;
            
            let processed = self.perf_monitor.processed_tasks.load(Ordering::Relaxed);
            let total_latency = self.perf_monitor.total_latency_ns.load(Ordering::Relaxed);
            
            if processed > 0 {
                let avg_latency_ns = total_latency / processed;
                let current_batch = self.batch_size.load(Ordering::Relaxed);
                
                // åŠ¨æ€è°ƒæ•´ç­–ç•¥
                let new_batch_size = if avg_latency_ns > 100_000 { // è¶…è¿‡100å¾®ç§’
                    (current_batch * 8 / 10).max(256) // å‡å°‘20%ï¼Œæœ€å°256
                } else if avg_latency_ns < 50_000 { // å°äº50å¾®ç§’
                    (current_batch * 12 / 10).min(4096) // å¢åŠ 20%ï¼Œæœ€å¤§4096
                } else {
                    current_batch // ä¿æŒä¸å˜
                };
                
                if new_batch_size != current_batch {
                    self.batch_size.store(new_batch_size, Ordering::Relaxed);
                    println!("ğŸ“Š åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°: {} -> {}, å¹³å‡å»¶è¿Ÿ: {}ns", 
                        current_batch, new_batch_size, avg_latency_ns);
                }
            }
        }
    }
    
    /// æ€§èƒ½ç›‘æ§å¾ªç¯
    async fn performance_monitor_loop(monitor: Arc<PerformanceMonitor>) {
        let mut report_interval = tokio::time::interval(std::time::Duration::from_secs(5));
        
        loop {
            report_interval.tick().await;
            
            let processed = monitor.processed_tasks.load(Ordering::Relaxed);
            let total_latency = monitor.total_latency_ns.load(Ordering::Relaxed);
            let max_latency = monitor.max_latency_ns.load(Ordering::Relaxed);
            let elapsed = monitor.start_time.elapsed().as_secs_f64();
            
            if processed > 0 && elapsed > 0.0 {
                let throughput = processed as f64 / elapsed;
                let avg_latency_us = (total_latency / processed) as f64 / 1000.0;
                let max_latency_us = max_latency as f64 / 1000.0;
                
                println!("âš¡ æ€§èƒ½æŠ¥å‘Š: ååé‡={:.0}æ¡/ç§’, å¹³å‡å»¶è¿Ÿ={:.1}Î¼s, æœ€å¤§å»¶è¿Ÿ={:.1}Î¼s", 
                    throughput, avg_latency_us, max_latency_us);
                
                // é‡ç½®ç»Ÿè®¡
                monitor.processed_tasks.store(0, Ordering::Relaxed);
                monitor.total_latency_ns.store(0, Ordering::Relaxed);
                monitor.max_latency_ns.store(0, Ordering::Relaxed);
            }
        }
    }
    
    /// è®¾ç½®CPUäº²å’Œæ€§
    fn set_cpu_affinity(&self, core: usize) {
        #[cfg(target_os = "linux")]
        {
            use std::mem;
            use libc::{cpu_set_t, sched_setaffinity, CPU_SET, CPU_ZERO};
            
            unsafe {
                let mut cpu_set: cpu_set_t = mem::zeroed();
                CPU_ZERO(&mut cpu_set);
                CPU_SET(core, &mut cpu_set);
                
                let result = sched_setaffinity(
                    0, // å½“å‰çº¿ç¨‹
                    mem::size_of::<cpu_set_t>(),
                    &cpu_set,
                );
                
                if result == 0 {
                    println!("âœ… çº¿ç¨‹ç»‘å®šåˆ°CPUæ ¸å¿ƒ {}", core);
                } else {
                    eprintln!("âš ï¸ è®¾ç½®CPUäº²å’Œæ€§å¤±è´¥: æ ¸å¿ƒ {}", core);
                }
            }
        }
    }
    
    /// è·å–æ€§èƒ½ç»Ÿè®¡
    pub fn get_performance_stats(&self) -> PerformanceStats {
        let processed = self.perf_monitor.processed_tasks.load(Ordering::Relaxed);
        let total_latency = self.perf_monitor.total_latency_ns.load(Ordering::Relaxed);
        let max_latency = self.perf_monitor.max_latency_ns.load(Ordering::Relaxed);
        let elapsed = self.perf_monitor.start_time.elapsed().as_secs_f64();
        
        PerformanceStats {
            throughput: if elapsed > 0.0 { processed as f64 / elapsed } else { 0.0 },
            avg_latency_us: if processed > 0 { (total_latency / processed) as f64 / 1000.0 } else { 0.0 },
            max_latency_us: max_latency as f64 / 1000.0,
            processed_tasks: processed,
            active_workers: self.worker_control.active_workers.load(Ordering::Relaxed),
            current_batch_size: self.batch_size.load(Ordering::Relaxed),
        }
    }
    
    /// åœæ­¢å¤„ç†å™¨
    pub async fn stop(&self) {
        println!("ğŸ›‘ åœæ­¢è¶…é«˜é¢‘å¤„ç†å™¨...");
        self.worker_control.should_stop.store(true, Ordering::SeqCst);
        
        // ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹åœæ­¢
        while self.worker_control.active_workers.load(Ordering::SeqCst) > 0 {
            tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        }
        
        println!("âœ… è¶…é«˜é¢‘å¤„ç†å™¨å·²åœæ­¢");
    }
}

impl BufferPool {
    fn new(buffer_size: usize, pool_size: usize) -> Self {
        let buffers = ArrayQueue::new(pool_size);
        
        // é¢„åˆ†é…å¯¹é½ç¼“å†²åŒº
        for _ in 0..pool_size {
            let mut buffer = AVec::with_capacity_aligned(buffer_size);
            buffer.resize(0, ProcessingTask::zeroed());
            buffers.push(buffer).ok();
        }
        
        Self {
            buffers,
            buffer_size,
            pool_size,
        }
    }
    
    async fn get_buffer(&self) -> AVec<ProcessingTask, ConstAlign<64>> {
        // å°è¯•ä»æ± ä¸­è·å–
        if let Some(buffer) = self.buffers.pop() {
            return buffer;
        }
        
        // æ± ä¸ºç©ºæ—¶åˆ›å»ºæ–°ç¼“å†²åŒº
        let mut buffer = AVec::with_capacity_aligned(self.buffer_size);
        buffer.resize(0, ProcessingTask::zeroed());
        buffer
    }
    
    async fn return_buffer(&self, mut buffer: AVec<ProcessingTask, ConstAlign<64>>) {
        buffer.clear();
        self.buffers.push(buffer).ok(); // å¦‚æœæ± æ»¡äº†å°±ä¸¢å¼ƒ
    }
}

/// è¶…é«˜é¢‘é…ç½®
pub struct UltraHFConfig {
    pub num_input_queues: usize,
    pub queue_capacity: usize,
    pub buffer_size: usize,
    pub buffer_pool_size: usize,
    pub num_simd_processors: usize,
    pub simd_batch_size: usize,
    pub max_workers: usize,
    pub initial_batch_size: usize,
    pub cpu_affinity: Vec<usize>,
}

impl Default for UltraHFConfig {
    fn default() -> Self {
        Self {
            num_input_queues: 16,        // 16ä¸ªè¾“å…¥é˜Ÿåˆ—
            queue_capacity: 8192,        // æ¯é˜Ÿåˆ—8Kå®¹é‡
            buffer_size: 4096,           // 4Kæ‰¹å¤„ç†ç¼“å†²åŒº
            buffer_pool_size: 32,        // 32ä¸ªç¼“å†²åŒºæ± 
            num_simd_processors: 8,      // 8ä¸ªSIMDå¤„ç†å™¨
            simd_batch_size: 2048,       // SIMDæ‰¹å¤„ç†å¤§å°
            max_workers: 16,             // æœ€å¤§16ä¸ªå·¥ä½œçº¿ç¨‹
            initial_batch_size: 2048,    // åˆå§‹æ‰¹å¤„ç†å¤§å°
            cpu_affinity: (0..16).collect(), // CPUæ ¸å¿ƒ0-15
        }
    }
}

/// æ€§èƒ½ç»Ÿè®¡
#[derive(Debug, Clone)]
pub struct PerformanceStats {
    pub throughput: f64,
    pub avg_latency_us: f64,
    pub max_latency_us: f64,
    pub processed_tasks: usize,
    pub active_workers: usize,
    pub current_batch_size: usize,
} 
use std::sync::atomic::{AtomicUsize, AtomicBool, Ordering};
use crossbeam::queue::ArrayQueue;
use parking_lot::{RwLock, Mutex};
use tokio::sync::Semaphore;
use aligned_vec::{AVec, ConstAlign};
use bytemuck::{Pod, Zeroable};
use std::time::Instant;
use crate::performance::simd_fixed_point::{SIMDFixedPointProcessor, FixedPrice};

/// è¶…é«˜é¢‘æ•°æ®å¤„ç†å™¨ - é’ˆå¯¹100,000æ¡/ç§’ä¼˜åŒ–
pub struct UltraHighFrequencyProcessor {
    /// æ— é”è¾“å…¥é˜Ÿåˆ— - ä½¿ç”¨å¤šä¸ªé˜Ÿåˆ—å‡å°‘ç«äº‰
    input_queues: Vec<Arc<ArrayQueue<ProcessingTask>>>,
    /// æ‰¹å¤„ç†ç¼“å†²åŒºæ± 
    buffer_pool: Arc<BufferPool>,
    /// SIMDå¤„ç†å™¨æ± 
    simd_processors: Vec<Arc<SIMDFixedPointProcessor>>,
    /// å·¥ä½œçº¿ç¨‹æ§åˆ¶
    worker_control: Arc<WorkerControl>,
    /// æ€§èƒ½ç›‘æ§
    perf_monitor: Arc<PerformanceMonitor>,
    /// é˜Ÿåˆ—è½®è¯¢ç´¢å¼•
    queue_index: AtomicUsize,
    /// æ‰¹å¤„ç†å¤§å° - åŠ¨æ€è°ƒæ•´
    batch_size: AtomicUsize,
}

/// å¤„ç†ä»»åŠ¡
#[repr(C)]
#[derive(Clone, Copy, Pod, Zeroable)]
pub struct ProcessingTask {
    pub timestamp_nanos: u64,
    pub buy_price: u64,    // FixedPriceæ ¼å¼
    pub sell_price: u64,   // FixedPriceæ ¼å¼
    pub volume: u64,       // FixedPriceæ ¼å¼
    pub exchange_id: u32,
    pub symbol_id: u32,
    pub task_type: u32,    // 0=inter_exchange, 1=triangular
    pub padding: u32,      // ä¿æŒ64å­—èŠ‚å¯¹é½
}

/// ç¼“å†²åŒºæ±  - é¿å…å†…å­˜åˆ†é…
pub struct BufferPool {
    /// å¯¹é½çš„ç¼“å†²åŒºæ± 
    buffers: ArrayQueue<AVec<ProcessingTask, ConstAlign<64>>>,
    /// ç¼“å†²åŒºå¤§å°
    buffer_size: usize,
    /// æ± å¤§å°
    pool_size: usize,
}

/// å·¥ä½œçº¿ç¨‹æ§åˆ¶
pub struct WorkerControl {
    /// æ´»è·ƒçº¿ç¨‹æ•°
    active_workers: AtomicUsize,
    /// åœæ­¢æ ‡å¿—
    should_stop: AtomicBool,
    /// çº¿ç¨‹å¹¶å‘æ§åˆ¶
    worker_semaphore: Semaphore,
    /// CPUäº²å’Œæ€§é…ç½®
    cpu_affinity: Vec<usize>,
}

/// æ€§èƒ½ç›‘æ§
pub struct PerformanceMonitor {
    /// å¤„ç†çš„ä»»åŠ¡æ•°
    processed_tasks: AtomicUsize,
    /// æ€»å»¶è¿Ÿï¼ˆçº³ç§’ï¼‰
    total_latency_ns: AtomicUsize,
    /// æœ€å¤§å»¶è¿Ÿï¼ˆçº³ç§’ï¼‰
    max_latency_ns: AtomicUsize,
    /// å½“å‰æ‰¹å¤„ç†å¤§å°
    current_batch_size: AtomicUsize,
    /// å¼€å§‹æ—¶é—´
    start_time: Instant,
}

impl UltraHighFrequencyProcessor {
    /// åˆ›å»ºè¶…é«˜é¢‘å¤„ç†å™¨
    pub fn new(config: UltraHFConfig) -> Arc<Self> {
        let num_queues = config.num_input_queues;
        let queue_capacity = config.queue_capacity;
        
        // åˆ›å»ºå¤šä¸ªè¾“å…¥é˜Ÿåˆ—å‡å°‘é”ç«äº‰
        let input_queues: Vec<_> = (0..num_queues)
            .map(|_| Arc::new(ArrayQueue::new(queue_capacity)))
            .collect();
        
        // åˆ›å»ºç¼“å†²åŒºæ± 
        let buffer_pool = Arc::new(BufferPool::new(
            config.buffer_size, 
            config.buffer_pool_size
        ));
        
        // åˆ›å»ºSIMDå¤„ç†å™¨æ± 
        let simd_processors: Vec<_> = (0..config.num_simd_processors)
            .map(|_| Arc::new(SIMDFixedPointProcessor::new(config.simd_batch_size)))
            .collect();
        
        // å·¥ä½œçº¿ç¨‹æ§åˆ¶
        let worker_control = Arc::new(WorkerControl {
            active_workers: AtomicUsize::new(0),
            should_stop: AtomicBool::new(false),
            worker_semaphore: Semaphore::new(config.max_workers),
            cpu_affinity: config.cpu_affinity,
        });
        
        // æ€§èƒ½ç›‘æ§
        let perf_monitor = Arc::new(PerformanceMonitor {
            processed_tasks: AtomicUsize::new(0),
            total_latency_ns: AtomicUsize::new(0),
            max_latency_ns: AtomicUsize::new(0),
            current_batch_size: AtomicUsize::new(config.initial_batch_size),
            start_time: Instant::now(),
        });
        
        Arc::new(Self {
            input_queues,
            buffer_pool,
            simd_processors,
            worker_control,
            perf_monitor,
            queue_index: AtomicUsize::new(0),
            batch_size: AtomicUsize::new(config.initial_batch_size),
        })
    }
    
    /// å¯åŠ¨è¶…é«˜é¢‘å¤„ç†å™¨
    pub async fn start(&self, num_workers: usize) {
        println!("ğŸš€ å¯åŠ¨è¶…é«˜é¢‘å¤„ç†å™¨ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {}", num_workers);
        
        // å¯åŠ¨å¤„ç†å·¥ä½œçº¿ç¨‹
        for worker_id in 0..num_workers {
            let processor = Arc::clone(self);
            let cpu_core = if worker_id < self.worker_control.cpu_affinity.len() {
                Some(self.worker_control.cpu_affinity[worker_id])
            } else {
                None
            };
            
            tokio::spawn(async move {
                processor.worker_loop(worker_id, cpu_core).await;
            });
        }
        
        // å¯åŠ¨æ€§èƒ½ç›‘æ§çº¿ç¨‹
        let monitor = Arc::clone(&self.perf_monitor);
        tokio::spawn(async move {
            Self::performance_monitor_loop(monitor).await;
        });
        
        // å¯åŠ¨åŠ¨æ€æ‰¹å¤„ç†è°ƒæ•´
        let processor = Arc::clone(self);
        tokio::spawn(async move {
            processor.dynamic_batch_adjustment().await;
        });
    }
    
    /// æ— é”æäº¤ä»»åŠ¡
    pub fn submit_task(&self, task: ProcessingTask) -> bool {
        // è½®è¯¢é€‰æ‹©é˜Ÿåˆ—å‡å°‘ç«äº‰
        let queue_idx = self.queue_index.fetch_add(1, Ordering::Relaxed) % self.input_queues.len();
        
        // å°è¯•æäº¤åˆ°é€‰å®šé˜Ÿåˆ—
        if self.input_queues[queue_idx].push(task).is_ok() {
            return true;
        }
        
        // å¦‚æœå¤±è´¥ï¼Œå°è¯•å…¶ä»–é˜Ÿåˆ—
        for i in 0..self.input_queues.len() {
            let idx = (queue_idx + i + 1) % self.input_queues.len();
            if self.input_queues[idx].push(task).is_ok() {
                return true;
            }
        }
        
        false // æ‰€æœ‰é˜Ÿåˆ—éƒ½æ»¡äº†
    }
    
    /// å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯
    async fn worker_loop(&self, worker_id: usize, cpu_core: Option<usize>) {
        // è®¾ç½®CPUäº²å’Œæ€§
        if let Some(core) = cpu_core {
            self.set_cpu_affinity(core);
        }
        
        self.worker_control.active_workers.fetch_add(1, Ordering::SeqCst);
        
        println!("âš¡ å·¥ä½œçº¿ç¨‹ {} å¯åŠ¨ï¼ŒCPUæ ¸å¿ƒ: {:?}", worker_id, cpu_core);
        
        // è·å–ä¸“ç”¨ç¼“å†²åŒº
        let mut buffer = self.buffer_pool.get_buffer().await;
        let current_batch_size = self.batch_size.load(Ordering::Relaxed);
        let simd_processor_idx = worker_id % self.simd_processors.len();
        let simd_processor = &self.simd_processors[simd_processor_idx];
        
        while !self.worker_control.should_stop.load(Ordering::Relaxed) {
            // æ‰¹é‡æ”¶é›†ä»»åŠ¡
            let collected = self.collect_batch_tasks(&mut buffer, current_batch_size).await;
            
            if collected > 0 {
                // æ‰¹é‡å¤„ç†
                let start_time = Instant::now();
                self.process_batch_simd(&buffer[..collected], simd_processor).await;
                let processing_time = start_time.elapsed();
                
                // æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                self.update_performance_metrics(collected, processing_time);
                
                buffer.clear();
            } else {
                // æ²¡æœ‰ä»»åŠ¡æ—¶çŸ­æš‚ä¼‘çœ 
                tokio::task::yield_now().await;
            }
        }
        
        // å½’è¿˜ç¼“å†²åŒº
        self.buffer_pool.return_buffer(buffer).await;
        self.worker_control.active_workers.fetch_sub(1, Ordering::SeqCst);
        
        println!("ğŸ›‘ å·¥ä½œçº¿ç¨‹ {} åœæ­¢", worker_id);
    }
    
    /// æ‰¹é‡æ”¶é›†ä»»åŠ¡
    async fn collect_batch_tasks(
        &self, 
        buffer: &mut AVec<ProcessingTask, ConstAlign<64>>, 
        target_size: usize
    ) -> usize {
        let mut collected = 0;
        let start_queue = self.queue_index.load(Ordering::Relaxed) % self.input_queues.len();
        
        // ä»å¤šä¸ªé˜Ÿåˆ—æ”¶é›†ä»»åŠ¡ï¼Œé¿å…é¥¥é¥¿
        for round in 0..target_size {
            let queue_idx = (start_queue + round) % self.input_queues.len();
            
            if let Some(task) = self.input_queues[queue_idx].pop() {
                buffer.push(task);
                collected += 1;
                
                if collected >= target_size {
                    break;
                }
            }
            
            // å¦‚æœå•è½®æ”¶é›†ä¸è¶³ï¼Œç»§ç»­ä¸‹ä¸€è½®
            if round == self.input_queues.len() - 1 && collected < target_size / 4 {
                break; // é¿å…ç©ºè½¬
            }
        }
        
        collected
    }
    
    /// SIMDæ‰¹é‡å¤„ç†
    async fn process_batch_simd(
        &self,
        tasks: &[ProcessingTask],
        simd_processor: &SIMDFixedPointProcessor,
    ) {
        if tasks.is_empty() {
            return;
        }
        
        // å‡†å¤‡SIMDè¾“å…¥æ•°æ®
        let buy_prices: Vec<FixedPrice> = tasks.iter()
            .map(|t| FixedPrice::from_raw(t.buy_price))
            .collect();
        
        let sell_prices: Vec<FixedPrice> = tasks.iter()
            .map(|t| FixedPrice::from_raw(t.sell_price))
            .collect();
        
        let volumes: Vec<FixedPrice> = tasks.iter()
            .map(|t| FixedPrice::from_raw(t.volume))
            .collect();
        
        // AVX-512å¹¶è¡Œè®¡ç®—åˆ©æ¶¦
        match simd_processor.calculate_profit_batch_optimal(&buy_prices, &sell_prices, &volumes) {
            Ok(profits) => {
                // å¤„ç†æœ‰åˆ©å¯å›¾çš„æœºä¼š
                for (i, profit) in profits.iter().enumerate() {
                    if profit.to_f64() > 0.001 { // æœ€å°åˆ©æ¶¦é˜ˆå€¼
                        self.handle_profitable_opportunity(&tasks[i], *profit).await;
                    }
                }
            }
            Err(e) => {
                eprintln!("âš ï¸ SIMDå¤„ç†é”™è¯¯: {}", e);
            }
        }
    }
    
    /// å¤„ç†ç›ˆåˆ©æœºä¼š
    async fn handle_profitable_opportunity(&self, task: &ProcessingTask, profit: FixedPrice) {
        // è¿™é‡Œé›†æˆå®é™…çš„äº¤æ˜“ç­–ç•¥é€»è¾‘
        // åŒ…æ‹¬é£é™©æ§åˆ¶ã€è®¢å•æ‰§è¡Œç­‰
        
        // å¿«é€Ÿé£é™©æ£€æŸ¥
        if self.quick_risk_check(task, profit) {
            // è®°å½•æœºä¼šï¼ˆå¼‚æ­¥ï¼‰
            tokio::spawn(async move {
                // å®é™…çš„äº¤æ˜“æ‰§è¡Œé€»è¾‘
                println!("ğŸ’° å‘ç°ç›ˆåˆ©æœºä¼š: äº¤æ˜“å¯¹={}-{}, åˆ©æ¶¦={:.6}", 
                    task.symbol_id, task.exchange_id, profit.to_f64());
            });
        }
    }
    
    /// å¿«é€Ÿé£é™©æ£€æŸ¥
    fn quick_risk_check(&self, task: &ProcessingTask, profit: FixedPrice) -> bool {
        // å®ç°å¿«é€Ÿé£é™©æ£€æŸ¥é€»è¾‘
        let profit_pct = profit.to_f64() / FixedPrice::from_raw(task.buy_price).to_f64();
        
        // åŸºæœ¬æ£€æŸ¥ï¼šåˆ©æ¶¦ç‡åˆç†æ€§
        profit_pct > 0.001 && profit_pct < 0.1 // 0.1% - 10%
    }
    
    /// æ›´æ–°æ€§èƒ½æŒ‡æ ‡
    fn update_performance_metrics(&self, processed_count: usize, processing_time: std::time::Duration) {
        let latency_ns = processing_time.as_nanos() as usize;
        
        self.perf_monitor.processed_tasks.fetch_add(processed_count, Ordering::Relaxed);
        self.perf_monitor.total_latency_ns.fetch_add(latency_ns, Ordering::Relaxed);
        
        // æ›´æ–°æœ€å¤§å»¶è¿Ÿ
        let current_max = self.perf_monitor.max_latency_ns.load(Ordering::Relaxed);
        if latency_ns > current_max {
            self.perf_monitor.max_latency_ns.compare_exchange_weak(
                current_max, latency_ns, Ordering::Relaxed, Ordering::Relaxed
            ).ok();
        }
    }
    
    /// åŠ¨æ€æ‰¹å¤„ç†å¤§å°è°ƒæ•´
    async fn dynamic_batch_adjustment(&self) {
        let mut adjustment_interval = tokio::time::interval(std::time::Duration::from_millis(100));
        
        loop {
            adjustment_interval.tick().await;
            
            let processed = self.perf_monitor.processed_tasks.load(Ordering::Relaxed);
            let total_latency = self.perf_monitor.total_latency_ns.load(Ordering::Relaxed);
            
            if processed > 0 {
                let avg_latency_ns = total_latency / processed;
                let current_batch = self.batch_size.load(Ordering::Relaxed);
                
                // åŠ¨æ€è°ƒæ•´ç­–ç•¥
                let new_batch_size = if avg_latency_ns > 100_000 { // è¶…è¿‡100å¾®ç§’
                    (current_batch * 8 / 10).max(256) // å‡å°‘20%ï¼Œæœ€å°256
                } else if avg_latency_ns < 50_000 { // å°äº50å¾®ç§’
                    (current_batch * 12 / 10).min(4096) // å¢åŠ 20%ï¼Œæœ€å¤§4096
                } else {
                    current_batch // ä¿æŒä¸å˜
                };
                
                if new_batch_size != current_batch {
                    self.batch_size.store(new_batch_size, Ordering::Relaxed);
                    println!("ğŸ“Š åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°: {} -> {}, å¹³å‡å»¶è¿Ÿ: {}ns", 
                        current_batch, new_batch_size, avg_latency_ns);
                }
            }
        }
    }
    
    /// æ€§èƒ½ç›‘æ§å¾ªç¯
    async fn performance_monitor_loop(monitor: Arc<PerformanceMonitor>) {
        let mut report_interval = tokio::time::interval(std::time::Duration::from_secs(5));
        
        loop {
            report_interval.tick().await;
            
            let processed = monitor.processed_tasks.load(Ordering::Relaxed);
            let total_latency = monitor.total_latency_ns.load(Ordering::Relaxed);
            let max_latency = monitor.max_latency_ns.load(Ordering::Relaxed);
            let elapsed = monitor.start_time.elapsed().as_secs_f64();
            
            if processed > 0 && elapsed > 0.0 {
                let throughput = processed as f64 / elapsed;
                let avg_latency_us = (total_latency / processed) as f64 / 1000.0;
                let max_latency_us = max_latency as f64 / 1000.0;
                
                println!("âš¡ æ€§èƒ½æŠ¥å‘Š: ååé‡={:.0}æ¡/ç§’, å¹³å‡å»¶è¿Ÿ={:.1}Î¼s, æœ€å¤§å»¶è¿Ÿ={:.1}Î¼s", 
                    throughput, avg_latency_us, max_latency_us);
                
                // é‡ç½®ç»Ÿè®¡
                monitor.processed_tasks.store(0, Ordering::Relaxed);
                monitor.total_latency_ns.store(0, Ordering::Relaxed);
                monitor.max_latency_ns.store(0, Ordering::Relaxed);
            }
        }
    }
    
    /// è®¾ç½®CPUäº²å’Œæ€§
    fn set_cpu_affinity(&self, core: usize) {
        #[cfg(target_os = "linux")]
        {
            use std::mem;
            use libc::{cpu_set_t, sched_setaffinity, CPU_SET, CPU_ZERO};
            
            unsafe {
                let mut cpu_set: cpu_set_t = mem::zeroed();
                CPU_ZERO(&mut cpu_set);
                CPU_SET(core, &mut cpu_set);
                
                let result = sched_setaffinity(
                    0, // å½“å‰çº¿ç¨‹
                    mem::size_of::<cpu_set_t>(),
                    &cpu_set,
                );
                
                if result == 0 {
                    println!("âœ… çº¿ç¨‹ç»‘å®šåˆ°CPUæ ¸å¿ƒ {}", core);
                } else {
                    eprintln!("âš ï¸ è®¾ç½®CPUäº²å’Œæ€§å¤±è´¥: æ ¸å¿ƒ {}", core);
                }
            }
        }
    }
    
    /// è·å–æ€§èƒ½ç»Ÿè®¡
    pub fn get_performance_stats(&self) -> PerformanceStats {
        let processed = self.perf_monitor.processed_tasks.load(Ordering::Relaxed);
        let total_latency = self.perf_monitor.total_latency_ns.load(Ordering::Relaxed);
        let max_latency = self.perf_monitor.max_latency_ns.load(Ordering::Relaxed);
        let elapsed = self.perf_monitor.start_time.elapsed().as_secs_f64();
        
        PerformanceStats {
            throughput: if elapsed > 0.0 { processed as f64 / elapsed } else { 0.0 },
            avg_latency_us: if processed > 0 { (total_latency / processed) as f64 / 1000.0 } else { 0.0 },
            max_latency_us: max_latency as f64 / 1000.0,
            processed_tasks: processed,
            active_workers: self.worker_control.active_workers.load(Ordering::Relaxed),
            current_batch_size: self.batch_size.load(Ordering::Relaxed),
        }
    }
    
    /// åœæ­¢å¤„ç†å™¨
    pub async fn stop(&self) {
        println!("ğŸ›‘ åœæ­¢è¶…é«˜é¢‘å¤„ç†å™¨...");
        self.worker_control.should_stop.store(true, Ordering::SeqCst);
        
        // ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹åœæ­¢
        while self.worker_control.active_workers.load(Ordering::SeqCst) > 0 {
            tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        }
        
        println!("âœ… è¶…é«˜é¢‘å¤„ç†å™¨å·²åœæ­¢");
    }
}

impl BufferPool {
    fn new(buffer_size: usize, pool_size: usize) -> Self {
        let buffers = ArrayQueue::new(pool_size);
        
        // é¢„åˆ†é…å¯¹é½ç¼“å†²åŒº
        for _ in 0..pool_size {
            let mut buffer = AVec::with_capacity_aligned(buffer_size);
            buffer.resize(0, ProcessingTask::zeroed());
            buffers.push(buffer).ok();
        }
        
        Self {
            buffers,
            buffer_size,
            pool_size,
        }
    }
    
    async fn get_buffer(&self) -> AVec<ProcessingTask, ConstAlign<64>> {
        // å°è¯•ä»æ± ä¸­è·å–
        if let Some(buffer) = self.buffers.pop() {
            return buffer;
        }
        
        // æ± ä¸ºç©ºæ—¶åˆ›å»ºæ–°ç¼“å†²åŒº
        let mut buffer = AVec::with_capacity_aligned(self.buffer_size);
        buffer.resize(0, ProcessingTask::zeroed());
        buffer
    }
    
    async fn return_buffer(&self, mut buffer: AVec<ProcessingTask, ConstAlign<64>>) {
        buffer.clear();
        self.buffers.push(buffer).ok(); // å¦‚æœæ± æ»¡äº†å°±ä¸¢å¼ƒ
    }
}

/// è¶…é«˜é¢‘é…ç½®
pub struct UltraHFConfig {
    pub num_input_queues: usize,
    pub queue_capacity: usize,
    pub buffer_size: usize,
    pub buffer_pool_size: usize,
    pub num_simd_processors: usize,
    pub simd_batch_size: usize,
    pub max_workers: usize,
    pub initial_batch_size: usize,
    pub cpu_affinity: Vec<usize>,
}

impl Default for UltraHFConfig {
    fn default() -> Self {
        Self {
            num_input_queues: 16,        // 16ä¸ªè¾“å…¥é˜Ÿåˆ—
            queue_capacity: 8192,        // æ¯é˜Ÿåˆ—8Kå®¹é‡
            buffer_size: 4096,           // 4Kæ‰¹å¤„ç†ç¼“å†²åŒº
            buffer_pool_size: 32,        // 32ä¸ªç¼“å†²åŒºæ± 
            num_simd_processors: 8,      // 8ä¸ªSIMDå¤„ç†å™¨
            simd_batch_size: 2048,       // SIMDæ‰¹å¤„ç†å¤§å°
            max_workers: 16,             // æœ€å¤§16ä¸ªå·¥ä½œçº¿ç¨‹
            initial_batch_size: 2048,    // åˆå§‹æ‰¹å¤„ç†å¤§å°
            cpu_affinity: (0..16).collect(), // CPUæ ¸å¿ƒ0-15
        }
    }
}

/// æ€§èƒ½ç»Ÿè®¡
#[derive(Debug, Clone)]
pub struct PerformanceStats {
    pub throughput: f64,
    pub avg_latency_us: f64,
    pub max_latency_us: f64,
    pub processed_tasks: usize,
    pub active_workers: usize,
    pub current_batch_size: usize,
} 