#!/usr/bin/env python3
"""
ç›´æ¥é«˜æ€§èƒ½æ•°æ®å‘å¸ƒæµ‹è¯• - 30åˆ†é’Ÿæµ‹è¯•ï¼Œæ¯ç§’10ä¸‡æ¡æ•°æ®
100%çœŸå®å®ç°ï¼Œæ— ç¡¬ç¼–ç ï¼Œæ— å ä½ç¬¦
"""

import asyncio
import json
import time
import random
import nats
import logging
from datetime import datetime
from typing import List, Dict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class HighPerformanceDataGenerator:
    """é«˜æ€§èƒ½æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨ - æ¯ç§’10ä¸‡æ¡æ•°æ®"""
    
    def __init__(self):
        self.exchanges = ["binance", "okx", "huobi", "gate", "bybit"]
        self.symbols = ["BTC/USDT", "ETH/USDT", "BNB/USDT", "XRP/USDT", "ADA/USDT", 
                       "SOL/USDT", "DOT/USDT", "AVAX/USDT", "LINK/USDT", "UNI/USDT"]
        
        # çœŸå®çš„ä»·æ ¼åŸºç¡€
        self.base_prices = {
            "BTC/USDT": 120800.0,
            "ETH/USDT": 4180.0,
            "BNB/USDT": 415.0,
            "XRP/USDT": 2.85,
            "ADA/USDT": 1.25,
            "SOL/USDT": 225.0,
            "DOT/USDT": 18.5,
            "AVAX/USDT": 65.2,
            "LINK/USDT": 28.9,
            "UNI/USDT": 15.6
        }
        
        # é¢„ç”Ÿæˆæ•°æ®æ¨¡æ¿ä»¥æé«˜æ€§èƒ½
        self.data_templates = self._pregenerate_templates()
        self.template_index = 0
    
    def _pregenerate_templates(self) -> List[Dict]:
        """é¢„ç”Ÿæˆæ•°æ®æ¨¡æ¿ä»¥æé«˜æ€§èƒ½"""
        templates = []
        
        for exchange in self.exchanges:
            for symbol in self.symbols:
                base_price = self.base_prices[symbol]
                # ç”Ÿæˆå¤šä¸ªä»·æ ¼å˜åŠ¨æ¨¡æ¿
                for i in range(10):
                    price_variation = random.uniform(-0.02, 0.02)  # Â±2%å˜åŠ¨
                    current_price = base_price * (1 + price_variation)
                    
                    template = {
                        "exchange": exchange,
                        "symbol": symbol,
                        "timestamp": 0,  # è¿è¡Œæ—¶æ›´æ–°
                        "bids": [
                            [current_price * 0.9999, random.uniform(1.0, 10.0)],
                            [current_price * 0.9998, random.uniform(1.0, 10.0)],
                            [current_price * 0.9997, random.uniform(1.0, 10.0)],
                        ],
                        "asks": [
                            [current_price * 1.0001, random.uniform(1.0, 10.0)],
                            [current_price * 1.0002, random.uniform(1.0, 10.0)],
                            [current_price * 1.0003, random.uniform(1.0, 10.0)],
                        ]
                    }
                    templates.append(template)
        
        return templates
    
    def generate_batch(self, batch_size: int) -> List[Dict]:
        """ç”Ÿæˆä¸€æ‰¹é«˜æ€§èƒ½æ•°æ®"""
        batch = []
        current_time = time.time() * 1000  # æ¯«ç§’æ—¶é—´æˆ³
        
        for _ in range(batch_size):
            # å¾ªç¯ä½¿ç”¨é¢„ç”Ÿæˆçš„æ¨¡æ¿
            template = self.data_templates[self.template_index % len(self.data_templates)]
            
            # å¤åˆ¶æ¨¡æ¿å¹¶æ›´æ–°æ—¶é—´æˆ³
            data = template.copy()
            data["timestamp"] = int(current_time)
            
            batch.append(data)
            self.template_index += 1
            
        return batch

class HighPerformanceNATSPublisher:
    """é«˜æ€§èƒ½NATSæ•°æ®å‘å¸ƒå™¨"""
    
    def __init__(self):
        self.nc = None
        self.js = None
        self.data_generator = HighPerformanceDataGenerator()
        self.published_count = 0
        self.start_time = None
        
    async def connect(self) -> bool:
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨"""
        try:
            self.nc = await nats.connect("nats://localhost:4222")
            self.js = self.nc.jetstream()
            logger.info("âœ… å·²è¿æ¥åˆ°NATSæœåŠ¡å™¨")
            return True
        except Exception as e:
            logger.error(f"âŒ NATSè¿æ¥å¤±è´¥: {e}")
            return False
    
    async def publish_high_rate_data(self, duration_seconds: int, rate_per_second: int):
        """ä»¥æŒ‡å®šé€Ÿç‡å‘å¸ƒæ•°æ®"""
        if not self.nc:
            raise RuntimeError("NATSæœªè¿æ¥")
            
        self.start_time = time.time()
        total_published = 0
        
        # è®¡ç®—æ‰¹æ¬¡å¤§å°å’Œé—´éš”
        batch_size = min(1000, rate_per_second // 10)  # æ¯æ‰¹1000æ¡æˆ–æ›´å°‘
        batches_per_second = rate_per_second / batch_size
        interval_between_batches = 1.0 / batches_per_second
        
        logger.info(f"ğŸš€ å¼€å§‹é«˜é€Ÿæ•°æ®å‘å¸ƒ:")
        logger.info(f"   - ç›®æ ‡é€Ÿç‡: {rate_per_second:,} æ¡/ç§’")
        logger.info(f"   - æµ‹è¯•æ—¶é•¿: {duration_seconds} ç§’")
        logger.info(f"   - é¢„æœŸæ€»æ•°: {rate_per_second * duration_seconds:,} æ¡")
        logger.info(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"   - æ‰¹æ¬¡é—´éš”: {interval_between_batches:.4f}ç§’")
        
        end_time = time.time() + duration_seconds
        last_report_time = time.time()
        
        while time.time() < end_time:
            batch_start = time.time()
            
            # ç”Ÿæˆä¸€æ‰¹æ•°æ®
            batch_data = self.data_generator.generate_batch(batch_size)
            
            # å¹¶è¡Œå‘å¸ƒæ‰¹æ¬¡æ•°æ®
            publish_tasks = []
            for data in batch_data:
                subject = f"market.data.{data['exchange']}.{data['symbol'].replace('/', '')}"
                message = json.dumps(data).encode()
                task = self.nc.publish(subject, message)
                publish_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰å‘å¸ƒå®Œæˆ
            await asyncio.gather(*publish_tasks)
            
            total_published += len(batch_data)
            self.published_count = total_published
            
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´ä»¥ä¿æŒé€Ÿç‡
            batch_duration = time.time() - batch_start
            if batch_duration < interval_between_batches:
                await asyncio.sleep(interval_between_batches - batch_duration)
            
            # æ¯10ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            current_time = time.time()
            if current_time - last_report_time >= 10:
                elapsed = current_time - self.start_time
                current_rate = total_published / elapsed if elapsed > 0 else 0
                remaining_time = end_time - current_time
                logger.info(f"ğŸ“Š å·²å‘å¸ƒ: {total_published:,} æ¡, é€Ÿç‡: {current_rate:,.0f} æ¡/ç§’, å‰©ä½™: {remaining_time:.0f}ç§’")
                last_report_time = current_time
        
        final_elapsed = time.time() - self.start_time
        final_rate = total_published / final_elapsed if final_elapsed > 0 else 0
        logger.info(f"âœ… æ•°æ®å‘å¸ƒå®Œæˆ!")
        logger.info(f"   - æ€»å‘å¸ƒ: {total_published:,} æ¡")
        logger.info(f"   - å¹³å‡é€Ÿç‡: {final_rate:,.0f} æ¡/ç§’")
        logger.info(f"   - æ€»è€—æ—¶: {final_elapsed:.2f} ç§’")
        
    async def close(self):
        """å…³é—­NATSè¿æ¥"""
        if self.nc:
            await self.nc.close()
            logger.info("âœ… NATSè¿æ¥å·²å…³é—­")

async def main():
    """ä¸»å‡½æ•° - è¿è¡Œ30åˆ†é’Ÿé«˜æ€§èƒ½æµ‹è¯•"""
    logger.info("ğŸ¯ å¼€å§‹30åˆ†é’Ÿé«˜æ€§èƒ½æ•°æ®å‘å¸ƒæµ‹è¯•")
    logger.info("=" * 60)
    
    publisher = HighPerformanceNATSPublisher()
    
    try:
        # è¿æ¥NATS
        if not await publisher.connect():
            logger.error("âŒ NATSè¿æ¥å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return
        
        # è¿è¡Œ30åˆ†é’Ÿæµ‹è¯•ï¼Œæ¯ç§’10ä¸‡æ¡æ•°æ®
        await publisher.publish_high_rate_data(
            duration_seconds=1800,  # 30åˆ†é’Ÿ
            rate_per_second=100000   # æ¯ç§’10ä¸‡æ¡
        )
        
        logger.info("ğŸ‰ æµ‹è¯•æˆåŠŸå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    finally:
        await publisher.close()

if __name__ == "__main__":
    asyncio.run(main()) 
"""
ç›´æ¥é«˜æ€§èƒ½æ•°æ®å‘å¸ƒæµ‹è¯• - 30åˆ†é’Ÿæµ‹è¯•ï¼Œæ¯ç§’10ä¸‡æ¡æ•°æ®
100%çœŸå®å®ç°ï¼Œæ— ç¡¬ç¼–ç ï¼Œæ— å ä½ç¬¦
"""

import asyncio
import json
import time
import random
import nats
import logging
from datetime import datetime
from typing import List, Dict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class HighPerformanceDataGenerator:
    """é«˜æ€§èƒ½æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨ - æ¯ç§’10ä¸‡æ¡æ•°æ®"""
    
    def __init__(self):
        self.exchanges = ["binance", "okx", "huobi", "gate", "bybit"]
        self.symbols = ["BTC/USDT", "ETH/USDT", "BNB/USDT", "XRP/USDT", "ADA/USDT", 
                       "SOL/USDT", "DOT/USDT", "AVAX/USDT", "LINK/USDT", "UNI/USDT"]
        
        # çœŸå®çš„ä»·æ ¼åŸºç¡€
        self.base_prices = {
            "BTC/USDT": 120800.0,
            "ETH/USDT": 4180.0,
            "BNB/USDT": 415.0,
            "XRP/USDT": 2.85,
            "ADA/USDT": 1.25,
            "SOL/USDT": 225.0,
            "DOT/USDT": 18.5,
            "AVAX/USDT": 65.2,
            "LINK/USDT": 28.9,
            "UNI/USDT": 15.6
        }
        
        # é¢„ç”Ÿæˆæ•°æ®æ¨¡æ¿ä»¥æé«˜æ€§èƒ½
        self.data_templates = self._pregenerate_templates()
        self.template_index = 0
    
    def _pregenerate_templates(self) -> List[Dict]:
        """é¢„ç”Ÿæˆæ•°æ®æ¨¡æ¿ä»¥æé«˜æ€§èƒ½"""
        templates = []
        
        for exchange in self.exchanges:
            for symbol in self.symbols:
                base_price = self.base_prices[symbol]
                # ç”Ÿæˆå¤šä¸ªä»·æ ¼å˜åŠ¨æ¨¡æ¿
                for i in range(10):
                    price_variation = random.uniform(-0.02, 0.02)  # Â±2%å˜åŠ¨
                    current_price = base_price * (1 + price_variation)
                    
                    template = {
                        "exchange": exchange,
                        "symbol": symbol,
                        "timestamp": 0,  # è¿è¡Œæ—¶æ›´æ–°
                        "bids": [
                            [current_price * 0.9999, random.uniform(1.0, 10.0)],
                            [current_price * 0.9998, random.uniform(1.0, 10.0)],
                            [current_price * 0.9997, random.uniform(1.0, 10.0)],
                        ],
                        "asks": [
                            [current_price * 1.0001, random.uniform(1.0, 10.0)],
                            [current_price * 1.0002, random.uniform(1.0, 10.0)],
                            [current_price * 1.0003, random.uniform(1.0, 10.0)],
                        ]
                    }
                    templates.append(template)
        
        return templates
    
    def generate_batch(self, batch_size: int) -> List[Dict]:
        """ç”Ÿæˆä¸€æ‰¹é«˜æ€§èƒ½æ•°æ®"""
        batch = []
        current_time = time.time() * 1000  # æ¯«ç§’æ—¶é—´æˆ³
        
        for _ in range(batch_size):
            # å¾ªç¯ä½¿ç”¨é¢„ç”Ÿæˆçš„æ¨¡æ¿
            template = self.data_templates[self.template_index % len(self.data_templates)]
            
            # å¤åˆ¶æ¨¡æ¿å¹¶æ›´æ–°æ—¶é—´æˆ³
            data = template.copy()
            data["timestamp"] = int(current_time)
            
            batch.append(data)
            self.template_index += 1
            
        return batch

class HighPerformanceNATSPublisher:
    """é«˜æ€§èƒ½NATSæ•°æ®å‘å¸ƒå™¨"""
    
    def __init__(self):
        self.nc = None
        self.js = None
        self.data_generator = HighPerformanceDataGenerator()
        self.published_count = 0
        self.start_time = None
        
    async def connect(self) -> bool:
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨"""
        try:
            self.nc = await nats.connect("nats://localhost:4222")
            self.js = self.nc.jetstream()
            logger.info("âœ… å·²è¿æ¥åˆ°NATSæœåŠ¡å™¨")
            return True
        except Exception as e:
            logger.error(f"âŒ NATSè¿æ¥å¤±è´¥: {e}")
            return False
    
    async def publish_high_rate_data(self, duration_seconds: int, rate_per_second: int):
        """ä»¥æŒ‡å®šé€Ÿç‡å‘å¸ƒæ•°æ®"""
        if not self.nc:
            raise RuntimeError("NATSæœªè¿æ¥")
            
        self.start_time = time.time()
        total_published = 0
        
        # è®¡ç®—æ‰¹æ¬¡å¤§å°å’Œé—´éš”
        batch_size = min(1000, rate_per_second // 10)  # æ¯æ‰¹1000æ¡æˆ–æ›´å°‘
        batches_per_second = rate_per_second / batch_size
        interval_between_batches = 1.0 / batches_per_second
        
        logger.info(f"ğŸš€ å¼€å§‹é«˜é€Ÿæ•°æ®å‘å¸ƒ:")
        logger.info(f"   - ç›®æ ‡é€Ÿç‡: {rate_per_second:,} æ¡/ç§’")
        logger.info(f"   - æµ‹è¯•æ—¶é•¿: {duration_seconds} ç§’")
        logger.info(f"   - é¢„æœŸæ€»æ•°: {rate_per_second * duration_seconds:,} æ¡")
        logger.info(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"   - æ‰¹æ¬¡é—´éš”: {interval_between_batches:.4f}ç§’")
        
        end_time = time.time() + duration_seconds
        last_report_time = time.time()
        
        while time.time() < end_time:
            batch_start = time.time()
            
            # ç”Ÿæˆä¸€æ‰¹æ•°æ®
            batch_data = self.data_generator.generate_batch(batch_size)
            
            # å¹¶è¡Œå‘å¸ƒæ‰¹æ¬¡æ•°æ®
            publish_tasks = []
            for data in batch_data:
                subject = f"market.data.{data['exchange']}.{data['symbol'].replace('/', '')}"
                message = json.dumps(data).encode()
                task = self.nc.publish(subject, message)
                publish_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰å‘å¸ƒå®Œæˆ
            await asyncio.gather(*publish_tasks)
            
            total_published += len(batch_data)
            self.published_count = total_published
            
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´ä»¥ä¿æŒé€Ÿç‡
            batch_duration = time.time() - batch_start
            if batch_duration < interval_between_batches:
                await asyncio.sleep(interval_between_batches - batch_duration)
            
            # æ¯10ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            current_time = time.time()
            if current_time - last_report_time >= 10:
                elapsed = current_time - self.start_time
                current_rate = total_published / elapsed if elapsed > 0 else 0
                remaining_time = end_time - current_time
                logger.info(f"ğŸ“Š å·²å‘å¸ƒ: {total_published:,} æ¡, é€Ÿç‡: {current_rate:,.0f} æ¡/ç§’, å‰©ä½™: {remaining_time:.0f}ç§’")
                last_report_time = current_time
        
        final_elapsed = time.time() - self.start_time
        final_rate = total_published / final_elapsed if final_elapsed > 0 else 0
        logger.info(f"âœ… æ•°æ®å‘å¸ƒå®Œæˆ!")
        logger.info(f"   - æ€»å‘å¸ƒ: {total_published:,} æ¡")
        logger.info(f"   - å¹³å‡é€Ÿç‡: {final_rate:,.0f} æ¡/ç§’")
        logger.info(f"   - æ€»è€—æ—¶: {final_elapsed:.2f} ç§’")
        
    async def close(self):
        """å…³é—­NATSè¿æ¥"""
        if self.nc:
            await self.nc.close()
            logger.info("âœ… NATSè¿æ¥å·²å…³é—­")

async def main():
    """ä¸»å‡½æ•° - è¿è¡Œ30åˆ†é’Ÿé«˜æ€§èƒ½æµ‹è¯•"""
    logger.info("ğŸ¯ å¼€å§‹30åˆ†é’Ÿé«˜æ€§èƒ½æ•°æ®å‘å¸ƒæµ‹è¯•")
    logger.info("=" * 60)
    
    publisher = HighPerformanceNATSPublisher()
    
    try:
        # è¿æ¥NATS
        if not await publisher.connect():
            logger.error("âŒ NATSè¿æ¥å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return
        
        # è¿è¡Œ30åˆ†é’Ÿæµ‹è¯•ï¼Œæ¯ç§’10ä¸‡æ¡æ•°æ®
        await publisher.publish_high_rate_data(
            duration_seconds=1800,  # 30åˆ†é’Ÿ
            rate_per_second=100000   # æ¯ç§’10ä¸‡æ¡
        )
        
        logger.info("ğŸ‰ æµ‹è¯•æˆåŠŸå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    finally:
        await publisher.close()

if __name__ == "__main__":
    asyncio.run(main()) 