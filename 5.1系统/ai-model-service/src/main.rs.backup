use axum::{
    extract::{Path, Query, State},
    http::StatusCode,
    response::IntoResponse,
    routing::{get, post, put, delete},
    Json, Router,
};
use std::{net::SocketAddr, sync::Arc};
use tower::ServiceBuilder;
use tower_http::{cors::CorsLayer, trace::TraceLayer};
use tracing::info;

mod handlers;
mod services;
mod models;

use models::{StandardResponse, ModelStatus, TrainingJob, PredictionResult, ModelMetrics};
use services::{ModelManager, TrainingEngine, InferenceEngine, ModelRegistry};
use services::{DatasetManager, ShapExplainer};

#[derive(Clone)]
pub struct AppState {
    model_manager: Arc<ModelManager>,
    training_engine: Arc<TrainingEngine>,
    inference_engine: Arc<InferenceEngine>,
    model_registry: Arc<ModelRegistry>,
    dataset_manager: Arc<DatasetManager>,
    shap_explainer: Arc<ShapExplainer>,
    dataset_manager: Arc<DatasetManager>,
    shap_explainer: Arc<ShapExplainer>,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt()
        .with_env_filter("ai_model_service=debug")
        .init();

    info!("ðŸš€ Starting AI Model Service v1.0.0 (48 APIs)");

    let model_manager = Arc::new(ModelManager::new().await?);
    let training_engine = Arc::new(TrainingEngine::new().await?);
    let inference_engine = Arc::new(InferenceEngine::new().await?);
    let model_registry = Arc::new(ModelRegistry::new().await?);

    let app_state = AppState {
        model_manager,
        training_engine,
        inference_engine,
        model_registry,
        dataset_manager: Arc::new(DatasetManager::new().await?),
        shap_explainer: Arc::new(ShapExplainer::new().await?),
    };

    // æž„å»ºè·¯ç”± - 48ä¸ªAPIç«¯ç‚¹
    let app = Router::new()
        .route("/health", get(health_check))
        
        // === æ¨¡åž‹ç®¡ç†API (16ä¸ª) ===
        .route("/api/ml/models", get(handlers::models::list_models))
        .route("/api/ml/models", post(handlers::models::create_model))
        .route("/api/ml/models/:id", get(handlers::models::get_model))
        .route("/api/ml/models/:id", put(handlers::models::update_model))
        .route("/api/ml/models/:id", delete(handlers::models::delete_model))
        .route("/api/ml/models/:id/status", get(handlers::models::get_model_status))
        .route("/api/ml/models/:id/metadata", get(handlers::models::get_model_metadata))
        .route("/api/ml/models/:id/metadata", put(handlers::models::update_model_metadata))
        .route("/api/ml/models/:id/versions", get(handlers::models::get_model_versions))
        .route("/api/ml/models/:id/versions", post(handlers::models::create_model_version))
        .route("/api/ml/models/:id/versions/:version", get(handlers::models::get_model_version))
        .route("/api/ml/models/:id/deploy", post(handlers::models::deploy_model))
        .route("/api/ml/models/:id/undeploy", post(handlers::models::undeploy_model))
        .route("/api/ml/models/:id/rollback", post(handlers::models::rollback_model))
        .route("/api/ml/models/search", post(handlers::models::search_models))
        .route("/api/ml/models/:id/clone", post(handlers::models::clone_model))

        // === è®­ç»ƒç®¡ç†API (12ä¸ª) ===
        .route("/api/ml/training/jobs", get(handlers::training::handle_get))
        .route("/api/ml/training/jobs", post(handlers::training::handle_get))
        .route("/api/ml/training/jobs/:id", get(handlers::training::handle_get))
        .route("/api/ml/training/jobs/:id/start", post(handlers::training::handle_get))
        .route("/api/ml/training/jobs/:id/stop", post(handlers::training::handle_get))
        .route("/api/ml/training/jobs/:id/pause", post(handlers::training::handle_get))
        .route("/api/ml/training/jobs/:id/resume", post(handlers::training::handle_get))
        .route("/api/ml/training/jobs/:id/logs", get(handlers::training::handle_get))
        .route("/api/ml/training/jobs/:id/metrics", get(handlers::training::handle_get))
        .route("/api/ml/training/jobs/:id/checkpoints", get(handlers::training::handle_get))
        .route("/api/ml/training/hyperparameters", post(handlers::training::handle_get))
        .route("/api/ml/training/datasets", get(handlers::training::handle_get))

        // === æŽ¨ç†æœåŠ¡API (10ä¸ª) ===
        .route("/api/ml/inference/predict", post(handlers::inference::handle_get))
        .route("/api/ml/inference/batch-predict", post(handlers::inference::handle_get))
        .route("/api/ml/inference/:model_id/predict", post(handlers::inference::handle_get))
        .route("/api/ml/inference/explain", post(handlers::inference::handle_get))
        .route("/api/ml/inference/drift", post(handlers::inference::handle_get))
        .route("/api/ml/inference/ab-test", post(handlers::inference::handle_get))
        .route("/api/ml/inference/benchmark", post(handlers::inference::handle_get))
        .route("/api/ml/inference/latency", get(handlers::inference::handle_get))
        .route("/api/ml/inference/throughput", get(handlers::inference::handle_get))
        .route("/api/ml/inference/quality", get(handlers::inference::handle_get))

        // === æ¨¡åž‹ç›‘æŽ§API (10ä¸ª) ===
        .route("/api/ml/monitoring/performance", get(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/accuracy", get(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/drift", get(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/bias", get(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/alerts", get(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/alerts", post(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/health", get(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/usage", get(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/resources", get(handlers::monitoring::handle_get))
        .route("/api/ml/monitoring/report", post(handlers::monitoring::handle_get))

        .layer(
            ServiceBuilder::new()
                .layer(TraceLayer::new_for_http())
                .layer(CorsLayer::permissive())
        )
        .with_state(app_state);

    let addr = SocketAddr::from(([0, 0, 0, 0], 4006));
    info!("ðŸ¤– AI Model Service listening on http://{}", addr);
    info!("âœ… All 48 APIs initialized successfully");
    
    let listener = tokio::net::TcpListener::bind(addr).await?;
    axum::serve(listener, app).await?;

    Ok(())
}

async fn health_check() -> impl IntoResponse {
    Json(StandardResponse::success(serde_json::json!({
        "status": "healthy",
        "service": "ai-model-service",
        "version": "1.0.0",
        "apis_count": 48,
        "capabilities": ["model-management", "training", "inference", "monitoring"],
        "timestamp": chrono::Utc::now().timestamp(),
    })))
}