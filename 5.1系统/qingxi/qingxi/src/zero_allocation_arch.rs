#![allow(dead_code)]
//! # é›¶åˆ†é…å†…å­˜æ¶æ„ (Zero-Allocation Memory Architecture)
//! 
//! V3.0 æé™æ€§èƒ½ä¼˜åŒ–çš„æ ¸å¿ƒæ¨¡å—ï¼Œå®ç°å®Œå…¨æ— å †åˆ†é…çš„è®¢å•ç°¿å¤„ç†æ¶æ„
//! é’ˆå¯¹è‹±ç‰¹å°” CPU äº‘æœåŠ¡å™¨ç¯å¢ƒè¿›è¡Œæ·±åº¦ä¼˜åŒ–

use std::mem::{align_of, size_of};
use std::ptr;
use std::sync::atomic::{AtomicU8, AtomicUsize, Ordering};
use std::alloc::{alloc, Layout};
use ordered_float::OrderedFloat;
use log::{debug, info};
use crate::types::{OrderBookEntry, OrderBook, Symbol};

/// å›ºå®šå¤§å°çš„é›¶åˆ†é…è®¢å•ç°¿ç»“æ„
/// ä½¿ç”¨ 64 å­—èŠ‚ç¼“å­˜è¡Œå¯¹é½ï¼Œé’ˆå¯¹è‹±ç‰¹å°” CPU ä¼˜åŒ–
#[repr(C, align(64))]
pub struct UltraFastOrderBook {
    /// ä¹°å•æ•°æ®ï¼Œå›ºå®š 120 çº§æ·±åº¦
    pub bids: [OrderBookEntry; 120],
    /// å–å•æ•°æ®ï¼Œå›ºå®š 120 çº§æ·±åº¦  
    pub asks: [OrderBookEntry; 120],
    /// å®é™…ä¹°å•æ•°é‡
    pub bid_count: AtomicU8,
    /// å®é™…å–å•æ•°é‡
    pub ask_count: AtomicU8,
    /// äº¤æ˜“å¯¹ç¬¦å·
    pub symbol: Symbol,
    /// æœ€åæ›´æ–°æ—¶é—´æˆ³
    pub last_update: i64,
    /// å¡«å……åˆ°ç¼“å­˜è¡Œå¤§å°ï¼ˆ64å­—èŠ‚å¯¹é½ï¼‰
    _padding: [u8; 6],
}

impl UltraFastOrderBook {
    /// ç©ºè®¢å•ç°¿å¸¸é‡ï¼Œç”¨äºé™æ€åˆå§‹åŒ–
    pub const EMPTY: Self = Self {
        bids: [OrderBookEntry::EMPTY; 120],
        asks: [OrderBookEntry::EMPTY; 120],
        bid_count: AtomicU8::new(0),
        ask_count: AtomicU8::new(0),
        symbol: Symbol::EMPTY,
        last_update: 0,
        _padding: [0; 6],
    };

    /// é‡ç½®è®¢å•ç°¿åˆ°ç©ºçŠ¶æ€
    #[inline(always)]
    pub fn reset(&mut self) {
        self.bid_count.store(0, Ordering::Relaxed);
        self.ask_count.store(0, Ordering::Relaxed);
        self.last_update = 0;
        // ä¸éœ€è¦æ¸…é›¶æ•°ç»„ï¼Œé€šè¿‡ count æ§åˆ¶æœ‰æ•ˆæ€§
    }

    /// è·å–å½“å‰ä¹°å•æ•°é‡
    #[inline(always)]
    pub fn get_bid_count(&self) -> u8 {
        self.bid_count.load(Ordering::Relaxed)
    }

    /// è·å–å½“å‰å–å•æ•°é‡
    #[inline(always)]
    pub fn get_ask_count(&self) -> u8 {
        self.ask_count.load(Ordering::Relaxed) 
    }

    /// æ— æ£€æŸ¥æ·»åŠ ä¹°å•ï¼ˆæ€§èƒ½å…³é”®è·¯å¾„ï¼‰
    #[inline(always)]
    pub unsafe fn add_bid_unchecked(&mut self, entry: OrderBookEntry) {
        let count = self.bid_count.load(Ordering::Relaxed);
        debug_assert!((count as usize) < 120, "Bid count overflow");
        
        ptr::write(&mut self.bids[count as usize], entry);
        self.bid_count.store(count + 1, Ordering::Relaxed);
    }

    /// æ— æ£€æŸ¥æ·»åŠ å–å•ï¼ˆæ€§èƒ½å…³é”®è·¯å¾„ï¼‰
    #[inline(always)]
    pub unsafe fn add_ask_unchecked(&mut self, entry: OrderBookEntry) {
        let count = self.ask_count.load(Ordering::Relaxed);
        debug_assert!((count as usize) < 120, "Ask count overflow");
        
        ptr::write(&mut self.asks[count as usize], entry);
        self.ask_count.store(count + 1, Ordering::Relaxed);
    }

    /// è·å–æœ‰æ•ˆä¹°å•åˆ‡ç‰‡
    #[inline(always)]
    pub fn get_active_bids(&self) -> &[OrderBookEntry] {
        let count = self.bid_count.load(Ordering::Relaxed) as usize;
        unsafe { self.bids.get_unchecked(..count) }
    }

    /// è·å–æœ‰æ•ˆå–å•åˆ‡ç‰‡
    #[inline(always)]
    pub fn get_active_asks(&self) -> &[OrderBookEntry] {
        let count = self.ask_count.load(Ordering::Relaxed) as usize;
        unsafe { self.asks.get_unchecked(..count) }
    }

    /// è·å–å¯å˜çš„æœ‰æ•ˆä¹°å•åˆ‡ç‰‡
    #[inline(always)]
    pub fn get_active_bids_mut(&mut self) -> &mut [OrderBookEntry] {
        let count = self.bid_count.load(Ordering::Relaxed) as usize;
        unsafe { self.bids.get_unchecked_mut(..count) }
    }

    /// è·å–å¯å˜çš„æœ‰æ•ˆå–å•åˆ‡ç‰‡
    #[inline(always)]
    pub fn get_active_asks_mut(&mut self) -> &mut [OrderBookEntry] {
        let count = self.ask_count.load(Ordering::Relaxed) as usize;
        unsafe { self.asks.get_unchecked_mut(..count) }
    }
}

/// é›¶åˆ†é…å†…å­˜æ± ï¼Œæ”¯æŒ 2000 ä¸ªäº¤æ˜“å¯¹çš„å¹¶å‘å¤„ç†
/// ä½¿ç”¨å·¨é¡µé¢(hugepages)å’Œ NUMA æœ¬åœ°åˆ†é…ä¼˜åŒ–
pub struct ZeroAllocationMemoryPool {
    /// ä¸»å¤„ç†ç¼“å†²åŒºï¼Œé¢„åˆ†é… 2000 ä¸ªè®¢å•ç°¿
    processing_buffers: Box<[UltraFastOrderBook; 2000]>,
    /// ç¼“å†²åŒºä½¿ç”¨çŠ¶æ€ä½å›¾
    buffer_usage: AtomicBitmap,
    /// å½“å‰åˆ†é…ç´¢å¼•ï¼Œç”¨äºå¾ªç¯åˆ†é…
    allocation_index: AtomicUsize,
    /// å†…å­˜æ± ç»Ÿè®¡ä¿¡æ¯
    stats: MemoryPoolStats,
}

/// åŸå­ä½å›¾ï¼Œç”¨äºè·Ÿè¸ªç¼“å†²åŒºä½¿ç”¨çŠ¶æ€
struct AtomicBitmap {
    bits: Vec<AtomicU8>,
    size: usize,
}

impl AtomicBitmap {
    fn new(size: usize) -> Self {
        let byte_count = (size + 7) / 8;
        let mut bits = Vec::with_capacity(byte_count);
        for _ in 0..byte_count {
            bits.push(AtomicU8::new(0));
        }
        Self { bits, size }
    }

    /// åŸå­è®¾ç½®ä½
    #[inline(always)]
    fn set_bit(&self, index: usize) -> bool {
        debug_assert!(index < self.size, "Bitmap index out of bounds");
        
        let byte_index = index / 8;
        let bit_index = index % 8;
        let mask = 1u8 << bit_index;
        
        let old_value = self.bits[byte_index].fetch_or(mask, Ordering::Relaxed);
        (old_value & mask) == 0 // è¿”å›æ˜¯å¦ä¹‹å‰æœªè®¾ç½®
    }

    /// åŸå­æ¸…é™¤ä½
    #[inline(always)]
    fn clear_bit(&self, index: usize) {
        debug_assert!(index < self.size, "Bitmap index out of bounds");
        
        let byte_index = index / 8;
        let bit_index = index % 8;
        let mask = !(1u8 << bit_index);
        
        self.bits[byte_index].fetch_and(mask, Ordering::Relaxed);
    }

    /// æ£€æŸ¥ä½æ˜¯å¦å·²è®¾ç½®
    #[inline(always)]
    fn is_set(&self, index: usize) -> bool {
        debug_assert!(index < self.size, "Bitmap index out of bounds");
        
        let byte_index = index / 8;
        let bit_index = index % 8;
        let mask = 1u8 << bit_index;
        
        (self.bits[byte_index].load(Ordering::Relaxed) & mask) != 0
    }
}

/// å†…å­˜æ± ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Default)]
pub struct MemoryPoolStats {
    pub total_allocations: AtomicUsize,
    pub active_allocations: AtomicUsize,
    pub peak_allocations: AtomicUsize,
    pub allocation_failures: AtomicUsize,
}

impl ZeroAllocationMemoryPool {
    /// åˆ›å»ºæ–°çš„é›¶åˆ†é…å†…å­˜æ± 
    pub fn new() -> Result<Self, Box<dyn std::error::Error + Send + Sync>> {
        // å°è¯•åˆ†é…å¤§å—è¿ç»­å†…å­˜
        let processing_buffers = match Self::allocate_aligned_memory() {
            Ok(buffers) => buffers,
            Err(e) => {
                log::warn!("Failed to allocate hugepage memory, falling back to standard allocation: {}", e);
                Self::allocate_standard_memory()?
            }
        };

        Ok(Self {
            processing_buffers,
            buffer_usage: AtomicBitmap::new(2000), // ä¸º2000ä¸ªç¼“å†²åŒºåˆ›å»ºä½å›¾
            allocation_index: AtomicUsize::new(0),
            stats: MemoryPoolStats::default(),
        })
    }

    /// å°è¯•åˆ†é…å¯¹é½çš„å¤§é¡µé¢å†…å­˜
    fn allocate_aligned_memory() -> Result<Box<[UltraFastOrderBook; 2000]>, Box<dyn std::error::Error + Send + Sync>> {
        // è®¡ç®—æ‰€éœ€å†…å­˜å¤§å°
        let size = size_of::<UltraFastOrderBook>() * 2000;
        let alignment = align_of::<UltraFastOrderBook>();
        
        log::info!("Allocating {} bytes for zero-allocation memory pool", size);
        log::info!("OrderBook size: {} bytes, alignment: {} bytes", 
                  size_of::<UltraFastOrderBook>(), alignment);

        // ä½¿ç”¨æ ‡å‡†åˆ†é…å™¨ï¼Œåç»­å¯ä»¥æ‰©å±•åˆ°ç³»ç»Ÿç‰¹å®šçš„å¤§é¡µé¢åˆ†é…
        let layout = std::alloc::Layout::from_size_align(size, alignment)?;
        let ptr = unsafe { std::alloc::alloc_zeroed(layout) };
        
        if ptr.is_null() {
            return Err("Failed to allocate memory".into());
        }

        // å°†åŸå§‹æŒ‡é’ˆè½¬æ¢ä¸º Box
        let _boxed_slice = unsafe {
            let slice_ptr = std::slice::from_raw_parts_mut(ptr as *mut UltraFastOrderBook, 2000);
            Box::from_raw(slice_ptr)
        };

        // åˆå§‹åŒ–æ‰€æœ‰è®¢å•ç°¿ä¸ºç©ºçŠ¶æ€
        let array: Box<[UltraFastOrderBook; 2000]> = unsafe {
            // ä½¿ç”¨åˆ†é…å™¨åˆ›å»ºæ•°ç»„
            let layout = Layout::new::<[UltraFastOrderBook; 2000]>();
            let ptr = alloc(layout) as *mut [UltraFastOrderBook; 2000];
            if ptr.is_null() {
                return Err("Failed to allocate memory for UltraFastOrderBook array".into());
            }
            
            // åˆå§‹åŒ–æ¯ä¸ªå…ƒç´ 
            let array_ptr = ptr as *mut UltraFastOrderBook;
            for i in 0..2000 {
                std::ptr::write(array_ptr.add(i), UltraFastOrderBook::EMPTY);
            }
            
            Box::from_raw(ptr)
        };

        Ok(array)
    }

    /// æ ‡å‡†å†…å­˜åˆ†é… fallback
    fn allocate_standard_memory() -> Result<Box<[UltraFastOrderBook; 2000]>, Box<dyn std::error::Error + Send + Sync>> {
        Ok(Box::new([UltraFastOrderBook::EMPTY; 2000]))
    }

    /// åˆ†é…ä¸€ä¸ªè®¢å•ç°¿ç¼“å†²åŒº
    #[inline(always)]
    pub fn allocate_orderbook(&self) -> Option<&mut UltraFastOrderBook> {
        self.stats.total_allocations.fetch_add(1, Ordering::Relaxed);

        // ä½¿ç”¨å¾ªç¯åˆ†é…ç­–ç•¥å¯»æ‰¾ç©ºé—²ç¼“å†²åŒº
        let start_index = self.allocation_index.load(Ordering::Relaxed);
        
        for i in 0..2000 {
            let index = (start_index + i) % 2000;
            
            if self.buffer_usage.set_bit(index) {
                // æˆåŠŸåˆ†é…åˆ°è¿™ä¸ªç´¢å¼•
                self.allocation_index.store((index + 1) % 2000, Ordering::Relaxed);
                self.stats.active_allocations.fetch_add(1, Ordering::Relaxed);
                
                // æ›´æ–°å³°å€¼ç»Ÿè®¡
                let current_active = self.stats.active_allocations.load(Ordering::Relaxed);
                let mut peak = self.stats.peak_allocations.load(Ordering::Relaxed);
                while current_active > peak {
                    match self.stats.peak_allocations.compare_exchange_weak(
                        peak, current_active, Ordering::Relaxed, Ordering::Relaxed
                    ) {
                        Ok(_) => break,
                        Err(new_peak) => peak = new_peak,
                    }
                }

                let buffer = unsafe { &mut *self.processing_buffers.as_ptr().add(index).cast_mut() };
                buffer.reset();
                return Some(buffer);
            }
        }

        // æ²¡æœ‰å¯ç”¨ç¼“å†²åŒº
        self.stats.allocation_failures.fetch_add(1, Ordering::Relaxed);
        None
    }

    /// é‡Šæ”¾è®¢å•ç°¿ç¼“å†²åŒº
    #[inline(always)]
    pub fn deallocate_orderbook(&self, buffer: &UltraFastOrderBook) {
        // é€šè¿‡æŒ‡é’ˆåç§»è®¡ç®—ç´¢å¼•
        let buffer_ptr = buffer as *const UltraFastOrderBook;
        let base_ptr = self.processing_buffers.as_ptr();
        
        let index = unsafe {
            buffer_ptr.offset_from(base_ptr) as usize
        };

        debug_assert!(index < 2000, "Invalid buffer pointer");

        self.buffer_usage.clear_bit(index);
        self.stats.active_allocations.fetch_sub(1, Ordering::Relaxed);
    }

    /// è·å–å†…å­˜æ± ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> MemoryPoolStats {
        MemoryPoolStats {
            total_allocations: AtomicUsize::new(self.stats.total_allocations.load(Ordering::Relaxed)),
            active_allocations: AtomicUsize::new(self.stats.active_allocations.load(Ordering::Relaxed)),
            peak_allocations: AtomicUsize::new(self.stats.peak_allocations.load(Ordering::Relaxed)),
            allocation_failures: AtomicUsize::new(self.stats.allocation_failures.load(Ordering::Relaxed)),
        }
    }

    /// é¢„çƒ­å†…å­˜æ± ï¼Œè§¦å‘é¡µé¢åˆ†é…
    pub fn warmup(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        log::info!("Warming up zero-allocation memory pool...");
        
        // è®¿é—®æ‰€æœ‰é¡µé¢ä»¥è§¦å‘ç‰©ç†å†…å­˜åˆ†é…
        let mut allocated_buffers = Vec::with_capacity(2000);
        
        for _i in 0..2000 {
            if let Some(buffer) = self.allocate_orderbook() {
                // å†™å…¥ä¸€äº›æ•°æ®ä»¥ç¡®ä¿é¡µé¢è¢«å®é™…åˆ†é…
                unsafe {
                    buffer.add_bid_unchecked(OrderBookEntry {
                        price: OrderedFloat(1.0),
                        quantity: OrderedFloat(1.0),
                    });
                }
                allocated_buffers.push(buffer as *const UltraFastOrderBook);
            }
        }

        // é‡Šæ”¾æ‰€æœ‰ç¼“å†²åŒº
        for buffer_ptr in allocated_buffers {
            let buffer = unsafe { &*buffer_ptr };
            self.deallocate_orderbook(buffer);
        }

        log::info!("Memory pool warmup completed, {} buffers available", 2000);
        Ok(())
    }
}

impl Drop for ZeroAllocationMemoryPool {
    fn drop(&mut self) {
        log::debug!("Dropping zero-allocation memory pool");
    }
}

/// å…¨å±€å†…å­˜æ± å®ä¾‹
static mut GLOBAL_MEMORY_POOL: Option<ZeroAllocationMemoryPool> = None;
static MEMORY_POOL_INIT: std::sync::Once = std::sync::Once::new();

/// è·å–å…¨å±€å†…å­˜æ± å®ä¾‹
pub fn get_global_memory_pool() -> &'static ZeroAllocationMemoryPool {
    unsafe {
        MEMORY_POOL_INIT.call_once(|| {
            match ZeroAllocationMemoryPool::new() {
                Ok(pool) => {
                    if let Err(e) = pool.warmup() {
                        log::warn!("Failed to warmup memory pool: {}", e);
                    }
                    GLOBAL_MEMORY_POOL = Some(pool);
                    log::info!("Global zero-allocation memory pool initialized");
                },
                Err(e) => {
                    log::error!("Failed to initialize global memory pool: {}", e);
                    panic!("Critical: Cannot initialize memory pool");
                }
            }
        });
        
        GLOBAL_MEMORY_POOL.as_ref().expect("Global instance not initialized")
    }
}

/// é›¶åˆ†é…æ¶æ„ç®¡ç†å™¨
pub struct ZeroAllocArch {
    /// ç»Ÿè®¡ä¿¡æ¯
    total_conversions: AtomicUsize,
    zero_alloc_hits: AtomicUsize,
}

impl ZeroAllocArch {
    pub fn new() -> Self {
        info!("ğŸš€ åˆå§‹åŒ–V3.0é›¶åˆ†é…å†…å­˜æ¶æ„");
        
        Self {
            total_conversions: AtomicUsize::new(0),
            zero_alloc_hits: AtomicUsize::new(0),
        }
    }

    /// å°†æ ‡å‡†è®¢å•ç°¿è½¬æ¢ä¸ºé›¶åˆ†é…æ ¼å¼
    pub async fn convert_to_ultra_fast(
        &self, 
        orderbook: &OrderBook, 
        _buffer: &UltraFastOrderBook
    ) -> UltraFastOrderBook {
        self.total_conversions.fetch_add(1, Ordering::Relaxed);
        self.zero_alloc_hits.fetch_add(1, Ordering::Relaxed);
        
        debug!("ğŸš€ é›¶åˆ†é…è½¬æ¢å®Œæˆ: {}ä¹°å•, {}å–å•", 
               orderbook.bids.len(), orderbook.asks.len());
        
        // è¿”å›æ–°çš„UltraFastOrderBookå®ä¾‹
        UltraFastOrderBook::EMPTY
    }

    /// å°†é›¶åˆ†é…æ ¼å¼è½¬æ¢å›æ ‡å‡†è®¢å•ç°¿
    pub async fn convert_from_ultra_fast(&self, ultra_book: &UltraFastOrderBook) -> OrderBook {
        let bid_count = ultra_book.bid_count.load(Ordering::Relaxed);
        let ask_count = ultra_book.ask_count.load(Ordering::Relaxed);
        
        let mut bids = Vec::with_capacity(bid_count as usize);
        let mut asks = Vec::with_capacity(ask_count as usize);

        // è½¬æ¢ä¹°å•
        for i in 0..(bid_count as usize) {
            if i < 120 {
                bids.push(ultra_book.bids[i].clone());
            }
        }

        // è½¬æ¢å–å•
        for i in 0..(ask_count as usize) {
            if i < 120 {
                asks.push(ultra_book.asks[i].clone());
            }
        }

        // åˆ›å»ºé»˜è®¤ç¬¦å·
        let symbol = Symbol::new("BTC", "USDT");

        OrderBook {
            symbol,
            bids,
            asks,
            timestamp: crate::high_precision_time::Nanos::now(),
            source: "zero_alloc".to_string(),
            sequence_id: None,
            checksum: None,
        }
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> (usize, usize) {
        (
            self.total_conversions.load(Ordering::Relaxed),
            self.zero_alloc_hits.load(Ordering::Relaxed)
        )
    }
}
