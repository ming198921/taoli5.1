#!/usr/bin/env python3
import json
from collections import defaultdict
from datetime import datetime
import statistics

def parse_timestamp(timestamp_str):
    return datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))

def analyze_cleaning_stability():
    print("ğŸ” åˆ†ææ¯ä¸ªå¸ç§æ¸…æ´—æ•°æ®çš„å¹³ç¨³æ€§...")
    
    # Group cleaning times by symbol and exchange
    symbol_cleaning_times = defaultdict(list)
    symbol_timestamps = defaultdict(list)
    
    print("ğŸ“– è¯»å–æ—¥å¿—æ–‡ä»¶...")
    with open('qingxi_system_20250726_050553.log', 'r') as f:
        for line_num, line in enumerate(f, 1):
            if line_num % 100000 == 0:
                print(f"  å¤„ç†äº† {line_num} è¡Œ...")
            
            try:
                data = json.loads(line.strip())
                message = data['fields']['message']
                
                if "ğŸ§¹ Performing data cleaning" in message:
                    # Get the timestamp for cleaning start
                    timestamp = parse_timestamp(data['timestamp'])
                    
                    # Try to find the exchange and symbol information from context
                    # Look for recent "Received OrderBook" messages
                    continue
                    
                elif "âœ… Data cleaning successful for" in message:
                    timestamp = parse_timestamp(data['timestamp'])
                    
                    # Extract exchange
                    if "successful for bybit" in message:
                        exchange = "bybit"
                    elif "successful for okx" in message:
                        exchange = "okx"
                    elif "successful for huobi" in message:
                        exchange = "huobi"
                    else:
                        continue
                    
                    # For now, we'll analyze by exchange since we don't have symbol info in cleaning messages
                    symbol_cleaning_times[exchange].append(timestamp)
                    
            except (json.JSONDecodeError, KeyError) as e:
                continue
    
    print(f"\næ‰¾åˆ°æ¸…æ´—äº‹ä»¶æ•°æ®æŒ‰äº¤æ˜“æ‰€åˆ†ç»„:")
    for exchange, times in symbol_cleaning_times.items():
        print(f"  {exchange}: {len(times)} æ¡è®°å½•")
    
    # Analyze time intervals between cleaning operations
    print("\nğŸ“Š æ¸…æ´—æ•°æ®ç¨³å®šæ€§åˆ†æç»“æœ:")
    print("=" * 100)
    print(f"{'äº¤æ˜“æ‰€':<10} {'æ€»è®°å½•æ•°':<10} {'å¹³å‡é—´éš”(ms)':<15} {'æ ‡å‡†å·®(ms)':<15} {'å˜å¼‚ç³»æ•°':<12} {'ç¨³å®šæ€§è¯„ä¼°':<12}")
    print("-" * 100)
    
    stability_results = []
    
    for exchange in sorted(symbol_cleaning_times.keys()):
        timestamps = sorted(symbol_cleaning_times[exchange])
        
        if len(timestamps) < 2:
            continue
            
        # Calculate intervals between consecutive cleaning operations
        intervals = []
        for i in range(1, len(timestamps)):
            interval = (timestamps[i] - timestamps[i-1]).total_seconds() * 1000  # ms
            intervals.append(interval)
        
        if not intervals:
            continue
            
        # Statistical analysis
        avg_interval = statistics.mean(intervals)
        std_dev = statistics.stdev(intervals) if len(intervals) > 1 else 0
        cv = std_dev / avg_interval if avg_interval > 0 else 0  # Coefficient of variation
        
        # Stability assessment
        if cv < 0.1:
            stability = "éå¸¸ç¨³å®š"
        elif cv < 0.3:
            stability = "ç¨³å®š"
        elif cv < 0.6:
            stability = "ä¸­ç­‰æ³¢åŠ¨"
        elif cv < 1.0:
            stability = "æ³¢åŠ¨è¾ƒå¤§"
        else:
            stability = "æä¸ç¨³å®š"
        
        print(f"{exchange:<10} {len(timestamps):<10} {avg_interval:<15.2f} {std_dev:<15.2f} {cv:<12.4f} {stability:<12}")
        
        stability_results.append({
            'exchange': exchange,
            'count': len(timestamps),
            'avg_interval': avg_interval,
            'std_dev': std_dev,
            'cv': cv,
            'stability': stability,
            'intervals': intervals
        })
    
    # Find exchanges with high volatility
    high_volatility = [r for r in stability_results if r['cv'] > 0.5]
    if high_volatility:
        print(f"\nâš ï¸  æ³¢åŠ¨è¾ƒå¤§çš„äº¤æ˜“æ‰€è¯¦ç»†åˆ†æ:")
        print("=" * 80)
        
        for result in high_volatility:
            print(f"\nğŸ” {result['exchange']} äº¤æ˜“æ‰€è¯¦ç»†åˆ†æ:")
            print(f"  ğŸ“Š æ€»è®°å½•æ•°: {result['count']}")
            print(f"  â±ï¸  å¹³å‡æ¸…æ´—é—´éš”: {result['avg_interval']:.2f}ms")
            print(f"  ğŸ“ˆ æ ‡å‡†å·®: {result['std_dev']:.2f}ms")
            print(f"  ğŸ“Š å˜å¼‚ç³»æ•°: {result['cv']:.4f}")
            print(f"  ğŸ¯ ç¨³å®šæ€§è¯„ä¼°: {result['stability']}")
            
            intervals = result['intervals']
            min_interval = min(intervals)
            max_interval = max(intervals)
            median_interval = statistics.median(intervals)
            
            print(f"  ğŸ“ æœ€å°é—´éš”: {min_interval:.2f}ms")
            print(f"  ğŸ“ æœ€å¤§é—´éš”: {max_interval:.2f}ms")
            print(f"  ğŸ“ ä¸­ä½æ•°é—´éš”: {median_interval:.2f}ms")
            print(f"  ğŸ“ é—´éš”èŒƒå›´: {max_interval - min_interval:.2f}ms")
            
            # Find outliers (intervals > 3 standard deviations from mean)
            outliers = [i for i in intervals if abs(i - result['avg_interval']) > 3 * result['std_dev']]
            if outliers:
                print(f"  âš ï¸  å¼‚å¸¸é—´éš”æ•°é‡: {len(outliers)}")
                print(f"  ğŸš¨ å¼‚å¸¸é—´éš”èŒƒå›´: {min(outliers):.2f}ms - {max(outliers):.2f}ms")
    
    # Time series analysis for the most volatile exchange
    if stability_results:
        most_volatile = max(stability_results, key=lambda x: x['cv'])
        print(f"\nğŸ“ˆ æœ€ä¸ç¨³å®šäº¤æ˜“æ‰€ ({most_volatile['exchange']}) æ—¶é—´åºåˆ—åˆ†æ:")
        print("=" * 80)
        
        timestamps = sorted(symbol_cleaning_times[most_volatile['exchange']])
        intervals = most_volatile['intervals']
        
        # Show first 20 intervals
        print("å‰20ä¸ªæ¸…æ´—é—´éš”æ—¶é—´ (ms):")
        for i, interval in enumerate(intervals[:20]):
            print(f"  {i+1:2d}. {interval:8.2f}ms")
        
        # Group by time periods to see if there are patterns
        print(f"\nğŸ“Š æŒ‰æ—¶é—´æ®µåˆ†æ (æ¯1åˆ†é’Ÿ):")
        minute_intervals = defaultdict(list)
        
        for i, timestamp in enumerate(timestamps[1:], 1):
            minute_key = timestamp.replace(second=0, microsecond=0)
            minute_intervals[minute_key].append(intervals[i-1])
        
        print(f"{'æ—¶é—´æ®µ':<20} {'è®°å½•æ•°':<8} {'å¹³å‡é—´éš”(ms)':<15} {'æ ‡å‡†å·®(ms)':<15}")
        print("-" * 65)
        
        for minute in sorted(minute_intervals.keys())[:10]:  # Show first 10 minutes
            minute_data = minute_intervals[minute]
            avg = statistics.mean(minute_data)
            std = statistics.stdev(minute_data) if len(minute_data) > 1 else 0
            
            time_str = minute.strftime('%H:%M:%S')
            print(f"{time_str:<20} {len(minute_data):<8} {avg:<15.2f} {std:<15.2f}")

if __name__ == "__main__":
    analyze_cleaning_stability()
